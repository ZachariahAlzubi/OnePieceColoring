{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74812950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270c19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        middle = self.middle(encoded)\n",
    "        decoded = self.decoder(middle)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b61048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, train_loader, num_epochs, device):\n",
    "    criterion_GAN = nn.MSELoss()\n",
    "    criterion_cycle = nn.L1Loss()\n",
    "    criterion_identity = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (bw_imgs, color_imgs) in enumerate(train_loader):\n",
    "            bw_imgs = bw_imgs.to(device)\n",
    "            color_imgs = color_imgs.to(device)\n",
    "\n",
    "            # Train the generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            fake_color_imgs = generator(bw_imgs)\n",
    "\n",
    "            # Identity loss\n",
    "            resized_color_imgs = F.interpolate(color_imgs, size=fake_color_imgs.size()[2:], mode='bilinear', align_corners=True)\n",
    "            loss_identity = criterion_identity(fake_color_imgs, resized_color_imgs) * 5.0\n",
    "\n",
    "            # GAN loss\n",
    "            pred_fake = discriminator(fake_color_imgs)\n",
    "            loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "\n",
    "            # Cycle loss\n",
    "            recovered_bw_imgs = generator(fake_color_imgs)\n",
    "            loss_cycle = criterion_cycle(recovered_bw_imgs, bw_imgs) * 10.0\n",
    "\n",
    "            # Total generator loss\n",
    "            loss_G = loss_identity + loss_GAN + loss_cycle\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train the discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            pred_real = discriminator(color_imgs)\n",
    "            loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "            pred_fake = discriminator(fake_color_imgs.detach())\n",
    "            loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "            # Total discriminator loss\n",
    "            loss_D = (loss_real + loss_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss G: {loss_G.item()}, Loss D: {loss_D.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233725d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def load_dataset(bw_folder, color_folder, image_size, batch_size):\n",
    "    bw_image_paths = [os.path.join(bw_folder, filename) for filename in os.listdir(bw_folder) if not filename.startswith('.')]\n",
    "    color_image_paths = [os.path.join(color_folder, filename) for filename in os.listdir(color_folder) if not filename.startswith('.')]\n",
    "\n",
    "    dataset = [(preprocess_image(bw_path, image_size), preprocess_image(color_path, image_size)) for bw_path, color_path in zip(bw_image_paths, color_image_paths)]\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f989f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Batch 0/17, Loss G: 13.09152603149414, Loss D: 0.2650907337665558\n",
      "Epoch 1/200, Batch 1/17, Loss G: 12.609439849853516, Loss D: 0.23490405082702637\n",
      "Epoch 1/200, Batch 2/17, Loss G: 12.114734649658203, Loss D: 0.2103501260280609\n",
      "Epoch 1/200, Batch 3/17, Loss G: 11.147067070007324, Loss D: 0.2101161628961563\n",
      "Epoch 1/200, Batch 4/17, Loss G: 10.430885314941406, Loss D: 0.2173001766204834\n",
      "Epoch 1/200, Batch 5/17, Loss G: 9.90399169921875, Loss D: 0.17615407705307007\n",
      "Epoch 1/200, Batch 6/17, Loss G: 9.613513946533203, Loss D: 0.16853740811347961\n",
      "Epoch 1/200, Batch 7/17, Loss G: 9.433649063110352, Loss D: 0.17952758073806763\n",
      "Epoch 1/200, Batch 8/17, Loss G: 9.1299409866333, Loss D: 0.1319616734981537\n",
      "Epoch 1/200, Batch 9/17, Loss G: 8.888096809387207, Loss D: 0.15965761244297028\n",
      "Epoch 1/200, Batch 10/17, Loss G: 8.867410659790039, Loss D: 0.12664148211479187\n",
      "Epoch 1/200, Batch 11/17, Loss G: 8.708919525146484, Loss D: 0.11643802374601364\n",
      "Epoch 1/200, Batch 12/17, Loss G: 8.909759521484375, Loss D: 0.11503826081752777\n",
      "Epoch 1/200, Batch 13/17, Loss G: 8.405900955200195, Loss D: 0.11766597628593445\n",
      "Epoch 1/200, Batch 14/17, Loss G: 8.572206497192383, Loss D: 0.13338589668273926\n",
      "Epoch 1/200, Batch 15/17, Loss G: 8.191408157348633, Loss D: 0.11500483751296997\n",
      "Epoch 1/200, Batch 16/17, Loss G: 8.20364761352539, Loss D: 0.11923335492610931\n",
      "Epoch 2/200, Batch 0/17, Loss G: 8.235710144042969, Loss D: 0.09605783224105835\n",
      "Epoch 2/200, Batch 1/17, Loss G: 7.898003578186035, Loss D: 0.08448223024606705\n",
      "Epoch 2/200, Batch 2/17, Loss G: 8.28519058227539, Loss D: 0.06963759660720825\n",
      "Epoch 2/200, Batch 3/17, Loss G: 7.930635452270508, Loss D: 0.1260651797056198\n",
      "Epoch 2/200, Batch 4/17, Loss G: 7.777914047241211, Loss D: 0.0915190577507019\n",
      "Epoch 2/200, Batch 5/17, Loss G: 8.10651683807373, Loss D: 0.07478311657905579\n",
      "Epoch 2/200, Batch 6/17, Loss G: 8.051870346069336, Loss D: 0.08592396974563599\n",
      "Epoch 2/200, Batch 7/17, Loss G: 7.94575309753418, Loss D: 0.089395672082901\n",
      "Epoch 2/200, Batch 8/17, Loss G: 7.777202606201172, Loss D: 0.07308828830718994\n",
      "Epoch 2/200, Batch 9/17, Loss G: 7.472745895385742, Loss D: 0.11311570554971695\n",
      "Epoch 2/200, Batch 10/17, Loss G: 7.685979843139648, Loss D: 0.10879893600940704\n",
      "Epoch 2/200, Batch 11/17, Loss G: 7.355255126953125, Loss D: 0.12565521895885468\n",
      "Epoch 2/200, Batch 12/17, Loss G: 7.729002952575684, Loss D: 0.1735006421804428\n",
      "Epoch 2/200, Batch 13/17, Loss G: 7.618082523345947, Loss D: 0.14280928671360016\n",
      "Epoch 2/200, Batch 14/17, Loss G: 7.317908763885498, Loss D: 0.15312924981117249\n",
      "Epoch 2/200, Batch 15/17, Loss G: 7.455833911895752, Loss D: 0.18615025281906128\n",
      "Epoch 2/200, Batch 16/17, Loss G: 6.924861431121826, Loss D: 0.10398740321397781\n",
      "Epoch 3/200, Batch 0/17, Loss G: 7.274538993835449, Loss D: 0.08436934649944305\n",
      "Epoch 3/200, Batch 1/17, Loss G: 7.100838661193848, Loss D: 0.09184548258781433\n",
      "Epoch 3/200, Batch 2/17, Loss G: 6.918299674987793, Loss D: 0.07480286806821823\n",
      "Epoch 3/200, Batch 3/17, Loss G: 7.182252883911133, Loss D: 0.07278761267662048\n",
      "Epoch 3/200, Batch 4/17, Loss G: 7.018507957458496, Loss D: 0.11569640040397644\n",
      "Epoch 3/200, Batch 5/17, Loss G: 7.280689239501953, Loss D: 0.11326073855161667\n",
      "Epoch 3/200, Batch 6/17, Loss G: 7.2446794509887695, Loss D: 0.08982133865356445\n",
      "Epoch 3/200, Batch 7/17, Loss G: 7.068845748901367, Loss D: 0.10282696783542633\n",
      "Epoch 3/200, Batch 8/17, Loss G: 6.855507850646973, Loss D: 0.12508660554885864\n",
      "Epoch 3/200, Batch 9/17, Loss G: 7.433876037597656, Loss D: 0.10843729972839355\n",
      "Epoch 3/200, Batch 10/17, Loss G: 7.314634799957275, Loss D: 0.18217891454696655\n",
      "Epoch 3/200, Batch 11/17, Loss G: 7.166104316711426, Loss D: 0.10935844480991364\n",
      "Epoch 3/200, Batch 12/17, Loss G: 6.753510475158691, Loss D: 0.11383670568466187\n",
      "Epoch 3/200, Batch 13/17, Loss G: 7.080228328704834, Loss D: 0.10609622299671173\n",
      "Epoch 3/200, Batch 14/17, Loss G: 6.959665775299072, Loss D: 0.13697482645511627\n",
      "Epoch 3/200, Batch 15/17, Loss G: 6.95587682723999, Loss D: 0.1343921273946762\n",
      "Epoch 3/200, Batch 16/17, Loss G: 6.850560188293457, Loss D: 0.11675764620304108\n",
      "Epoch 4/200, Batch 0/17, Loss G: 6.685927867889404, Loss D: 0.1215687245130539\n",
      "Epoch 4/200, Batch 1/17, Loss G: 6.831483840942383, Loss D: 0.16466109454631805\n",
      "Epoch 4/200, Batch 2/17, Loss G: 6.831596374511719, Loss D: 0.1259479522705078\n",
      "Epoch 4/200, Batch 3/17, Loss G: 6.518558979034424, Loss D: 0.22663041949272156\n",
      "Epoch 4/200, Batch 4/17, Loss G: 7.2361836433410645, Loss D: 0.23208589851856232\n",
      "Epoch 4/200, Batch 5/17, Loss G: 6.972965240478516, Loss D: 0.14651411771774292\n",
      "Epoch 4/200, Batch 6/17, Loss G: 6.947788715362549, Loss D: 0.15991029143333435\n",
      "Epoch 4/200, Batch 7/17, Loss G: 6.721218109130859, Loss D: 0.17149928212165833\n",
      "Epoch 4/200, Batch 8/17, Loss G: 6.922290325164795, Loss D: 0.0645379051566124\n",
      "Epoch 4/200, Batch 9/17, Loss G: 6.503895282745361, Loss D: 0.12312005460262299\n",
      "Epoch 4/200, Batch 10/17, Loss G: 6.657574653625488, Loss D: 0.14073386788368225\n",
      "Epoch 4/200, Batch 11/17, Loss G: 6.612052917480469, Loss D: 0.1556241363286972\n",
      "Epoch 4/200, Batch 12/17, Loss G: 6.63533878326416, Loss D: 0.15055814385414124\n",
      "Epoch 4/200, Batch 13/17, Loss G: 6.5992326736450195, Loss D: 0.1641901731491089\n",
      "Epoch 4/200, Batch 14/17, Loss G: 6.673418998718262, Loss D: 0.13708238303661346\n",
      "Epoch 4/200, Batch 15/17, Loss G: 6.425980567932129, Loss D: 0.10141594707965851\n",
      "Epoch 4/200, Batch 16/17, Loss G: 6.621557235717773, Loss D: 0.1477770060300827\n",
      "Epoch 5/200, Batch 0/17, Loss G: 6.532246112823486, Loss D: 0.16589941084384918\n",
      "Epoch 5/200, Batch 1/17, Loss G: 6.5038909912109375, Loss D: 0.1687929928302765\n",
      "Epoch 5/200, Batch 2/17, Loss G: 6.611428260803223, Loss D: 0.10736977308988571\n",
      "Epoch 5/200, Batch 3/17, Loss G: 6.388555526733398, Loss D: 0.10568434000015259\n",
      "Epoch 5/200, Batch 4/17, Loss G: 6.6469550132751465, Loss D: 0.20950119197368622\n",
      "Epoch 5/200, Batch 5/17, Loss G: 6.649025917053223, Loss D: 0.24041065573692322\n",
      "Epoch 5/200, Batch 6/17, Loss G: 7.004345417022705, Loss D: 0.1032165065407753\n",
      "Epoch 5/200, Batch 7/17, Loss G: 6.365967750549316, Loss D: 0.15212535858154297\n",
      "Epoch 5/200, Batch 8/17, Loss G: 6.607196807861328, Loss D: 0.14788475632667542\n",
      "Epoch 5/200, Batch 9/17, Loss G: 6.42279052734375, Loss D: 0.12121768295764923\n",
      "Epoch 5/200, Batch 10/17, Loss G: 6.561668395996094, Loss D: 0.11645583063364029\n",
      "Epoch 5/200, Batch 11/17, Loss G: 6.175321578979492, Loss D: 0.12042360752820969\n",
      "Epoch 5/200, Batch 12/17, Loss G: 6.532140254974365, Loss D: 0.10361133515834808\n",
      "Epoch 5/200, Batch 13/17, Loss G: 6.509149074554443, Loss D: 0.13783198595046997\n",
      "Epoch 5/200, Batch 14/17, Loss G: 5.844719886779785, Loss D: 0.18736255168914795\n",
      "Epoch 5/200, Batch 15/17, Loss G: 7.042240142822266, Loss D: 0.12646934390068054\n",
      "Epoch 5/200, Batch 16/17, Loss G: 7.289809226989746, Loss D: 0.13399982452392578\n",
      "Epoch 6/200, Batch 0/17, Loss G: 7.393339157104492, Loss D: 0.09064660966396332\n",
      "Epoch 6/200, Batch 1/17, Loss G: 6.937779426574707, Loss D: 0.05280352383852005\n",
      "Epoch 6/200, Batch 2/17, Loss G: 7.065901279449463, Loss D: 0.057961784303188324\n",
      "Epoch 6/200, Batch 3/17, Loss G: 6.823640823364258, Loss D: 0.06653944402933121\n",
      "Epoch 6/200, Batch 4/17, Loss G: 6.48579216003418, Loss D: 0.11665807664394379\n",
      "Epoch 6/200, Batch 5/17, Loss G: 6.863618850708008, Loss D: 0.10148494690656662\n",
      "Epoch 6/200, Batch 6/17, Loss G: 6.808491230010986, Loss D: 0.09058241546154022\n",
      "Epoch 6/200, Batch 7/17, Loss G: 6.659656524658203, Loss D: 0.09223373234272003\n",
      "Epoch 6/200, Batch 8/17, Loss G: 6.609498023986816, Loss D: 0.07850103825330734\n",
      "Epoch 6/200, Batch 9/17, Loss G: 6.585932731628418, Loss D: 0.05833026394248009\n",
      "Epoch 6/200, Batch 10/17, Loss G: 6.706081390380859, Loss D: 0.061627767980098724\n",
      "Epoch 6/200, Batch 11/17, Loss G: 6.5542755126953125, Loss D: 0.06927315890789032\n",
      "Epoch 6/200, Batch 12/17, Loss G: 6.844601631164551, Loss D: 0.0968656837940216\n",
      "Epoch 6/200, Batch 13/17, Loss G: 6.786201477050781, Loss D: 0.15990296006202698\n",
      "Epoch 6/200, Batch 14/17, Loss G: 6.7020649909973145, Loss D: 0.12483642995357513\n",
      "Epoch 6/200, Batch 15/17, Loss G: 6.723947525024414, Loss D: 0.18261949717998505\n",
      "Epoch 6/200, Batch 16/17, Loss G: 5.829155445098877, Loss D: 0.15577229857444763\n",
      "Epoch 7/200, Batch 0/17, Loss G: 6.857804298400879, Loss D: 0.038078673183918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Batch 1/17, Loss G: 6.432782173156738, Loss D: 0.06283333152532578\n",
      "Epoch 7/200, Batch 2/17, Loss G: 6.343683242797852, Loss D: 0.09160955250263214\n",
      "Epoch 7/200, Batch 3/17, Loss G: 6.814825057983398, Loss D: 0.1115419790148735\n",
      "Epoch 7/200, Batch 4/17, Loss G: 6.3721923828125, Loss D: 0.11993217468261719\n",
      "Epoch 7/200, Batch 5/17, Loss G: 6.58156681060791, Loss D: 0.1399574726819992\n",
      "Epoch 7/200, Batch 6/17, Loss G: 6.355290412902832, Loss D: 0.13299697637557983\n",
      "Epoch 7/200, Batch 7/17, Loss G: 6.118590354919434, Loss D: 0.16448067128658295\n",
      "Epoch 7/200, Batch 8/17, Loss G: 6.687137126922607, Loss D: 0.12190774828195572\n",
      "Epoch 7/200, Batch 9/17, Loss G: 6.188803672790527, Loss D: 0.09463074803352356\n",
      "Epoch 7/200, Batch 10/17, Loss G: 6.4184746742248535, Loss D: 0.15777212381362915\n",
      "Epoch 7/200, Batch 11/17, Loss G: 6.660912990570068, Loss D: 0.24760396778583527\n",
      "Epoch 7/200, Batch 12/17, Loss G: 6.237063407897949, Loss D: 0.07776187360286713\n",
      "Epoch 7/200, Batch 13/17, Loss G: 6.107056617736816, Loss D: 0.11558794975280762\n",
      "Epoch 7/200, Batch 14/17, Loss G: 6.328856468200684, Loss D: 0.2132555991411209\n",
      "Epoch 7/200, Batch 15/17, Loss G: 6.096147537231445, Loss D: 0.13081681728363037\n",
      "Epoch 7/200, Batch 16/17, Loss G: 6.249752998352051, Loss D: 0.13927356898784637\n",
      "Epoch 8/200, Batch 0/17, Loss G: 6.497979164123535, Loss D: 0.076712466776371\n",
      "Epoch 8/200, Batch 1/17, Loss G: 6.064340591430664, Loss D: 0.07870657742023468\n",
      "Epoch 8/200, Batch 2/17, Loss G: 6.4950151443481445, Loss D: 0.07953044772148132\n",
      "Epoch 8/200, Batch 3/17, Loss G: 6.472879409790039, Loss D: 0.0599108561873436\n",
      "Epoch 8/200, Batch 4/17, Loss G: 6.436502933502197, Loss D: 0.09667018800973892\n",
      "Epoch 8/200, Batch 5/17, Loss G: 6.495068550109863, Loss D: 0.16675156354904175\n",
      "Epoch 8/200, Batch 6/17, Loss G: 6.338495254516602, Loss D: 0.0869942158460617\n",
      "Epoch 8/200, Batch 7/17, Loss G: 5.956103801727295, Loss D: 0.09610158950090408\n",
      "Epoch 8/200, Batch 8/17, Loss G: 6.531978607177734, Loss D: 0.06925953924655914\n",
      "Epoch 8/200, Batch 9/17, Loss G: 6.426873207092285, Loss D: 0.15241079032421112\n",
      "Epoch 8/200, Batch 10/17, Loss G: 6.648787021636963, Loss D: 0.05686889961361885\n",
      "Epoch 8/200, Batch 11/17, Loss G: 6.218440055847168, Loss D: 0.07566160708665848\n",
      "Epoch 8/200, Batch 12/17, Loss G: 6.466022968292236, Loss D: 0.06152395159006119\n",
      "Epoch 8/200, Batch 13/17, Loss G: 6.216228485107422, Loss D: 0.13585621118545532\n",
      "Epoch 8/200, Batch 14/17, Loss G: 6.10286283493042, Loss D: 0.21911337971687317\n",
      "Epoch 8/200, Batch 15/17, Loss G: 5.896167755126953, Loss D: 0.16455620527267456\n",
      "Epoch 8/200, Batch 16/17, Loss G: 6.1331095695495605, Loss D: 0.1927412748336792\n",
      "Epoch 9/200, Batch 0/17, Loss G: 6.076456069946289, Loss D: 0.16933757066726685\n",
      "Epoch 9/200, Batch 1/17, Loss G: 6.841282844543457, Loss D: 0.12160789221525192\n",
      "Epoch 9/200, Batch 2/17, Loss G: 6.211674690246582, Loss D: 0.059868596494197845\n",
      "Epoch 9/200, Batch 3/17, Loss G: 6.026872634887695, Loss D: 0.07576882839202881\n",
      "Epoch 9/200, Batch 4/17, Loss G: 6.005620002746582, Loss D: 0.08819526433944702\n",
      "Epoch 9/200, Batch 5/17, Loss G: 6.032258987426758, Loss D: 0.2343044877052307\n",
      "Epoch 9/200, Batch 6/17, Loss G: 6.397840976715088, Loss D: 0.132146418094635\n",
      "Epoch 9/200, Batch 7/17, Loss G: 6.283731460571289, Loss D: 0.07346637547016144\n",
      "Epoch 9/200, Batch 8/17, Loss G: 6.257997035980225, Loss D: 0.04977669566869736\n",
      "Epoch 9/200, Batch 9/17, Loss G: 5.821123123168945, Loss D: 0.08766048401594162\n",
      "Epoch 9/200, Batch 10/17, Loss G: 6.5649309158325195, Loss D: 0.06971803307533264\n",
      "Epoch 9/200, Batch 11/17, Loss G: 6.416416168212891, Loss D: 0.09236344695091248\n",
      "Epoch 9/200, Batch 12/17, Loss G: 5.919861316680908, Loss D: 0.08041296899318695\n",
      "Epoch 9/200, Batch 13/17, Loss G: 6.137840270996094, Loss D: 0.08551924675703049\n",
      "Epoch 9/200, Batch 14/17, Loss G: 6.222028732299805, Loss D: 0.03622662276029587\n",
      "Epoch 9/200, Batch 15/17, Loss G: 6.03262186050415, Loss D: 0.059099700301885605\n",
      "Epoch 9/200, Batch 16/17, Loss G: 6.419276237487793, Loss D: 0.08973072469234467\n",
      "Epoch 10/200, Batch 0/17, Loss G: 5.846986770629883, Loss D: 0.09649423509836197\n",
      "Epoch 10/200, Batch 1/17, Loss G: 6.433982849121094, Loss D: 0.12437321990728378\n",
      "Epoch 10/200, Batch 2/17, Loss G: 6.191368103027344, Loss D: 0.05983328819274902\n",
      "Epoch 10/200, Batch 3/17, Loss G: 6.080132961273193, Loss D: 0.07680151611566544\n",
      "Epoch 10/200, Batch 4/17, Loss G: 5.888396739959717, Loss D: 0.09492182731628418\n",
      "Epoch 10/200, Batch 5/17, Loss G: 6.161142349243164, Loss D: 0.11911870539188385\n",
      "Epoch 10/200, Batch 6/17, Loss G: 5.804466247558594, Loss D: 0.16228994727134705\n",
      "Epoch 10/200, Batch 7/17, Loss G: 6.248507976531982, Loss D: 0.2649421989917755\n",
      "Epoch 10/200, Batch 8/17, Loss G: 6.070632457733154, Loss D: 0.06717811524868011\n",
      "Epoch 10/200, Batch 9/17, Loss G: 5.8476972579956055, Loss D: 0.11865266412496567\n",
      "Epoch 10/200, Batch 10/17, Loss G: 6.166850566864014, Loss D: 0.1103883758187294\n",
      "Epoch 10/200, Batch 11/17, Loss G: 6.329195022583008, Loss D: 0.06924155354499817\n",
      "Epoch 10/200, Batch 12/17, Loss G: 5.859628677368164, Loss D: 0.0642276331782341\n",
      "Epoch 10/200, Batch 13/17, Loss G: 5.979972839355469, Loss D: 0.12495234608650208\n",
      "Epoch 10/200, Batch 14/17, Loss G: 6.229568004608154, Loss D: 0.10262436419725418\n",
      "Epoch 10/200, Batch 15/17, Loss G: 5.740300178527832, Loss D: 0.09899595379829407\n",
      "Epoch 10/200, Batch 16/17, Loss G: 6.042015075683594, Loss D: 0.08814838528633118\n",
      "Epoch 11/200, Batch 0/17, Loss G: 6.163546562194824, Loss D: 0.09316787868738174\n",
      "Epoch 11/200, Batch 1/17, Loss G: 5.773562431335449, Loss D: 0.13186928629875183\n",
      "Epoch 11/200, Batch 2/17, Loss G: 6.199213027954102, Loss D: 0.13143247365951538\n",
      "Epoch 11/200, Batch 3/17, Loss G: 6.057407379150391, Loss D: 0.05861058086156845\n",
      "Epoch 11/200, Batch 4/17, Loss G: 5.809199333190918, Loss D: 0.10609413683414459\n",
      "Epoch 11/200, Batch 5/17, Loss G: 6.0831804275512695, Loss D: 0.1405942142009735\n",
      "Epoch 11/200, Batch 6/17, Loss G: 5.499899864196777, Loss D: 0.13530102372169495\n",
      "Epoch 11/200, Batch 7/17, Loss G: 6.110067844390869, Loss D: 0.04199500009417534\n",
      "Epoch 11/200, Batch 8/17, Loss G: 6.138778209686279, Loss D: 0.07942959666252136\n",
      "Epoch 11/200, Batch 9/17, Loss G: 6.0195393562316895, Loss D: 0.054239191114902496\n",
      "Epoch 11/200, Batch 10/17, Loss G: 5.746870994567871, Loss D: 0.06842901557683945\n",
      "Epoch 11/200, Batch 11/17, Loss G: 6.250988960266113, Loss D: 0.052231624722480774\n",
      "Epoch 11/200, Batch 12/17, Loss G: 5.863199234008789, Loss D: 0.04260505363345146\n",
      "Epoch 11/200, Batch 13/17, Loss G: 5.747188568115234, Loss D: 0.0651116669178009\n",
      "Epoch 11/200, Batch 14/17, Loss G: 5.817707538604736, Loss D: 0.12229864299297333\n",
      "Epoch 11/200, Batch 15/17, Loss G: 6.449072360992432, Loss D: 0.18525813519954681\n",
      "Epoch 11/200, Batch 16/17, Loss G: 6.4016618728637695, Loss D: 0.05787624418735504\n",
      "Epoch 12/200, Batch 0/17, Loss G: 5.692861557006836, Loss D: 0.1427014321088791\n",
      "Epoch 12/200, Batch 1/17, Loss G: 6.1618194580078125, Loss D: 0.22496947646141052\n",
      "Epoch 12/200, Batch 2/17, Loss G: 6.061931610107422, Loss D: 0.12648867070674896\n",
      "Epoch 12/200, Batch 3/17, Loss G: 5.456931114196777, Loss D: 0.15325315296649933\n",
      "Epoch 12/200, Batch 4/17, Loss G: 5.9476189613342285, Loss D: 0.10839270055294037\n",
      "Epoch 12/200, Batch 5/17, Loss G: 5.776004791259766, Loss D: 0.09145063161849976\n",
      "Epoch 12/200, Batch 6/17, Loss G: 6.016180992126465, Loss D: 0.07079949975013733\n",
      "Epoch 12/200, Batch 7/17, Loss G: 5.6533660888671875, Loss D: 0.05239848047494888\n",
      "Epoch 12/200, Batch 8/17, Loss G: 5.76143217086792, Loss D: 0.06358279287815094\n",
      "Epoch 12/200, Batch 9/17, Loss G: 5.707656383514404, Loss D: 0.05488809198141098\n",
      "Epoch 12/200, Batch 10/17, Loss G: 6.14192533493042, Loss D: 0.09719939529895782\n",
      "Epoch 12/200, Batch 11/17, Loss G: 5.736517906188965, Loss D: 0.10500522702932358\n",
      "Epoch 12/200, Batch 12/17, Loss G: 5.705929756164551, Loss D: 0.20834212005138397\n",
      "Epoch 12/200, Batch 13/17, Loss G: 6.176369667053223, Loss D: 0.07487742602825165\n",
      "Epoch 12/200, Batch 14/17, Loss G: 5.752463340759277, Loss D: 0.09614045917987823\n",
      "Epoch 12/200, Batch 15/17, Loss G: 6.077544212341309, Loss D: 0.06629685312509537\n",
      "Epoch 12/200, Batch 16/17, Loss G: 6.196019172668457, Loss D: 0.09597978740930557\n",
      "Epoch 13/200, Batch 0/17, Loss G: 5.746691703796387, Loss D: 0.09197326004505157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Batch 1/17, Loss G: 5.402318000793457, Loss D: 0.1356886923313141\n",
      "Epoch 13/200, Batch 2/17, Loss G: 6.151425361633301, Loss D: 0.032498106360435486\n",
      "Epoch 13/200, Batch 3/17, Loss G: 5.6973466873168945, Loss D: 0.07681769877672195\n",
      "Epoch 13/200, Batch 4/17, Loss G: 5.655092239379883, Loss D: 0.06450702995061874\n",
      "Epoch 13/200, Batch 5/17, Loss G: 5.887468338012695, Loss D: 0.0416824072599411\n",
      "Epoch 13/200, Batch 6/17, Loss G: 5.93261194229126, Loss D: 0.05630677938461304\n",
      "Epoch 13/200, Batch 7/17, Loss G: 5.813591003417969, Loss D: 0.07484953850507736\n",
      "Epoch 13/200, Batch 8/17, Loss G: 5.486230850219727, Loss D: 0.12387014180421829\n",
      "Epoch 13/200, Batch 9/17, Loss G: 6.343947410583496, Loss D: 0.04154100641608238\n",
      "Epoch 13/200, Batch 10/17, Loss G: 6.136608123779297, Loss D: 0.14904572069644928\n",
      "Epoch 13/200, Batch 11/17, Loss G: 5.536066055297852, Loss D: 0.2080211341381073\n",
      "Epoch 13/200, Batch 12/17, Loss G: 6.230137825012207, Loss D: 0.11286080628633499\n",
      "Epoch 13/200, Batch 13/17, Loss G: 5.948800563812256, Loss D: 0.08297998458147049\n",
      "Epoch 13/200, Batch 14/17, Loss G: 5.49257755279541, Loss D: 0.16666103899478912\n",
      "Epoch 13/200, Batch 15/17, Loss G: 6.195605278015137, Loss D: 0.053155723959207535\n",
      "Epoch 13/200, Batch 16/17, Loss G: 5.990843296051025, Loss D: 0.1322854906320572\n",
      "Epoch 14/200, Batch 0/17, Loss G: 5.3616042137146, Loss D: 0.13635170459747314\n",
      "Epoch 14/200, Batch 1/17, Loss G: 5.7378740310668945, Loss D: 0.03958503529429436\n",
      "Epoch 14/200, Batch 2/17, Loss G: 5.928878307342529, Loss D: 0.09223809838294983\n",
      "Epoch 14/200, Batch 3/17, Loss G: 5.615660667419434, Loss D: 0.05078461021184921\n",
      "Epoch 14/200, Batch 4/17, Loss G: 5.452420234680176, Loss D: 0.0798032134771347\n",
      "Epoch 14/200, Batch 5/17, Loss G: 5.823941230773926, Loss D: 0.08623382449150085\n",
      "Epoch 14/200, Batch 6/17, Loss G: 5.719912528991699, Loss D: 0.09681017696857452\n",
      "Epoch 14/200, Batch 7/17, Loss G: 6.040851593017578, Loss D: 0.0795823410153389\n",
      "Epoch 14/200, Batch 8/17, Loss G: 5.856464385986328, Loss D: 0.061583470553159714\n",
      "Epoch 14/200, Batch 9/17, Loss G: 5.909958839416504, Loss D: 0.05059568211436272\n",
      "Epoch 14/200, Batch 10/17, Loss G: 5.875674724578857, Loss D: 0.05590087175369263\n",
      "Epoch 14/200, Batch 11/17, Loss G: 6.133392810821533, Loss D: 0.10471858084201813\n",
      "Epoch 14/200, Batch 12/17, Loss G: 5.797012805938721, Loss D: 0.06719744205474854\n",
      "Epoch 14/200, Batch 13/17, Loss G: 5.697470664978027, Loss D: 0.09166066348552704\n",
      "Epoch 14/200, Batch 14/17, Loss G: 5.908510208129883, Loss D: 0.17029958963394165\n",
      "Epoch 14/200, Batch 15/17, Loss G: 5.357126235961914, Loss D: 0.24753570556640625\n",
      "Epoch 14/200, Batch 16/17, Loss G: 5.551966667175293, Loss D: 0.1606791615486145\n",
      "Epoch 15/200, Batch 0/17, Loss G: 5.490497589111328, Loss D: 0.10916075110435486\n",
      "Epoch 15/200, Batch 1/17, Loss G: 5.547304153442383, Loss D: 0.11829632520675659\n",
      "Epoch 15/200, Batch 2/17, Loss G: 5.702589511871338, Loss D: 0.09252569079399109\n",
      "Epoch 15/200, Batch 3/17, Loss G: 5.478639602661133, Loss D: 0.07465001195669174\n",
      "Epoch 15/200, Batch 4/17, Loss G: 5.874618053436279, Loss D: 0.07453582435846329\n",
      "Epoch 15/200, Batch 5/17, Loss G: 5.832704067230225, Loss D: 0.07037746161222458\n",
      "Epoch 15/200, Batch 6/17, Loss G: 5.504444122314453, Loss D: 0.09047861397266388\n",
      "Epoch 15/200, Batch 7/17, Loss G: 5.98577880859375, Loss D: 0.11490282416343689\n",
      "Epoch 15/200, Batch 8/17, Loss G: 5.740863800048828, Loss D: 0.05917789787054062\n",
      "Epoch 15/200, Batch 9/17, Loss G: 5.405738353729248, Loss D: 0.06817781925201416\n",
      "Epoch 15/200, Batch 10/17, Loss G: 5.869967460632324, Loss D: 0.07222524285316467\n",
      "Epoch 15/200, Batch 11/17, Loss G: 5.98876953125, Loss D: 0.15342053771018982\n",
      "Epoch 15/200, Batch 12/17, Loss G: 5.566812038421631, Loss D: 0.08037146925926208\n",
      "Epoch 15/200, Batch 13/17, Loss G: 5.695644378662109, Loss D: 0.07052003592252731\n",
      "Epoch 15/200, Batch 14/17, Loss G: 5.704537868499756, Loss D: 0.045751847326755524\n",
      "Epoch 15/200, Batch 15/17, Loss G: 5.602535247802734, Loss D: 0.06139020621776581\n",
      "Epoch 15/200, Batch 16/17, Loss G: 5.223323822021484, Loss D: 0.09210677444934845\n",
      "Epoch 16/200, Batch 0/17, Loss G: 5.43458366394043, Loss D: 0.13224035501480103\n",
      "Epoch 16/200, Batch 1/17, Loss G: 6.281970977783203, Loss D: 0.11606545746326447\n",
      "Epoch 16/200, Batch 2/17, Loss G: 5.947287559509277, Loss D: 0.03419656679034233\n",
      "Epoch 16/200, Batch 3/17, Loss G: 5.89853572845459, Loss D: 0.03600504994392395\n",
      "Epoch 16/200, Batch 4/17, Loss G: 5.4390459060668945, Loss D: 0.06538291275501251\n",
      "Epoch 16/200, Batch 5/17, Loss G: 5.8922576904296875, Loss D: 0.040670186281204224\n",
      "Epoch 16/200, Batch 6/17, Loss G: 5.7317962646484375, Loss D: 0.096981480717659\n",
      "Epoch 16/200, Batch 7/17, Loss G: 5.31564998626709, Loss D: 0.15843278169631958\n",
      "Epoch 16/200, Batch 8/17, Loss G: 5.805065155029297, Loss D: 0.2574845850467682\n",
      "Epoch 16/200, Batch 9/17, Loss G: 5.67293119430542, Loss D: 0.051930226385593414\n",
      "Epoch 16/200, Batch 10/17, Loss G: 5.47432804107666, Loss D: 0.17020991444587708\n",
      "Epoch 16/200, Batch 11/17, Loss G: 5.727651596069336, Loss D: 0.11420346796512604\n",
      "Epoch 16/200, Batch 12/17, Loss G: 5.217499732971191, Loss D: 0.12935326993465424\n",
      "Epoch 16/200, Batch 13/17, Loss G: 6.1278076171875, Loss D: 0.14885146915912628\n",
      "Epoch 16/200, Batch 14/17, Loss G: 5.506561279296875, Loss D: 0.09348104894161224\n",
      "Epoch 16/200, Batch 15/17, Loss G: 5.742330551147461, Loss D: 0.054433293640613556\n",
      "Epoch 16/200, Batch 16/17, Loss G: 5.7567548751831055, Loss D: 0.06506109237670898\n",
      "Epoch 17/200, Batch 0/17, Loss G: 5.458988666534424, Loss D: 0.10373345017433167\n",
      "Epoch 17/200, Batch 1/17, Loss G: 5.482832908630371, Loss D: 0.1196267232298851\n",
      "Epoch 17/200, Batch 2/17, Loss G: 5.520926475524902, Loss D: 0.09271091967821121\n",
      "Epoch 17/200, Batch 3/17, Loss G: 5.4397711753845215, Loss D: 0.1348632127046585\n",
      "Epoch 17/200, Batch 4/17, Loss G: 5.197641372680664, Loss D: 0.1191767156124115\n",
      "Epoch 17/200, Batch 5/17, Loss G: 5.686802864074707, Loss D: 0.04886522889137268\n",
      "Epoch 17/200, Batch 6/17, Loss G: 5.856540679931641, Loss D: 0.04379361867904663\n",
      "Epoch 17/200, Batch 7/17, Loss G: 5.478341102600098, Loss D: 0.05933281034231186\n",
      "Epoch 17/200, Batch 8/17, Loss G: 5.320589065551758, Loss D: 0.1327567845582962\n",
      "Epoch 17/200, Batch 9/17, Loss G: 6.4178619384765625, Loss D: 0.11894120275974274\n",
      "Epoch 17/200, Batch 10/17, Loss G: 5.982232093811035, Loss D: 0.05035475641489029\n",
      "Epoch 17/200, Batch 11/17, Loss G: 5.842202663421631, Loss D: 0.029726358130574226\n",
      "Epoch 17/200, Batch 12/17, Loss G: 5.848577499389648, Loss D: 0.018907781690359116\n",
      "Epoch 17/200, Batch 13/17, Loss G: 5.9705657958984375, Loss D: 0.03407782316207886\n",
      "Epoch 17/200, Batch 14/17, Loss G: 5.309848785400391, Loss D: 0.18437203764915466\n",
      "Epoch 17/200, Batch 15/17, Loss G: 6.213444709777832, Loss D: 0.18130479753017426\n",
      "Epoch 17/200, Batch 16/17, Loss G: 5.710868835449219, Loss D: 0.11300966143608093\n",
      "Epoch 18/200, Batch 0/17, Loss G: 5.537393569946289, Loss D: 0.17383518815040588\n",
      "Epoch 18/200, Batch 1/17, Loss G: 5.807575702667236, Loss D: 0.08107934147119522\n",
      "Epoch 18/200, Batch 2/17, Loss G: 5.715130805969238, Loss D: 0.07998307794332504\n",
      "Epoch 18/200, Batch 3/17, Loss G: 5.254876136779785, Loss D: 0.16198189556598663\n",
      "Epoch 18/200, Batch 4/17, Loss G: 5.569633483886719, Loss D: 0.14965194463729858\n",
      "Epoch 18/200, Batch 5/17, Loss G: 5.344799995422363, Loss D: 0.05276940017938614\n",
      "Epoch 18/200, Batch 6/17, Loss G: 5.149096488952637, Loss D: 0.1329907476902008\n",
      "Epoch 18/200, Batch 7/17, Loss G: 5.726972579956055, Loss D: 0.1372925043106079\n",
      "Epoch 18/200, Batch 8/17, Loss G: 5.355415344238281, Loss D: 0.04552941769361496\n",
      "Epoch 18/200, Batch 9/17, Loss G: 5.243432998657227, Loss D: 0.119911327958107\n",
      "Epoch 18/200, Batch 10/17, Loss G: 5.738361358642578, Loss D: 0.09666655212640762\n",
      "Epoch 18/200, Batch 11/17, Loss G: 5.532008171081543, Loss D: 0.04313475638628006\n",
      "Epoch 18/200, Batch 12/17, Loss G: 5.403683662414551, Loss D: 0.07285432517528534\n",
      "Epoch 18/200, Batch 13/17, Loss G: 5.513985633850098, Loss D: 0.1162601187825203\n",
      "Epoch 18/200, Batch 14/17, Loss G: 5.4098358154296875, Loss D: 0.08123797178268433\n",
      "Epoch 18/200, Batch 15/17, Loss G: 5.679551124572754, Loss D: 0.06552541255950928\n",
      "Epoch 18/200, Batch 16/17, Loss G: 5.813101768493652, Loss D: 0.06060784310102463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Batch 0/17, Loss G: 5.202698707580566, Loss D: 0.11549351364374161\n",
      "Epoch 19/200, Batch 1/17, Loss G: 5.72239351272583, Loss D: 0.07175226509571075\n",
      "Epoch 19/200, Batch 2/17, Loss G: 5.584817886352539, Loss D: 0.031313762068748474\n",
      "Epoch 19/200, Batch 3/17, Loss G: 5.622849464416504, Loss D: 0.02048611454665661\n",
      "Epoch 19/200, Batch 4/17, Loss G: 5.316562652587891, Loss D: 0.04540642723441124\n",
      "Epoch 19/200, Batch 5/17, Loss G: 5.4059858322143555, Loss D: 0.0681653693318367\n",
      "Epoch 19/200, Batch 6/17, Loss G: 5.6468000411987305, Loss D: 0.09095162898302078\n",
      "Epoch 19/200, Batch 7/17, Loss G: 5.202019691467285, Loss D: 0.1755920946598053\n",
      "Epoch 19/200, Batch 8/17, Loss G: 5.995543479919434, Loss D: 0.15092217922210693\n",
      "Epoch 19/200, Batch 9/17, Loss G: 5.239152908325195, Loss D: 0.07461901754140854\n",
      "Epoch 19/200, Batch 10/17, Loss G: 5.350446701049805, Loss D: 0.17046640813350677\n",
      "Epoch 19/200, Batch 11/17, Loss G: 5.950477600097656, Loss D: 0.13114793598651886\n",
      "Epoch 19/200, Batch 12/17, Loss G: 5.746513366699219, Loss D: 0.11005737632513046\n",
      "Epoch 19/200, Batch 13/17, Loss G: 5.755765914916992, Loss D: 0.0706416517496109\n",
      "Epoch 19/200, Batch 14/17, Loss G: 4.867269515991211, Loss D: 0.12531644105911255\n",
      "Epoch 19/200, Batch 15/17, Loss G: 5.46721076965332, Loss D: 0.12314918637275696\n",
      "Epoch 19/200, Batch 16/17, Loss G: 5.643882751464844, Loss D: 0.2326248586177826\n",
      "Epoch 20/200, Batch 0/17, Loss G: 5.185400009155273, Loss D: 0.19944313168525696\n",
      "Epoch 20/200, Batch 1/17, Loss G: 5.3884358406066895, Loss D: 0.05717551335692406\n",
      "Epoch 20/200, Batch 2/17, Loss G: 5.674062728881836, Loss D: 0.04085727035999298\n",
      "Epoch 20/200, Batch 3/17, Loss G: 5.651848793029785, Loss D: 0.06279511749744415\n",
      "Epoch 20/200, Batch 4/17, Loss G: 5.380429267883301, Loss D: 0.06755030155181885\n",
      "Epoch 20/200, Batch 5/17, Loss G: 5.278037071228027, Loss D: 0.08536800742149353\n",
      "Epoch 20/200, Batch 6/17, Loss G: 5.216476917266846, Loss D: 0.07438461482524872\n",
      "Epoch 20/200, Batch 7/17, Loss G: 5.8588104248046875, Loss D: 0.04133402928709984\n",
      "Epoch 20/200, Batch 8/17, Loss G: 5.228475570678711, Loss D: 0.12027435004711151\n",
      "Epoch 20/200, Batch 9/17, Loss G: 4.898782730102539, Loss D: 0.24988771975040436\n",
      "Epoch 20/200, Batch 10/17, Loss G: 5.405094146728516, Loss D: 0.14685440063476562\n",
      "Epoch 20/200, Batch 11/17, Loss G: 5.810578346252441, Loss D: 0.037062130868434906\n",
      "Epoch 20/200, Batch 12/17, Loss G: 5.261568069458008, Loss D: 0.08499748259782791\n",
      "Epoch 20/200, Batch 13/17, Loss G: 5.615393161773682, Loss D: 0.056786321103572845\n",
      "Epoch 20/200, Batch 14/17, Loss G: 5.702632904052734, Loss D: 0.05552564933896065\n",
      "Epoch 20/200, Batch 15/17, Loss G: 5.57975959777832, Loss D: 0.0331091545522213\n",
      "Epoch 20/200, Batch 16/17, Loss G: 5.419197082519531, Loss D: 0.02912062406539917\n",
      "Epoch 21/200, Batch 0/17, Loss G: 5.574997901916504, Loss D: 0.07553894817829132\n",
      "Epoch 21/200, Batch 1/17, Loss G: 5.089682579040527, Loss D: 0.13216067850589752\n",
      "Epoch 21/200, Batch 2/17, Loss G: 5.74935245513916, Loss D: 0.17318002879619598\n",
      "Epoch 21/200, Batch 3/17, Loss G: 5.520529747009277, Loss D: 0.03451882302761078\n",
      "Epoch 21/200, Batch 4/17, Loss G: 5.415159225463867, Loss D: 0.02439112216234207\n",
      "Epoch 21/200, Batch 5/17, Loss G: 5.43086051940918, Loss D: 0.01989816129207611\n",
      "Epoch 21/200, Batch 6/17, Loss G: 5.4190993309021, Loss D: 0.05079524964094162\n",
      "Epoch 21/200, Batch 7/17, Loss G: 5.4938435554504395, Loss D: 0.04604474455118179\n",
      "Epoch 21/200, Batch 8/17, Loss G: 5.206203460693359, Loss D: 0.0738712027668953\n",
      "Epoch 21/200, Batch 9/17, Loss G: 5.486958980560303, Loss D: 0.049602705985307693\n",
      "Epoch 21/200, Batch 10/17, Loss G: 5.440817832946777, Loss D: 0.03374304249882698\n",
      "Epoch 21/200, Batch 11/17, Loss G: 5.479592800140381, Loss D: 0.13734160363674164\n",
      "Epoch 21/200, Batch 12/17, Loss G: 5.070821285247803, Loss D: 0.159762442111969\n",
      "Epoch 21/200, Batch 13/17, Loss G: 5.683413028717041, Loss D: 0.1406586766242981\n",
      "Epoch 21/200, Batch 14/17, Loss G: 5.450876235961914, Loss D: 0.033930450677871704\n",
      "Epoch 21/200, Batch 15/17, Loss G: 4.994602203369141, Loss D: 0.09426188468933105\n",
      "Epoch 21/200, Batch 16/17, Loss G: 5.326240539550781, Loss D: 0.09538287669420242\n",
      "Epoch 22/200, Batch 0/17, Loss G: 5.249143600463867, Loss D: 0.12857317924499512\n",
      "Epoch 22/200, Batch 1/17, Loss G: 5.667694568634033, Loss D: 0.16435767710208893\n",
      "Epoch 22/200, Batch 2/17, Loss G: 5.1282854080200195, Loss D: 0.1398361474275589\n",
      "Epoch 22/200, Batch 3/17, Loss G: 5.19716739654541, Loss D: 0.11743485182523727\n",
      "Epoch 22/200, Batch 4/17, Loss G: 6.205287933349609, Loss D: 0.13644228875637054\n",
      "Epoch 22/200, Batch 5/17, Loss G: 5.593589782714844, Loss D: 0.03602986037731171\n",
      "Epoch 22/200, Batch 6/17, Loss G: 5.88315486907959, Loss D: 0.03953786566853523\n",
      "Epoch 22/200, Batch 7/17, Loss G: 5.466512680053711, Loss D: 0.03926762193441391\n",
      "Epoch 22/200, Batch 8/17, Loss G: 5.198868751525879, Loss D: 0.11889362335205078\n",
      "Epoch 22/200, Batch 9/17, Loss G: 5.483063220977783, Loss D: 0.18537314236164093\n",
      "Epoch 22/200, Batch 10/17, Loss G: 5.196197509765625, Loss D: 0.09001575410366058\n",
      "Epoch 22/200, Batch 11/17, Loss G: 5.394289970397949, Loss D: 0.0781136006116867\n",
      "Epoch 22/200, Batch 12/17, Loss G: 5.552508354187012, Loss D: 0.06790526211261749\n",
      "Epoch 22/200, Batch 13/17, Loss G: 5.458749771118164, Loss D: 0.07098820805549622\n",
      "Epoch 22/200, Batch 14/17, Loss G: 5.41657829284668, Loss D: 0.08994486927986145\n",
      "Epoch 22/200, Batch 15/17, Loss G: 5.451303958892822, Loss D: 0.06883435696363449\n",
      "Epoch 22/200, Batch 16/17, Loss G: 5.4735107421875, Loss D: 0.05801571160554886\n",
      "Epoch 23/200, Batch 0/17, Loss G: 5.92639684677124, Loss D: 0.05514875799417496\n",
      "Epoch 23/200, Batch 1/17, Loss G: 5.465304374694824, Loss D: 0.04525122046470642\n",
      "Epoch 23/200, Batch 2/17, Loss G: 5.357251167297363, Loss D: 0.07294753193855286\n",
      "Epoch 23/200, Batch 3/17, Loss G: 5.392947673797607, Loss D: 0.07985630631446838\n",
      "Epoch 23/200, Batch 4/17, Loss G: 5.534786224365234, Loss D: 0.03914670646190643\n",
      "Epoch 23/200, Batch 5/17, Loss G: 5.321813583374023, Loss D: 0.024998359382152557\n",
      "Epoch 23/200, Batch 6/17, Loss G: 5.47447395324707, Loss D: 0.028044788166880608\n",
      "Epoch 23/200, Batch 7/17, Loss G: 5.111663818359375, Loss D: 0.24694442749023438\n",
      "Epoch 23/200, Batch 8/17, Loss G: 5.572776794433594, Loss D: 0.13294166326522827\n",
      "Epoch 23/200, Batch 9/17, Loss G: 5.201178550720215, Loss D: 0.10428039729595184\n",
      "Epoch 23/200, Batch 10/17, Loss G: 5.286104202270508, Loss D: 0.0875765010714531\n",
      "Epoch 23/200, Batch 11/17, Loss G: 5.455354690551758, Loss D: 0.15220284461975098\n",
      "Epoch 23/200, Batch 12/17, Loss G: 5.402843475341797, Loss D: 0.09146904945373535\n",
      "Epoch 23/200, Batch 13/17, Loss G: 5.557246208190918, Loss D: 0.0694245845079422\n",
      "Epoch 23/200, Batch 14/17, Loss G: 5.098997116088867, Loss D: 0.1125403642654419\n",
      "Epoch 23/200, Batch 15/17, Loss G: 5.5218682289123535, Loss D: 0.06345552206039429\n",
      "Epoch 23/200, Batch 16/17, Loss G: 5.351955413818359, Loss D: 0.08828862011432648\n",
      "Epoch 24/200, Batch 0/17, Loss G: 5.390230178833008, Loss D: 0.11461101472377777\n",
      "Epoch 24/200, Batch 1/17, Loss G: 5.542546272277832, Loss D: 0.1323796957731247\n",
      "Epoch 24/200, Batch 2/17, Loss G: 5.33232307434082, Loss D: 0.08299381285905838\n",
      "Epoch 24/200, Batch 3/17, Loss G: 5.287461757659912, Loss D: 0.05512525141239166\n",
      "Epoch 24/200, Batch 4/17, Loss G: 5.227363586425781, Loss D: 0.034681402146816254\n",
      "Epoch 24/200, Batch 5/17, Loss G: 5.378965377807617, Loss D: 0.05495421588420868\n",
      "Epoch 24/200, Batch 6/17, Loss G: 5.509971618652344, Loss D: 0.06830070167779922\n",
      "Epoch 24/200, Batch 7/17, Loss G: 5.132635116577148, Loss D: 0.07416842877864838\n",
      "Epoch 24/200, Batch 8/17, Loss G: 5.576169490814209, Loss D: 0.13688616454601288\n",
      "Epoch 24/200, Batch 9/17, Loss G: 5.031707286834717, Loss D: 0.09085142612457275\n",
      "Epoch 24/200, Batch 10/17, Loss G: 5.297090530395508, Loss D: 0.09218670427799225\n",
      "Epoch 24/200, Batch 11/17, Loss G: 5.285861015319824, Loss D: 0.06478087604045868\n",
      "Epoch 24/200, Batch 12/17, Loss G: 5.4520158767700195, Loss D: 0.07651174068450928\n",
      "Epoch 24/200, Batch 13/17, Loss G: 5.680056571960449, Loss D: 0.04306848719716072\n",
      "Epoch 24/200, Batch 14/17, Loss G: 5.369346618652344, Loss D: 0.07732076197862625\n",
      "Epoch 24/200, Batch 15/17, Loss G: 4.8594069480896, Loss D: 0.1257338523864746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Batch 16/17, Loss G: 5.774815559387207, Loss D: 0.05940798670053482\n",
      "Epoch 25/200, Batch 0/17, Loss G: 5.593822479248047, Loss D: 0.08447359502315521\n",
      "Epoch 25/200, Batch 1/17, Loss G: 5.340854644775391, Loss D: 0.03468197584152222\n",
      "Epoch 25/200, Batch 2/17, Loss G: 5.130141735076904, Loss D: 0.053131457418203354\n",
      "Epoch 25/200, Batch 3/17, Loss G: 5.364422798156738, Loss D: 0.0724051371216774\n",
      "Epoch 25/200, Batch 4/17, Loss G: 4.948331832885742, Loss D: 0.11713887006044388\n",
      "Epoch 25/200, Batch 5/17, Loss G: 4.768622398376465, Loss D: 0.23112693428993225\n",
      "Epoch 25/200, Batch 6/17, Loss G: 5.704066276550293, Loss D: 0.09375753253698349\n",
      "Epoch 25/200, Batch 7/17, Loss G: 5.53560733795166, Loss D: 0.0822746604681015\n",
      "Epoch 25/200, Batch 8/17, Loss G: 4.980512619018555, Loss D: 0.07486200332641602\n",
      "Epoch 25/200, Batch 9/17, Loss G: 5.308040618896484, Loss D: 0.02640945464372635\n",
      "Epoch 25/200, Batch 10/17, Loss G: 5.41053581237793, Loss D: 0.03141820430755615\n",
      "Epoch 25/200, Batch 11/17, Loss G: 5.51314640045166, Loss D: 0.0251801498234272\n",
      "Epoch 25/200, Batch 12/17, Loss G: 4.875673294067383, Loss D: 0.15883223712444305\n",
      "Epoch 25/200, Batch 13/17, Loss G: 5.586301803588867, Loss D: 0.17167392373085022\n",
      "Epoch 25/200, Batch 14/17, Loss G: 5.348517417907715, Loss D: 0.030495624989271164\n",
      "Epoch 25/200, Batch 15/17, Loss G: 5.127836227416992, Loss D: 0.04147401079535484\n",
      "Epoch 25/200, Batch 16/17, Loss G: 5.394045829772949, Loss D: 0.041093774139881134\n",
      "Epoch 26/200, Batch 0/17, Loss G: 5.373097896575928, Loss D: 0.12786012887954712\n",
      "Epoch 26/200, Batch 1/17, Loss G: 4.938090801239014, Loss D: 0.09417521953582764\n",
      "Epoch 26/200, Batch 2/17, Loss G: 5.347311496734619, Loss D: 0.06042856723070145\n",
      "Epoch 26/200, Batch 3/17, Loss G: 5.061352729797363, Loss D: 0.07442329823970795\n",
      "Epoch 26/200, Batch 4/17, Loss G: 5.196725845336914, Loss D: 0.16105037927627563\n",
      "Epoch 26/200, Batch 5/17, Loss G: 5.165642261505127, Loss D: 0.16508206725120544\n",
      "Epoch 26/200, Batch 6/17, Loss G: 5.574057102203369, Loss D: 0.11303232610225677\n",
      "Epoch 26/200, Batch 7/17, Loss G: 5.437386989593506, Loss D: 0.05203837901353836\n",
      "Epoch 26/200, Batch 8/17, Loss G: 4.902210235595703, Loss D: 0.07406963407993317\n",
      "Epoch 26/200, Batch 9/17, Loss G: 5.701855659484863, Loss D: 0.042826030403375626\n",
      "Epoch 26/200, Batch 10/17, Loss G: 5.422187805175781, Loss D: 0.17313063144683838\n",
      "Epoch 26/200, Batch 11/17, Loss G: 4.8176116943359375, Loss D: 0.14924278855323792\n",
      "Epoch 26/200, Batch 12/17, Loss G: 5.172992706298828, Loss D: 0.08477522432804108\n",
      "Epoch 26/200, Batch 13/17, Loss G: 5.263318061828613, Loss D: 0.04290317744016647\n",
      "Epoch 26/200, Batch 14/17, Loss G: 4.999349594116211, Loss D: 0.09437432885169983\n",
      "Epoch 26/200, Batch 15/17, Loss G: 5.405769348144531, Loss D: 0.06470219790935516\n",
      "Epoch 26/200, Batch 16/17, Loss G: 5.488072395324707, Loss D: 0.04643356800079346\n",
      "Epoch 27/200, Batch 0/17, Loss G: 5.232487678527832, Loss D: 0.07615705579519272\n",
      "Epoch 27/200, Batch 1/17, Loss G: 5.704741954803467, Loss D: 0.12081441283226013\n",
      "Epoch 27/200, Batch 2/17, Loss G: 5.106647491455078, Loss D: 0.09209954738616943\n",
      "Epoch 27/200, Batch 3/17, Loss G: 5.430398464202881, Loss D: 0.06418478488922119\n",
      "Epoch 27/200, Batch 4/17, Loss G: 5.269598007202148, Loss D: 0.04076535254716873\n",
      "Epoch 27/200, Batch 5/17, Loss G: 4.9428839683532715, Loss D: 0.08286794275045395\n",
      "Epoch 27/200, Batch 6/17, Loss G: 5.216193675994873, Loss D: 0.08482101559638977\n",
      "Epoch 27/200, Batch 7/17, Loss G: 5.377440452575684, Loss D: 0.05722178891301155\n",
      "Epoch 27/200, Batch 8/17, Loss G: 5.214851379394531, Loss D: 0.10428483784198761\n",
      "Epoch 27/200, Batch 9/17, Loss G: 5.0290937423706055, Loss D: 0.08085351437330246\n",
      "Epoch 27/200, Batch 10/17, Loss G: 5.4852399826049805, Loss D: 0.06451411545276642\n",
      "Epoch 27/200, Batch 11/17, Loss G: 5.316771507263184, Loss D: 0.0403556227684021\n",
      "Epoch 27/200, Batch 12/17, Loss G: 5.100275993347168, Loss D: 0.05027926713228226\n",
      "Epoch 27/200, Batch 13/17, Loss G: 5.013581275939941, Loss D: 0.0650508850812912\n",
      "Epoch 27/200, Batch 14/17, Loss G: 5.071983814239502, Loss D: 0.07247240841388702\n",
      "Epoch 27/200, Batch 15/17, Loss G: 5.377679824829102, Loss D: 0.11642292886972427\n",
      "Epoch 27/200, Batch 16/17, Loss G: 5.178752899169922, Loss D: 0.07684563845396042\n",
      "Epoch 28/200, Batch 0/17, Loss G: 5.531500816345215, Loss D: 0.028078246861696243\n",
      "Epoch 28/200, Batch 1/17, Loss G: 5.388972282409668, Loss D: 0.04297709837555885\n",
      "Epoch 28/200, Batch 2/17, Loss G: 5.096850395202637, Loss D: 0.05181484296917915\n",
      "Epoch 28/200, Batch 3/17, Loss G: 5.222324371337891, Loss D: 0.03942672163248062\n",
      "Epoch 28/200, Batch 4/17, Loss G: 5.190579414367676, Loss D: 0.12258335947990417\n",
      "Epoch 28/200, Batch 5/17, Loss G: 4.740034103393555, Loss D: 0.14439527690410614\n",
      "Epoch 28/200, Batch 6/17, Loss G: 5.512094974517822, Loss D: 0.1656823754310608\n",
      "Epoch 28/200, Batch 7/17, Loss G: 5.275974273681641, Loss D: 0.019545067101716995\n",
      "Epoch 28/200, Batch 8/17, Loss G: 5.176074028015137, Loss D: 0.031971316784620285\n",
      "Epoch 28/200, Batch 9/17, Loss G: 4.83913516998291, Loss D: 0.0700773298740387\n",
      "Epoch 28/200, Batch 10/17, Loss G: 5.342047691345215, Loss D: 0.08632286638021469\n",
      "Epoch 28/200, Batch 11/17, Loss G: 5.056047439575195, Loss D: 0.08508968353271484\n",
      "Epoch 28/200, Batch 12/17, Loss G: 5.025664806365967, Loss D: 0.07409121096134186\n",
      "Epoch 28/200, Batch 13/17, Loss G: 5.312094688415527, Loss D: 0.08061457425355911\n",
      "Epoch 28/200, Batch 14/17, Loss G: 4.915839195251465, Loss D: 0.1533062607049942\n",
      "Epoch 28/200, Batch 15/17, Loss G: 5.445025444030762, Loss D: 0.16156135499477386\n",
      "Epoch 28/200, Batch 16/17, Loss G: 5.795212745666504, Loss D: 0.03756164759397507\n",
      "Epoch 29/200, Batch 0/17, Loss G: 4.993178844451904, Loss D: 0.08800951391458511\n",
      "Epoch 29/200, Batch 1/17, Loss G: 5.313589096069336, Loss D: 0.0912228524684906\n",
      "Epoch 29/200, Batch 2/17, Loss G: 5.200708389282227, Loss D: 0.062498584389686584\n",
      "Epoch 29/200, Batch 3/17, Loss G: 5.302607536315918, Loss D: 0.0646679550409317\n",
      "Epoch 29/200, Batch 4/17, Loss G: 5.185480117797852, Loss D: 0.07263224571943283\n",
      "Epoch 29/200, Batch 5/17, Loss G: 4.929497718811035, Loss D: 0.11542297899723053\n",
      "Epoch 29/200, Batch 6/17, Loss G: 5.645781517028809, Loss D: 0.18571393191814423\n",
      "Epoch 29/200, Batch 7/17, Loss G: 5.0834784507751465, Loss D: 0.11442545056343079\n",
      "Epoch 29/200, Batch 8/17, Loss G: 5.318813800811768, Loss D: 0.0727587342262268\n",
      "Epoch 29/200, Batch 9/17, Loss G: 5.215221405029297, Loss D: 0.04213111847639084\n",
      "Epoch 29/200, Batch 10/17, Loss G: 5.141916275024414, Loss D: 0.03173878788948059\n",
      "Epoch 29/200, Batch 11/17, Loss G: 5.239896774291992, Loss D: 0.05232756584882736\n",
      "Epoch 29/200, Batch 12/17, Loss G: 4.881991863250732, Loss D: 0.11070068180561066\n",
      "Epoch 29/200, Batch 13/17, Loss G: 5.265785217285156, Loss D: 0.13756346702575684\n",
      "Epoch 29/200, Batch 14/17, Loss G: 5.15931510925293, Loss D: 0.12439935654401779\n",
      "Epoch 29/200, Batch 15/17, Loss G: 5.573208808898926, Loss D: 0.09921848773956299\n",
      "Epoch 29/200, Batch 16/17, Loss G: 5.437710762023926, Loss D: 0.06004303693771362\n",
      "Epoch 30/200, Batch 0/17, Loss G: 5.455681800842285, Loss D: 0.06289984285831451\n",
      "Epoch 30/200, Batch 1/17, Loss G: 5.185305595397949, Loss D: 0.0541631244122982\n",
      "Epoch 30/200, Batch 2/17, Loss G: 5.0791335105896, Loss D: 0.13011059165000916\n",
      "Epoch 30/200, Batch 3/17, Loss G: 5.288540363311768, Loss D: 0.06976761668920517\n",
      "Epoch 30/200, Batch 4/17, Loss G: 5.176477909088135, Loss D: 0.08849330246448517\n",
      "Epoch 30/200, Batch 5/17, Loss G: 4.86993408203125, Loss D: 0.09856066107749939\n",
      "Epoch 30/200, Batch 6/17, Loss G: 5.341619968414307, Loss D: 0.09644065797328949\n",
      "Epoch 30/200, Batch 7/17, Loss G: 4.8403120040893555, Loss D: 0.10275506228208542\n",
      "Epoch 30/200, Batch 8/17, Loss G: 5.215493202209473, Loss D: 0.0798102468252182\n",
      "Epoch 30/200, Batch 9/17, Loss G: 5.008698463439941, Loss D: 0.05106498301029205\n",
      "Epoch 30/200, Batch 10/17, Loss G: 5.179892539978027, Loss D: 0.02912995219230652\n",
      "Epoch 30/200, Batch 11/17, Loss G: 5.3372697830200195, Loss D: 0.02747027575969696\n",
      "Epoch 30/200, Batch 12/17, Loss G: 5.501712322235107, Loss D: 0.10657051205635071\n",
      "Epoch 30/200, Batch 13/17, Loss G: 4.852358818054199, Loss D: 0.12772096693515778\n",
      "Epoch 30/200, Batch 14/17, Loss G: 5.447226524353027, Loss D: 0.18839611113071442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Batch 15/17, Loss G: 4.912045955657959, Loss D: 0.060863103717565536\n",
      "Epoch 30/200, Batch 16/17, Loss G: 5.20429801940918, Loss D: 0.039657268673181534\n",
      "Epoch 31/200, Batch 0/17, Loss G: 5.066558361053467, Loss D: 0.051348790526390076\n",
      "Epoch 31/200, Batch 1/17, Loss G: 4.961837291717529, Loss D: 0.044273942708969116\n",
      "Epoch 31/200, Batch 2/17, Loss G: 5.110620498657227, Loss D: 0.08697861433029175\n",
      "Epoch 31/200, Batch 3/17, Loss G: 5.258382797241211, Loss D: 0.07048910856246948\n",
      "Epoch 31/200, Batch 4/17, Loss G: 5.028102874755859, Loss D: 0.06516862660646439\n",
      "Epoch 31/200, Batch 5/17, Loss G: 5.199514389038086, Loss D: 0.03349991887807846\n",
      "Epoch 31/200, Batch 6/17, Loss G: 5.444967269897461, Loss D: 0.027058478444814682\n",
      "Epoch 31/200, Batch 7/17, Loss G: 5.06801700592041, Loss D: 0.03800174593925476\n",
      "Epoch 31/200, Batch 8/17, Loss G: 5.147212982177734, Loss D: 0.045977793633937836\n",
      "Epoch 31/200, Batch 9/17, Loss G: 5.124728202819824, Loss D: 0.1068325787782669\n",
      "Epoch 31/200, Batch 10/17, Loss G: 4.701529026031494, Loss D: 0.18796133995056152\n",
      "Epoch 31/200, Batch 11/17, Loss G: 5.365612030029297, Loss D: 0.26391980051994324\n",
      "Epoch 31/200, Batch 12/17, Loss G: 5.245787620544434, Loss D: 0.015785180032253265\n",
      "Epoch 31/200, Batch 13/17, Loss G: 4.9378275871276855, Loss D: 0.044522397220134735\n",
      "Epoch 31/200, Batch 14/17, Loss G: 5.238186836242676, Loss D: 0.03450748696923256\n",
      "Epoch 31/200, Batch 15/17, Loss G: 5.219030857086182, Loss D: 0.02796989679336548\n",
      "Epoch 31/200, Batch 16/17, Loss G: 4.975811958312988, Loss D: 0.03682846575975418\n",
      "Epoch 32/200, Batch 0/17, Loss G: 5.182967662811279, Loss D: 0.0637970119714737\n",
      "Epoch 32/200, Batch 1/17, Loss G: 5.017225742340088, Loss D: 0.05031082034111023\n",
      "Epoch 32/200, Batch 2/17, Loss G: 5.197167873382568, Loss D: 0.0935811847448349\n",
      "Epoch 32/200, Batch 3/17, Loss G: 4.902565002441406, Loss D: 0.07479391992092133\n",
      "Epoch 32/200, Batch 4/17, Loss G: 5.1923322677612305, Loss D: 0.09413793683052063\n",
      "Epoch 32/200, Batch 5/17, Loss G: 5.048084735870361, Loss D: 0.0567169189453125\n",
      "Epoch 32/200, Batch 6/17, Loss G: 5.185085296630859, Loss D: 0.06714826077222824\n",
      "Epoch 32/200, Batch 7/17, Loss G: 5.0965166091918945, Loss D: 0.10611236095428467\n",
      "Epoch 32/200, Batch 8/17, Loss G: 5.613995552062988, Loss D: 0.09152483195066452\n",
      "Epoch 32/200, Batch 9/17, Loss G: 5.496349334716797, Loss D: 0.024639464914798737\n",
      "Epoch 32/200, Batch 10/17, Loss G: 4.80752420425415, Loss D: 0.1571594774723053\n",
      "Epoch 32/200, Batch 11/17, Loss G: 5.390336513519287, Loss D: 0.17576773464679718\n",
      "Epoch 32/200, Batch 12/17, Loss G: 4.58366060256958, Loss D: 0.08869454264640808\n",
      "Epoch 32/200, Batch 13/17, Loss G: 5.050214767456055, Loss D: 0.10382498800754547\n",
      "Epoch 32/200, Batch 14/17, Loss G: 5.4586358070373535, Loss D: 0.10545391589403152\n",
      "Epoch 32/200, Batch 15/17, Loss G: 5.334794044494629, Loss D: 0.08221034705638885\n",
      "Epoch 32/200, Batch 16/17, Loss G: 5.247326850891113, Loss D: 0.08427274227142334\n",
      "Epoch 33/200, Batch 0/17, Loss G: 5.2541961669921875, Loss D: 0.08665920048952103\n",
      "Epoch 33/200, Batch 1/17, Loss G: 5.414487838745117, Loss D: 0.16079957783222198\n",
      "Epoch 33/200, Batch 2/17, Loss G: 4.8571577072143555, Loss D: 0.08238092064857483\n",
      "Epoch 33/200, Batch 3/17, Loss G: 5.242083549499512, Loss D: 0.07076713442802429\n",
      "Epoch 33/200, Batch 4/17, Loss G: 5.127492904663086, Loss D: 0.06894394010305405\n",
      "Epoch 33/200, Batch 5/17, Loss G: 5.435726165771484, Loss D: 0.05989302322268486\n",
      "Epoch 33/200, Batch 6/17, Loss G: 5.296450614929199, Loss D: 0.09019213914871216\n",
      "Epoch 33/200, Batch 7/17, Loss G: 4.9927473068237305, Loss D: 0.08346829563379288\n",
      "Epoch 33/200, Batch 8/17, Loss G: 5.314279556274414, Loss D: 0.05376231670379639\n",
      "Epoch 33/200, Batch 9/17, Loss G: 4.984323501586914, Loss D: 0.03958361968398094\n",
      "Epoch 33/200, Batch 10/17, Loss G: 4.990176200866699, Loss D: 0.0483870767056942\n",
      "Epoch 33/200, Batch 11/17, Loss G: 4.982797622680664, Loss D: 0.055644772946834564\n",
      "Epoch 33/200, Batch 12/17, Loss G: 5.357857704162598, Loss D: 0.04274225980043411\n",
      "Epoch 33/200, Batch 13/17, Loss G: 5.1317644119262695, Loss D: 0.04982230067253113\n",
      "Epoch 33/200, Batch 14/17, Loss G: 5.0274553298950195, Loss D: 0.04936034977436066\n",
      "Epoch 33/200, Batch 15/17, Loss G: 4.852724075317383, Loss D: 0.08839740604162216\n",
      "Epoch 33/200, Batch 16/17, Loss G: 5.309672832489014, Loss D: 0.17637009918689728\n",
      "Epoch 34/200, Batch 0/17, Loss G: 4.912994384765625, Loss D: 0.06272466480731964\n",
      "Epoch 34/200, Batch 1/17, Loss G: 4.955317497253418, Loss D: 0.04048212245106697\n",
      "Epoch 34/200, Batch 2/17, Loss G: 5.371978759765625, Loss D: 0.0767042264342308\n",
      "Epoch 34/200, Batch 3/17, Loss G: 4.667794227600098, Loss D: 0.13821373879909515\n",
      "Epoch 34/200, Batch 4/17, Loss G: 5.155766010284424, Loss D: 0.21555310487747192\n",
      "Epoch 34/200, Batch 5/17, Loss G: 4.7282023429870605, Loss D: 0.07240108400583267\n",
      "Epoch 34/200, Batch 6/17, Loss G: 4.836618423461914, Loss D: 0.06212806701660156\n",
      "Epoch 34/200, Batch 7/17, Loss G: 5.380451202392578, Loss D: 0.09058071672916412\n",
      "Epoch 34/200, Batch 8/17, Loss G: 4.926450729370117, Loss D: 0.033434152603149414\n",
      "Epoch 34/200, Batch 9/17, Loss G: 4.8511576652526855, Loss D: 0.053373388946056366\n",
      "Epoch 34/200, Batch 10/17, Loss G: 5.148913383483887, Loss D: 0.059769053012132645\n",
      "Epoch 34/200, Batch 11/17, Loss G: 4.9415130615234375, Loss D: 0.07832886278629303\n",
      "Epoch 34/200, Batch 12/17, Loss G: 5.225530624389648, Loss D: 0.07725584506988525\n",
      "Epoch 34/200, Batch 13/17, Loss G: 5.338865280151367, Loss D: 0.0377470925450325\n",
      "Epoch 34/200, Batch 14/17, Loss G: 5.219367027282715, Loss D: 0.04591713100671768\n",
      "Epoch 34/200, Batch 15/17, Loss G: 4.907773971557617, Loss D: 0.059474170207977295\n",
      "Epoch 34/200, Batch 16/17, Loss G: 5.331530570983887, Loss D: 0.08670281618833542\n",
      "Epoch 35/200, Batch 0/17, Loss G: 4.674095153808594, Loss D: 0.12700729072093964\n",
      "Epoch 35/200, Batch 1/17, Loss G: 5.634966850280762, Loss D: 0.1326337307691574\n",
      "Epoch 35/200, Batch 2/17, Loss G: 4.997815132141113, Loss D: 0.050856590270996094\n",
      "Epoch 35/200, Batch 3/17, Loss G: 4.684815406799316, Loss D: 0.1359824240207672\n",
      "Epoch 35/200, Batch 4/17, Loss G: 5.281216621398926, Loss D: 0.2012668401002884\n",
      "Epoch 35/200, Batch 5/17, Loss G: 5.107804298400879, Loss D: 0.05273473262786865\n",
      "Epoch 35/200, Batch 6/17, Loss G: 4.758340835571289, Loss D: 0.06646183878183365\n",
      "Epoch 35/200, Batch 7/17, Loss G: 5.225404262542725, Loss D: 0.07194773107767105\n",
      "Epoch 35/200, Batch 8/17, Loss G: 4.816592693328857, Loss D: 0.07265589386224747\n",
      "Epoch 35/200, Batch 9/17, Loss G: 5.239038467407227, Loss D: 0.13753099739551544\n",
      "Epoch 35/200, Batch 10/17, Loss G: 4.578176498413086, Loss D: 0.15489034354686737\n",
      "Epoch 35/200, Batch 11/17, Loss G: 5.211714744567871, Loss D: 0.07782924920320511\n",
      "Epoch 35/200, Batch 12/17, Loss G: 5.193984031677246, Loss D: 0.027808208018541336\n",
      "Epoch 35/200, Batch 13/17, Loss G: 4.845229625701904, Loss D: 0.03278630971908569\n",
      "Epoch 35/200, Batch 14/17, Loss G: 4.7753448486328125, Loss D: 0.042151130735874176\n",
      "Epoch 35/200, Batch 15/17, Loss G: 5.297997951507568, Loss D: 0.03197075054049492\n",
      "Epoch 35/200, Batch 16/17, Loss G: 5.1534576416015625, Loss D: 0.04862925037741661\n",
      "Epoch 36/200, Batch 0/17, Loss G: 4.879981994628906, Loss D: 0.07112013548612595\n",
      "Epoch 36/200, Batch 1/17, Loss G: 5.159492492675781, Loss D: 0.05641082674264908\n",
      "Epoch 36/200, Batch 2/17, Loss G: 4.971498012542725, Loss D: 0.05833727866411209\n",
      "Epoch 36/200, Batch 3/17, Loss G: 4.740105628967285, Loss D: 0.05655888095498085\n",
      "Epoch 36/200, Batch 4/17, Loss G: 5.059523582458496, Loss D: 0.06966002285480499\n",
      "Epoch 36/200, Batch 5/17, Loss G: 5.313973426818848, Loss D: 0.10490559786558151\n",
      "Epoch 36/200, Batch 6/17, Loss G: 4.946298599243164, Loss D: 0.062050711363554\n",
      "Epoch 36/200, Batch 7/17, Loss G: 4.9875264167785645, Loss D: 0.04884563758969307\n",
      "Epoch 36/200, Batch 8/17, Loss G: 5.317333698272705, Loss D: 0.11575823277235031\n",
      "Epoch 36/200, Batch 9/17, Loss G: 4.606094837188721, Loss D: 0.17006681859493256\n",
      "Epoch 36/200, Batch 10/17, Loss G: 5.365669250488281, Loss D: 0.07843098044395447\n",
      "Epoch 36/200, Batch 11/17, Loss G: 4.873935699462891, Loss D: 0.031495947390794754\n",
      "Epoch 36/200, Batch 12/17, Loss G: 5.02695369720459, Loss D: 0.04492134600877762\n",
      "Epoch 36/200, Batch 13/17, Loss G: 5.144212245941162, Loss D: 0.034676454961299896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200, Batch 14/17, Loss G: 5.081669807434082, Loss D: 0.0774313285946846\n",
      "Epoch 36/200, Batch 15/17, Loss G: 4.780320167541504, Loss D: 0.09178205579519272\n",
      "Epoch 36/200, Batch 16/17, Loss G: 5.060397148132324, Loss D: 0.08587250858545303\n",
      "Epoch 37/200, Batch 0/17, Loss G: 5.0690107345581055, Loss D: 0.029181428253650665\n",
      "Epoch 37/200, Batch 1/17, Loss G: 4.763298988342285, Loss D: 0.03758987411856651\n",
      "Epoch 37/200, Batch 2/17, Loss G: 4.979291915893555, Loss D: 0.05261120945215225\n",
      "Epoch 37/200, Batch 3/17, Loss G: 4.937717437744141, Loss D: 0.04987373948097229\n",
      "Epoch 37/200, Batch 4/17, Loss G: 4.925477981567383, Loss D: 0.0882849395275116\n",
      "Epoch 37/200, Batch 5/17, Loss G: 4.667876720428467, Loss D: 0.1145033910870552\n",
      "Epoch 37/200, Batch 6/17, Loss G: 5.5106048583984375, Loss D: 0.06661435961723328\n",
      "Epoch 37/200, Batch 7/17, Loss G: 5.117129325866699, Loss D: 0.030345870181918144\n",
      "Epoch 37/200, Batch 8/17, Loss G: 4.783512592315674, Loss D: 0.04732035472989082\n",
      "Epoch 37/200, Batch 9/17, Loss G: 4.871962070465088, Loss D: 0.03644925728440285\n",
      "Epoch 37/200, Batch 10/17, Loss G: 4.975461006164551, Loss D: 0.04680020734667778\n",
      "Epoch 37/200, Batch 11/17, Loss G: 4.971917629241943, Loss D: 0.09422250092029572\n",
      "Epoch 37/200, Batch 12/17, Loss G: 4.908041000366211, Loss D: 0.06863810122013092\n",
      "Epoch 37/200, Batch 13/17, Loss G: 5.350107669830322, Loss D: 0.13258059322834015\n",
      "Epoch 37/200, Batch 14/17, Loss G: 4.873110771179199, Loss D: 0.08334266394376755\n",
      "Epoch 37/200, Batch 15/17, Loss G: 5.2934112548828125, Loss D: 0.035697080194950104\n",
      "Epoch 37/200, Batch 16/17, Loss G: 4.928142547607422, Loss D: 0.06467422842979431\n",
      "Epoch 38/200, Batch 0/17, Loss G: 4.928154945373535, Loss D: 0.04867607355117798\n",
      "Epoch 38/200, Batch 1/17, Loss G: 5.251194953918457, Loss D: 0.06176269054412842\n",
      "Epoch 38/200, Batch 2/17, Loss G: 5.05460262298584, Loss D: 0.05777250975370407\n",
      "Epoch 38/200, Batch 3/17, Loss G: 4.639555931091309, Loss D: 0.11678396910429001\n",
      "Epoch 38/200, Batch 4/17, Loss G: 5.3631086349487305, Loss D: 0.16269797086715698\n",
      "Epoch 38/200, Batch 5/17, Loss G: 4.939431667327881, Loss D: 0.03628788888454437\n",
      "Epoch 38/200, Batch 6/17, Loss G: 4.619370460510254, Loss D: 0.050890401005744934\n",
      "Epoch 38/200, Batch 7/17, Loss G: 5.035786151885986, Loss D: 0.026974288746714592\n",
      "Epoch 38/200, Batch 8/17, Loss G: 5.2823920249938965, Loss D: 0.04660617932677269\n",
      "Epoch 38/200, Batch 9/17, Loss G: 4.836525917053223, Loss D: 0.055952806025743484\n",
      "Epoch 38/200, Batch 10/17, Loss G: 5.035087585449219, Loss D: 0.02760753035545349\n",
      "Epoch 38/200, Batch 11/17, Loss G: 5.089481830596924, Loss D: 0.06581880152225494\n",
      "Epoch 38/200, Batch 12/17, Loss G: 4.710204124450684, Loss D: 0.08626764267683029\n",
      "Epoch 38/200, Batch 13/17, Loss G: 4.699787139892578, Loss D: 0.05961083993315697\n",
      "Epoch 38/200, Batch 14/17, Loss G: 4.9369797706604, Loss D: 0.20148421823978424\n",
      "Epoch 38/200, Batch 15/17, Loss G: 4.706315040588379, Loss D: 0.11741937696933746\n",
      "Epoch 38/200, Batch 16/17, Loss G: 5.5469512939453125, Loss D: 0.011862149462103844\n",
      "Epoch 39/200, Batch 0/17, Loss G: 5.074031829833984, Loss D: 0.06052419915795326\n",
      "Epoch 39/200, Batch 1/17, Loss G: 4.757862567901611, Loss D: 0.03212980180978775\n",
      "Epoch 39/200, Batch 2/17, Loss G: 4.588657379150391, Loss D: 0.06967363506555557\n",
      "Epoch 39/200, Batch 3/17, Loss G: 5.021653175354004, Loss D: 0.10849101096391678\n",
      "Epoch 39/200, Batch 4/17, Loss G: 5.131780624389648, Loss D: 0.08874040096998215\n",
      "Epoch 39/200, Batch 5/17, Loss G: 5.0894012451171875, Loss D: 0.034993644803762436\n",
      "Epoch 39/200, Batch 6/17, Loss G: 4.936417579650879, Loss D: 0.030340343713760376\n",
      "Epoch 39/200, Batch 7/17, Loss G: 4.87315559387207, Loss D: 0.06002460792660713\n",
      "Epoch 39/200, Batch 8/17, Loss G: 4.826005935668945, Loss D: 0.05697132647037506\n",
      "Epoch 39/200, Batch 9/17, Loss G: 4.961941242218018, Loss D: 0.08473390340805054\n",
      "Epoch 39/200, Batch 10/17, Loss G: 4.694104194641113, Loss D: 0.09896951913833618\n",
      "Epoch 39/200, Batch 11/17, Loss G: 5.128632545471191, Loss D: 0.10827965289354324\n",
      "Epoch 39/200, Batch 12/17, Loss G: 4.909701347351074, Loss D: 0.046884842216968536\n",
      "Epoch 39/200, Batch 13/17, Loss G: 4.888552665710449, Loss D: 0.039526425302028656\n",
      "Epoch 39/200, Batch 14/17, Loss G: 5.304712772369385, Loss D: 0.12196417152881622\n",
      "Epoch 39/200, Batch 15/17, Loss G: 4.738076686859131, Loss D: 0.0823933556675911\n",
      "Epoch 39/200, Batch 16/17, Loss G: 5.057717323303223, Loss D: 0.04339680075645447\n",
      "Epoch 40/200, Batch 0/17, Loss G: 5.304732322692871, Loss D: 0.06616052985191345\n",
      "Epoch 40/200, Batch 1/17, Loss G: 4.719938278198242, Loss D: 0.08137092739343643\n",
      "Epoch 40/200, Batch 2/17, Loss G: 5.186131954193115, Loss D: 0.0423617884516716\n",
      "Epoch 40/200, Batch 3/17, Loss G: 5.152161598205566, Loss D: 0.06368917971849442\n",
      "Epoch 40/200, Batch 4/17, Loss G: 4.75615930557251, Loss D: 0.06466489285230637\n",
      "Epoch 40/200, Batch 5/17, Loss G: 4.988950729370117, Loss D: 0.048277027904987335\n",
      "Epoch 40/200, Batch 6/17, Loss G: 5.096431732177734, Loss D: 0.06946125626564026\n",
      "Epoch 40/200, Batch 7/17, Loss G: 4.798811912536621, Loss D: 0.10264243185520172\n",
      "Epoch 40/200, Batch 8/17, Loss G: 5.066080570220947, Loss D: 0.17086723446846008\n",
      "Epoch 40/200, Batch 9/17, Loss G: 4.568285942077637, Loss D: 0.06072351336479187\n",
      "Epoch 40/200, Batch 10/17, Loss G: 4.9188361167907715, Loss D: 0.047233883291482925\n",
      "Epoch 40/200, Batch 11/17, Loss G: 4.830312728881836, Loss D: 0.05728074535727501\n",
      "Epoch 40/200, Batch 12/17, Loss G: 4.817771911621094, Loss D: 0.05319288372993469\n",
      "Epoch 40/200, Batch 13/17, Loss G: 4.843198299407959, Loss D: 0.05288167670369148\n",
      "Epoch 40/200, Batch 14/17, Loss G: 4.963482856750488, Loss D: 0.048026323318481445\n",
      "Epoch 40/200, Batch 15/17, Loss G: 4.987278938293457, Loss D: 0.038932014256715775\n",
      "Epoch 40/200, Batch 16/17, Loss G: 4.928308486938477, Loss D: 0.06747524440288544\n",
      "Epoch 41/200, Batch 0/17, Loss G: 4.531655311584473, Loss D: 0.14551709592342377\n",
      "Epoch 41/200, Batch 1/17, Loss G: 5.125904560089111, Loss D: 0.23661281168460846\n",
      "Epoch 41/200, Batch 2/17, Loss G: 5.025763034820557, Loss D: 0.03298059478402138\n",
      "Epoch 41/200, Batch 3/17, Loss G: 4.686550140380859, Loss D: 0.06376262754201889\n",
      "Epoch 41/200, Batch 4/17, Loss G: 5.0233154296875, Loss D: 0.09118484705686569\n",
      "Epoch 41/200, Batch 5/17, Loss G: 4.857161998748779, Loss D: 0.07256756722927094\n",
      "Epoch 41/200, Batch 6/17, Loss G: 4.65927791595459, Loss D: 0.07098186016082764\n",
      "Epoch 41/200, Batch 7/17, Loss G: 5.350390911102295, Loss D: 0.09231122583150864\n",
      "Epoch 41/200, Batch 8/17, Loss G: 4.999999523162842, Loss D: 0.030466850847005844\n",
      "Epoch 41/200, Batch 9/17, Loss G: 4.757135391235352, Loss D: 0.051526471972465515\n",
      "Epoch 41/200, Batch 10/17, Loss G: 4.934139251708984, Loss D: 0.06634075939655304\n",
      "Epoch 41/200, Batch 11/17, Loss G: 4.852363586425781, Loss D: 0.059299446642398834\n",
      "Epoch 41/200, Batch 12/17, Loss G: 5.073550701141357, Loss D: 0.05985407531261444\n",
      "Epoch 41/200, Batch 13/17, Loss G: 4.928001403808594, Loss D: 0.065818190574646\n",
      "Epoch 41/200, Batch 14/17, Loss G: 4.967170238494873, Loss D: 0.04852697253227234\n",
      "Epoch 41/200, Batch 15/17, Loss G: 5.041879653930664, Loss D: 0.048858642578125\n",
      "Epoch 41/200, Batch 16/17, Loss G: 5.115070819854736, Loss D: 0.1573391556739807\n",
      "Epoch 42/200, Batch 0/17, Loss G: 4.393780708312988, Loss D: 0.21137821674346924\n",
      "Epoch 42/200, Batch 1/17, Loss G: 5.317543029785156, Loss D: 0.15753598511219025\n",
      "Epoch 42/200, Batch 2/17, Loss G: 5.146424293518066, Loss D: 0.035542357712984085\n",
      "Epoch 42/200, Batch 3/17, Loss G: 5.095901012420654, Loss D: 0.03878285363316536\n",
      "Epoch 42/200, Batch 4/17, Loss G: 4.726593971252441, Loss D: 0.0626208484172821\n",
      "Epoch 42/200, Batch 5/17, Loss G: 5.1040239334106445, Loss D: 0.07465746253728867\n",
      "Epoch 42/200, Batch 6/17, Loss G: 4.737789154052734, Loss D: 0.03018529713153839\n",
      "Epoch 42/200, Batch 7/17, Loss G: 4.888986587524414, Loss D: 0.03125455603003502\n",
      "Epoch 42/200, Batch 8/17, Loss G: 4.785398483276367, Loss D: 0.03467588126659393\n",
      "Epoch 42/200, Batch 9/17, Loss G: 4.745283126831055, Loss D: 0.04838372394442558\n",
      "Epoch 42/200, Batch 10/17, Loss G: 4.796789169311523, Loss D: 0.05464109033346176\n",
      "Epoch 42/200, Batch 11/17, Loss G: 5.06988525390625, Loss D: 0.03414233401417732\n",
      "Epoch 42/200, Batch 12/17, Loss G: 4.9225311279296875, Loss D: 0.01932564377784729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200, Batch 13/17, Loss G: 5.135463714599609, Loss D: 0.04270866513252258\n",
      "Epoch 42/200, Batch 14/17, Loss G: 4.929100036621094, Loss D: 0.03848728537559509\n",
      "Epoch 42/200, Batch 15/17, Loss G: 5.105210781097412, Loss D: 0.049605436623096466\n",
      "Epoch 42/200, Batch 16/17, Loss G: 5.043051719665527, Loss D: 0.07458343356847763\n",
      "Epoch 43/200, Batch 0/17, Loss G: 4.676884174346924, Loss D: 0.12064488977193832\n",
      "Epoch 43/200, Batch 1/17, Loss G: 5.1161627769470215, Loss D: 0.1747061163187027\n",
      "Epoch 43/200, Batch 2/17, Loss G: 4.759370803833008, Loss D: 0.04508460313081741\n",
      "Epoch 43/200, Batch 3/17, Loss G: 4.773391246795654, Loss D: 0.05734099820256233\n",
      "Epoch 43/200, Batch 4/17, Loss G: 5.095953941345215, Loss D: 0.13011562824249268\n",
      "Epoch 43/200, Batch 5/17, Loss G: 4.429844856262207, Loss D: 0.10027153789997101\n",
      "Epoch 43/200, Batch 6/17, Loss G: 5.087221145629883, Loss D: 0.10287771373987198\n",
      "Epoch 43/200, Batch 7/17, Loss G: 4.534930229187012, Loss D: 0.04747055470943451\n",
      "Epoch 43/200, Batch 8/17, Loss G: 4.75971794128418, Loss D: 0.05170673504471779\n",
      "Epoch 43/200, Batch 9/17, Loss G: 4.904679298400879, Loss D: 0.0421612486243248\n",
      "Epoch 43/200, Batch 10/17, Loss G: 4.77408504486084, Loss D: 0.04551613703370094\n",
      "Epoch 43/200, Batch 11/17, Loss G: 4.687755584716797, Loss D: 0.040327101945877075\n",
      "Epoch 43/200, Batch 12/17, Loss G: 5.030030727386475, Loss D: 0.045149751007556915\n",
      "Epoch 43/200, Batch 13/17, Loss G: 5.22873592376709, Loss D: 0.09107483178377151\n",
      "Epoch 43/200, Batch 14/17, Loss G: 4.856559753417969, Loss D: 0.050910186022520065\n",
      "Epoch 43/200, Batch 15/17, Loss G: 5.052987575531006, Loss D: 0.03976984694600105\n",
      "Epoch 43/200, Batch 16/17, Loss G: 4.920596122741699, Loss D: 0.040212228894233704\n",
      "Epoch 44/200, Batch 0/17, Loss G: 4.781376361846924, Loss D: 0.06111527234315872\n",
      "Epoch 44/200, Batch 1/17, Loss G: 5.152275085449219, Loss D: 0.030819708481431007\n",
      "Epoch 44/200, Batch 2/17, Loss G: 4.9610137939453125, Loss D: 0.03515119478106499\n",
      "Epoch 44/200, Batch 3/17, Loss G: 4.647432327270508, Loss D: 0.023510821163654327\n",
      "Epoch 44/200, Batch 4/17, Loss G: 4.613028526306152, Loss D: 0.04525875672698021\n",
      "Epoch 44/200, Batch 5/17, Loss G: 4.955077171325684, Loss D: 0.047827064990997314\n",
      "Epoch 44/200, Batch 6/17, Loss G: 4.882752418518066, Loss D: 0.08221688866615295\n",
      "Epoch 44/200, Batch 7/17, Loss G: 5.279908180236816, Loss D: 0.07464416325092316\n",
      "Epoch 44/200, Batch 8/17, Loss G: 5.029180526733398, Loss D: 0.02356749400496483\n",
      "Epoch 44/200, Batch 9/17, Loss G: 4.64247989654541, Loss D: 0.04965686798095703\n",
      "Epoch 44/200, Batch 10/17, Loss G: 4.958962917327881, Loss D: 0.026861857622861862\n",
      "Epoch 44/200, Batch 11/17, Loss G: 5.035983085632324, Loss D: 0.053138431161642075\n",
      "Epoch 44/200, Batch 12/17, Loss G: 4.273329257965088, Loss D: 0.08782036602497101\n",
      "Epoch 44/200, Batch 13/17, Loss G: 5.096528053283691, Loss D: 0.05029459670186043\n",
      "Epoch 44/200, Batch 14/17, Loss G: 5.068807601928711, Loss D: 0.04247831553220749\n",
      "Epoch 44/200, Batch 15/17, Loss G: 4.775464057922363, Loss D: 0.09444070607423782\n",
      "Epoch 44/200, Batch 16/17, Loss G: 4.962309837341309, Loss D: 0.07267571240663528\n",
      "Epoch 45/200, Batch 0/17, Loss G: 4.786274433135986, Loss D: 0.03338794782757759\n",
      "Epoch 45/200, Batch 1/17, Loss G: 4.90581750869751, Loss D: 0.03477595001459122\n",
      "Epoch 45/200, Batch 2/17, Loss G: 4.843783378601074, Loss D: 0.04756789654493332\n",
      "Epoch 45/200, Batch 3/17, Loss G: 4.616681098937988, Loss D: 0.06582412123680115\n",
      "Epoch 45/200, Batch 4/17, Loss G: 4.9649763107299805, Loss D: 0.06446222215890884\n",
      "Epoch 45/200, Batch 5/17, Loss G: 4.781059265136719, Loss D: 0.024850666522979736\n",
      "Epoch 45/200, Batch 6/17, Loss G: 4.799729347229004, Loss D: 0.026844661682844162\n",
      "Epoch 45/200, Batch 7/17, Loss G: 4.849471092224121, Loss D: 0.03805984556674957\n",
      "Epoch 45/200, Batch 8/17, Loss G: 5.004117012023926, Loss D: 0.040748804807662964\n",
      "Epoch 45/200, Batch 9/17, Loss G: 4.754073619842529, Loss D: 0.03131679818034172\n",
      "Epoch 45/200, Batch 10/17, Loss G: 4.763601303100586, Loss D: 0.042155347764492035\n",
      "Epoch 45/200, Batch 11/17, Loss G: 4.737482070922852, Loss D: 0.05471586808562279\n",
      "Epoch 45/200, Batch 12/17, Loss G: 5.18309211730957, Loss D: 0.045465629547834396\n",
      "Epoch 45/200, Batch 13/17, Loss G: 4.986807823181152, Loss D: 0.035056956112384796\n",
      "Epoch 45/200, Batch 14/17, Loss G: 4.988539218902588, Loss D: 0.03742062300443649\n",
      "Epoch 45/200, Batch 15/17, Loss G: 4.944244384765625, Loss D: 0.047578033059835434\n",
      "Epoch 45/200, Batch 16/17, Loss G: 5.09733772277832, Loss D: 0.02332662045955658\n",
      "Epoch 46/200, Batch 0/17, Loss G: 4.882619857788086, Loss D: 0.026785578578710556\n",
      "Epoch 46/200, Batch 1/17, Loss G: 5.047921180725098, Loss D: 0.025574667379260063\n",
      "Epoch 46/200, Batch 2/17, Loss G: 4.7268900871276855, Loss D: 0.04296770691871643\n",
      "Epoch 46/200, Batch 3/17, Loss G: 4.756898880004883, Loss D: 0.05829649418592453\n",
      "Epoch 46/200, Batch 4/17, Loss G: 4.746459007263184, Loss D: 0.05131043866276741\n",
      "Epoch 46/200, Batch 5/17, Loss G: 4.968140602111816, Loss D: 0.03938011825084686\n",
      "Epoch 46/200, Batch 6/17, Loss G: 4.678042411804199, Loss D: 0.02960815280675888\n",
      "Epoch 46/200, Batch 7/17, Loss G: 4.90824031829834, Loss D: 0.02325545623898506\n",
      "Epoch 46/200, Batch 8/17, Loss G: 5.016218185424805, Loss D: 0.026445087045431137\n",
      "Epoch 46/200, Batch 9/17, Loss G: 4.988925457000732, Loss D: 0.0289315115660429\n",
      "Epoch 46/200, Batch 10/17, Loss G: 4.70591402053833, Loss D: 0.05056221783161163\n",
      "Epoch 46/200, Batch 11/17, Loss G: 5.055764198303223, Loss D: 0.04775523021817207\n",
      "Epoch 46/200, Batch 12/17, Loss G: 4.7958903312683105, Loss D: 0.02464717999100685\n",
      "Epoch 46/200, Batch 13/17, Loss G: 4.673456192016602, Loss D: 0.04763726890087128\n",
      "Epoch 46/200, Batch 14/17, Loss G: 4.925088405609131, Loss D: 0.03885433077812195\n",
      "Epoch 46/200, Batch 15/17, Loss G: 4.899340629577637, Loss D: 0.0401245579123497\n",
      "Epoch 46/200, Batch 16/17, Loss G: 5.293456077575684, Loss D: 0.02989891730248928\n",
      "Epoch 47/200, Batch 0/17, Loss G: 4.896439552307129, Loss D: 0.039285868406295776\n",
      "Epoch 47/200, Batch 1/17, Loss G: 4.6000213623046875, Loss D: 0.11722998321056366\n",
      "Epoch 47/200, Batch 2/17, Loss G: 5.188859462738037, Loss D: 0.12246537208557129\n",
      "Epoch 47/200, Batch 3/17, Loss G: 4.906257152557373, Loss D: 0.014035282656550407\n",
      "Epoch 47/200, Batch 4/17, Loss G: 4.469346523284912, Loss D: 0.09723976254463196\n",
      "Epoch 47/200, Batch 5/17, Loss G: 5.246528148651123, Loss D: 0.20271092653274536\n",
      "Epoch 47/200, Batch 6/17, Loss G: 4.327214241027832, Loss D: 0.15128616988658905\n",
      "Epoch 47/200, Batch 7/17, Loss G: 4.886871337890625, Loss D: 0.1138233095407486\n",
      "Epoch 47/200, Batch 8/17, Loss G: 4.537582874298096, Loss D: 0.07031448930501938\n",
      "Epoch 47/200, Batch 9/17, Loss G: 4.717666149139404, Loss D: 0.059337832033634186\n",
      "Epoch 47/200, Batch 10/17, Loss G: 4.590039253234863, Loss D: 0.06226125359535217\n",
      "Epoch 47/200, Batch 11/17, Loss G: 4.7762370109558105, Loss D: 0.06672832369804382\n",
      "Epoch 47/200, Batch 12/17, Loss G: 4.881447792053223, Loss D: 0.049616798758506775\n",
      "Epoch 47/200, Batch 13/17, Loss G: 4.64854621887207, Loss D: 0.06651908904314041\n",
      "Epoch 47/200, Batch 14/17, Loss G: 5.050193786621094, Loss D: 0.05344628915190697\n",
      "Epoch 47/200, Batch 15/17, Loss G: 4.817905426025391, Loss D: 0.038634054362773895\n",
      "Epoch 47/200, Batch 16/17, Loss G: 4.940513610839844, Loss D: 0.05158398672938347\n",
      "Epoch 48/200, Batch 0/17, Loss G: 4.909896373748779, Loss D: 0.030619872733950615\n",
      "Epoch 48/200, Batch 1/17, Loss G: 5.346126079559326, Loss D: 0.10704000294208527\n",
      "Epoch 48/200, Batch 2/17, Loss G: 4.318604946136475, Loss D: 0.11952324956655502\n",
      "Epoch 48/200, Batch 3/17, Loss G: 5.138058662414551, Loss D: 0.07042036205530167\n",
      "Epoch 48/200, Batch 4/17, Loss G: 4.848852634429932, Loss D: 0.03476584330201149\n",
      "Epoch 48/200, Batch 5/17, Loss G: 4.808401107788086, Loss D: 0.045358963310718536\n",
      "Epoch 48/200, Batch 6/17, Loss G: 4.908688545227051, Loss D: 0.03367723524570465\n",
      "Epoch 48/200, Batch 7/17, Loss G: 5.000736713409424, Loss D: 0.036284852772951126\n",
      "Epoch 48/200, Batch 8/17, Loss G: 5.028582572937012, Loss D: 0.05682067945599556\n",
      "Epoch 48/200, Batch 9/17, Loss G: 4.626605033874512, Loss D: 0.022343067452311516\n",
      "Epoch 48/200, Batch 10/17, Loss G: 4.562618732452393, Loss D: 0.027425123378634453\n",
      "Epoch 48/200, Batch 11/17, Loss G: 4.98009729385376, Loss D: 0.02817675471305847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200, Batch 12/17, Loss G: 4.836636066436768, Loss D: 0.054062601178884506\n",
      "Epoch 48/200, Batch 13/17, Loss G: 4.479140281677246, Loss D: 0.0907653197646141\n",
      "Epoch 48/200, Batch 14/17, Loss G: 5.118491172790527, Loss D: 0.1455095112323761\n",
      "Epoch 48/200, Batch 15/17, Loss G: 4.3655195236206055, Loss D: 0.057034824043512344\n",
      "Epoch 48/200, Batch 16/17, Loss G: 5.35826301574707, Loss D: 0.029731517657637596\n",
      "Epoch 49/200, Batch 0/17, Loss G: 4.570611953735352, Loss D: 0.05685931816697121\n",
      "Epoch 49/200, Batch 1/17, Loss G: 4.840471267700195, Loss D: 0.048999667167663574\n",
      "Epoch 49/200, Batch 2/17, Loss G: 4.670440673828125, Loss D: 0.03610677272081375\n",
      "Epoch 49/200, Batch 3/17, Loss G: 4.683894157409668, Loss D: 0.033928848803043365\n",
      "Epoch 49/200, Batch 4/17, Loss G: 4.761818885803223, Loss D: 0.06552385538816452\n",
      "Epoch 49/200, Batch 5/17, Loss G: 4.503826141357422, Loss D: 0.06747596710920334\n",
      "Epoch 49/200, Batch 6/17, Loss G: 4.988956451416016, Loss D: 0.0317460261285305\n",
      "Epoch 49/200, Batch 7/17, Loss G: 4.812872409820557, Loss D: 0.034400179982185364\n",
      "Epoch 49/200, Batch 8/17, Loss G: 4.775637626647949, Loss D: 0.03150457516312599\n",
      "Epoch 49/200, Batch 9/17, Loss G: 4.779485702514648, Loss D: 0.0460890457034111\n",
      "Epoch 49/200, Batch 10/17, Loss G: 4.455844402313232, Loss D: 0.09395450353622437\n",
      "Epoch 49/200, Batch 11/17, Loss G: 5.141287803649902, Loss D: 0.12088718265295029\n",
      "Epoch 49/200, Batch 12/17, Loss G: 4.851554870605469, Loss D: 0.03679686039686203\n",
      "Epoch 49/200, Batch 13/17, Loss G: 4.840702056884766, Loss D: 0.024540342390537262\n",
      "Epoch 49/200, Batch 14/17, Loss G: 4.9606122970581055, Loss D: 0.05045641213655472\n",
      "Epoch 49/200, Batch 15/17, Loss G: 4.56615686416626, Loss D: 0.06467902660369873\n",
      "Epoch 49/200, Batch 16/17, Loss G: 5.0646257400512695, Loss D: 0.10767737776041031\n",
      "Epoch 50/200, Batch 0/17, Loss G: 4.362972736358643, Loss D: 0.06556572020053864\n",
      "Epoch 50/200, Batch 1/17, Loss G: 4.865499496459961, Loss D: 0.01865079440176487\n",
      "Epoch 50/200, Batch 2/17, Loss G: 4.844840049743652, Loss D: 0.050087280571460724\n",
      "Epoch 50/200, Batch 3/17, Loss G: 5.0335493087768555, Loss D: 0.034058596938848495\n",
      "Epoch 50/200, Batch 4/17, Loss G: 4.647171974182129, Loss D: 0.034058816730976105\n",
      "Epoch 50/200, Batch 5/17, Loss G: 4.888856887817383, Loss D: 0.03941583260893822\n",
      "Epoch 50/200, Batch 6/17, Loss G: 4.701479911804199, Loss D: 0.04193294420838356\n",
      "Epoch 50/200, Batch 7/17, Loss G: 4.927312850952148, Loss D: 0.03653053939342499\n",
      "Epoch 50/200, Batch 8/17, Loss G: 4.825268268585205, Loss D: 0.034120939671993256\n",
      "Epoch 50/200, Batch 9/17, Loss G: 4.781869888305664, Loss D: 0.02477177605032921\n",
      "Epoch 50/200, Batch 10/17, Loss G: 4.829689025878906, Loss D: 0.03270932659506798\n",
      "Epoch 50/200, Batch 11/17, Loss G: 4.493842124938965, Loss D: 0.032933011651039124\n",
      "Epoch 50/200, Batch 12/17, Loss G: 4.898712158203125, Loss D: 0.04674411937594414\n",
      "Epoch 50/200, Batch 13/17, Loss G: 4.554898738861084, Loss D: 0.04911345988512039\n",
      "Epoch 50/200, Batch 14/17, Loss G: 4.644570827484131, Loss D: 0.04656798392534256\n",
      "Epoch 50/200, Batch 15/17, Loss G: 4.6691107749938965, Loss D: 0.018368691205978394\n",
      "Epoch 50/200, Batch 16/17, Loss G: 4.691346168518066, Loss D: 0.03122568503022194\n",
      "Epoch 51/200, Batch 0/17, Loss G: 4.79118537902832, Loss D: 0.029735779389739037\n",
      "Epoch 51/200, Batch 1/17, Loss G: 4.573662757873535, Loss D: 0.04236087575554848\n",
      "Epoch 51/200, Batch 2/17, Loss G: 4.971856117248535, Loss D: 0.06886544823646545\n",
      "Epoch 51/200, Batch 3/17, Loss G: 4.3994340896606445, Loss D: 0.12722408771514893\n",
      "Epoch 51/200, Batch 4/17, Loss G: 5.143694877624512, Loss D: 0.07267876714468002\n",
      "Epoch 51/200, Batch 5/17, Loss G: 5.220865249633789, Loss D: 0.015632543712854385\n",
      "Epoch 51/200, Batch 6/17, Loss G: 4.507546424865723, Loss D: 0.0365968756377697\n",
      "Epoch 51/200, Batch 7/17, Loss G: 4.848875522613525, Loss D: 0.021336372941732407\n",
      "Epoch 51/200, Batch 8/17, Loss G: 5.0710649490356445, Loss D: 0.07494671642780304\n",
      "Epoch 51/200, Batch 9/17, Loss G: 4.034865379333496, Loss D: 0.14477014541625977\n",
      "Epoch 51/200, Batch 10/17, Loss G: 4.984194755554199, Loss D: 0.15926797688007355\n",
      "Epoch 51/200, Batch 11/17, Loss G: 4.681090831756592, Loss D: 0.029575973749160767\n",
      "Epoch 51/200, Batch 12/17, Loss G: 4.539496421813965, Loss D: 0.03681943193078041\n",
      "Epoch 51/200, Batch 13/17, Loss G: 4.715811729431152, Loss D: 0.03843211755156517\n",
      "Epoch 51/200, Batch 14/17, Loss G: 4.883675575256348, Loss D: 0.04933745414018631\n",
      "Epoch 51/200, Batch 15/17, Loss G: 4.495417594909668, Loss D: 0.06659473478794098\n",
      "Epoch 51/200, Batch 16/17, Loss G: 5.0556230545043945, Loss D: 0.13949713110923767\n",
      "Epoch 52/200, Batch 0/17, Loss G: 4.462013244628906, Loss D: 0.07913842052221298\n",
      "Epoch 52/200, Batch 1/17, Loss G: 4.713716983795166, Loss D: 0.02590836212038994\n",
      "Epoch 52/200, Batch 2/17, Loss G: 4.6805267333984375, Loss D: 0.03197032958269119\n",
      "Epoch 52/200, Batch 3/17, Loss G: 4.905344009399414, Loss D: 0.040772344917058945\n",
      "Epoch 52/200, Batch 4/17, Loss G: 4.432862281799316, Loss D: 0.07037659734487534\n",
      "Epoch 52/200, Batch 5/17, Loss G: 4.719351291656494, Loss D: 0.0669669508934021\n",
      "Epoch 52/200, Batch 6/17, Loss G: 4.680215835571289, Loss D: 0.056803517043590546\n",
      "Epoch 52/200, Batch 7/17, Loss G: 4.815826416015625, Loss D: 0.06539493799209595\n",
      "Epoch 52/200, Batch 8/17, Loss G: 4.509552478790283, Loss D: 0.06560306251049042\n",
      "Epoch 52/200, Batch 9/17, Loss G: 4.862112998962402, Loss D: 0.07343444228172302\n",
      "Epoch 52/200, Batch 10/17, Loss G: 4.692809581756592, Loss D: 0.05736391246318817\n",
      "Epoch 52/200, Batch 11/17, Loss G: 4.685752868652344, Loss D: 0.031337060034275055\n",
      "Epoch 52/200, Batch 12/17, Loss G: 4.686362266540527, Loss D: 0.04233144223690033\n",
      "Epoch 52/200, Batch 13/17, Loss G: 4.569154739379883, Loss D: 0.04799599200487137\n",
      "Epoch 52/200, Batch 14/17, Loss G: 4.782380104064941, Loss D: 0.028385959565639496\n",
      "Epoch 52/200, Batch 15/17, Loss G: 4.9015374183654785, Loss D: 0.060095690190792084\n",
      "Epoch 52/200, Batch 16/17, Loss G: 4.409486770629883, Loss D: 0.09574226289987564\n",
      "Epoch 53/200, Batch 0/17, Loss G: 4.838491916656494, Loss D: 0.02687906101346016\n",
      "Epoch 53/200, Batch 1/17, Loss G: 5.108957290649414, Loss D: 0.03391515463590622\n",
      "Epoch 53/200, Batch 2/17, Loss G: 4.887093544006348, Loss D: 0.037756528705358505\n",
      "Epoch 53/200, Batch 3/17, Loss G: 4.436318397521973, Loss D: 0.0902988612651825\n",
      "Epoch 53/200, Batch 4/17, Loss G: 4.863482475280762, Loss D: 0.0771227478981018\n",
      "Epoch 53/200, Batch 5/17, Loss G: 4.3439202308654785, Loss D: 0.09316341578960419\n",
      "Epoch 53/200, Batch 6/17, Loss G: 5.001869201660156, Loss D: 0.1551397442817688\n",
      "Epoch 53/200, Batch 7/17, Loss G: 4.636190891265869, Loss D: 0.04942519590258598\n",
      "Epoch 53/200, Batch 8/17, Loss G: 4.658495903015137, Loss D: 0.027155349031090736\n",
      "Epoch 53/200, Batch 9/17, Loss G: 4.536794662475586, Loss D: 0.034178148955106735\n",
      "Epoch 53/200, Batch 10/17, Loss G: 4.667102813720703, Loss D: 0.050546642392873764\n",
      "Epoch 53/200, Batch 11/17, Loss G: 4.7361555099487305, Loss D: 0.1550014317035675\n",
      "Epoch 53/200, Batch 12/17, Loss G: 4.152750015258789, Loss D: 0.23358377814292908\n",
      "Epoch 53/200, Batch 13/17, Loss G: 4.814202308654785, Loss D: 0.21277819573879242\n",
      "Epoch 53/200, Batch 14/17, Loss G: 4.680205345153809, Loss D: 0.05055665969848633\n",
      "Epoch 53/200, Batch 15/17, Loss G: 4.460037708282471, Loss D: 0.09614677727222443\n",
      "Epoch 53/200, Batch 16/17, Loss G: 5.106532096862793, Loss D: 0.0993335098028183\n",
      "Epoch 54/200, Batch 0/17, Loss G: 4.7291717529296875, Loss D: 0.03941228985786438\n",
      "Epoch 54/200, Batch 1/17, Loss G: 4.750497817993164, Loss D: 0.06610378623008728\n",
      "Epoch 54/200, Batch 2/17, Loss G: 4.722961902618408, Loss D: 0.03755473345518112\n",
      "Epoch 54/200, Batch 3/17, Loss G: 4.51430606842041, Loss D: 0.045735400170087814\n",
      "Epoch 54/200, Batch 4/17, Loss G: 4.649660110473633, Loss D: 0.04957909137010574\n",
      "Epoch 54/200, Batch 5/17, Loss G: 4.6728715896606445, Loss D: 0.04777612164616585\n",
      "Epoch 54/200, Batch 6/17, Loss G: 4.743926048278809, Loss D: 0.08296847343444824\n",
      "Epoch 54/200, Batch 7/17, Loss G: 4.271742820739746, Loss D: 0.15246088802814484\n",
      "Epoch 54/200, Batch 8/17, Loss G: 4.802212715148926, Loss D: 0.052987340837717056\n",
      "Epoch 54/200, Batch 9/17, Loss G: 4.962369918823242, Loss D: 0.05162477865815163\n",
      "Epoch 54/200, Batch 10/17, Loss G: 4.357326507568359, Loss D: 0.07241524755954742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200, Batch 11/17, Loss G: 4.604257106781006, Loss D: 0.049598563462495804\n",
      "Epoch 54/200, Batch 12/17, Loss G: 4.430495262145996, Loss D: 0.02337808534502983\n",
      "Epoch 54/200, Batch 13/17, Loss G: 4.784598350524902, Loss D: 0.042114559561014175\n",
      "Epoch 54/200, Batch 14/17, Loss G: 4.7752766609191895, Loss D: 0.029649723321199417\n",
      "Epoch 54/200, Batch 15/17, Loss G: 4.680853843688965, Loss D: 0.02795824594795704\n",
      "Epoch 54/200, Batch 16/17, Loss G: 4.931338310241699, Loss D: 0.011006025597453117\n",
      "Epoch 55/200, Batch 0/17, Loss G: 4.57584285736084, Loss D: 0.05224762484431267\n",
      "Epoch 55/200, Batch 1/17, Loss G: 4.65554141998291, Loss D: 0.023762324824929237\n",
      "Epoch 55/200, Batch 2/17, Loss G: 4.806202411651611, Loss D: 0.030542852357029915\n",
      "Epoch 55/200, Batch 3/17, Loss G: 4.9178009033203125, Loss D: 0.03929171711206436\n",
      "Epoch 55/200, Batch 4/17, Loss G: 4.385136604309082, Loss D: 0.048885878175497055\n",
      "Epoch 55/200, Batch 5/17, Loss G: 4.825103282928467, Loss D: 0.033487431704998016\n",
      "Epoch 55/200, Batch 6/17, Loss G: 4.978338241577148, Loss D: 0.01418919675052166\n",
      "Epoch 55/200, Batch 7/17, Loss G: 4.841766357421875, Loss D: 0.018224351108074188\n",
      "Epoch 55/200, Batch 8/17, Loss G: 4.902715682983398, Loss D: 0.03003114089369774\n",
      "Epoch 55/200, Batch 9/17, Loss G: 4.875710487365723, Loss D: 0.03233436122536659\n",
      "Epoch 55/200, Batch 10/17, Loss G: 4.8176422119140625, Loss D: 0.040219638496637344\n",
      "Epoch 55/200, Batch 11/17, Loss G: 4.528844356536865, Loss D: 0.03876775503158569\n",
      "Epoch 55/200, Batch 12/17, Loss G: 4.271875381469727, Loss D: 0.10300134122371674\n",
      "Epoch 55/200, Batch 13/17, Loss G: 4.877810955047607, Loss D: 0.18779169023036957\n",
      "Epoch 55/200, Batch 14/17, Loss G: 4.448616981506348, Loss D: 0.039633236825466156\n",
      "Epoch 55/200, Batch 15/17, Loss G: 4.471056938171387, Loss D: 0.03958924114704132\n",
      "Epoch 55/200, Batch 16/17, Loss G: 4.728058338165283, Loss D: 0.026093989610671997\n",
      "Epoch 56/200, Batch 0/17, Loss G: 4.861991882324219, Loss D: 0.029216323047876358\n",
      "Epoch 56/200, Batch 1/17, Loss G: 4.550110340118408, Loss D: 0.06245323270559311\n",
      "Epoch 56/200, Batch 2/17, Loss G: 4.224177360534668, Loss D: 0.1533784121274948\n",
      "Epoch 56/200, Batch 3/17, Loss G: 4.93671989440918, Loss D: 0.18461228907108307\n",
      "Epoch 56/200, Batch 4/17, Loss G: 4.537489891052246, Loss D: 0.017207207158207893\n",
      "Epoch 56/200, Batch 5/17, Loss G: 4.382073879241943, Loss D: 0.05227315053343773\n",
      "Epoch 56/200, Batch 6/17, Loss G: 4.950117111206055, Loss D: 0.05734643340110779\n",
      "Epoch 56/200, Batch 7/17, Loss G: 4.459476470947266, Loss D: 0.07995063811540604\n",
      "Epoch 56/200, Batch 8/17, Loss G: 4.685769081115723, Loss D: 0.050821833312511444\n",
      "Epoch 56/200, Batch 9/17, Loss G: 4.991609573364258, Loss D: 0.046548597514629364\n",
      "Epoch 56/200, Batch 10/17, Loss G: 4.564210891723633, Loss D: 0.040300894528627396\n",
      "Epoch 56/200, Batch 11/17, Loss G: 4.819368839263916, Loss D: 0.03494478017091751\n",
      "Epoch 56/200, Batch 12/17, Loss G: 4.5829267501831055, Loss D: 0.04054946452379227\n",
      "Epoch 56/200, Batch 13/17, Loss G: 4.32526159286499, Loss D: 0.040587324649095535\n",
      "Epoch 56/200, Batch 14/17, Loss G: 4.6354780197143555, Loss D: 0.09356173127889633\n",
      "Epoch 56/200, Batch 15/17, Loss G: 4.250914573669434, Loss D: 0.18135203421115875\n",
      "Epoch 56/200, Batch 16/17, Loss G: 4.761160850524902, Loss D: 0.0691492035984993\n",
      "Epoch 57/200, Batch 0/17, Loss G: 5.018339157104492, Loss D: 0.020415857434272766\n",
      "Epoch 57/200, Batch 1/17, Loss G: 4.5007147789001465, Loss D: 0.06326122581958771\n",
      "Epoch 57/200, Batch 2/17, Loss G: 4.813392639160156, Loss D: 0.05548221245408058\n",
      "Epoch 57/200, Batch 3/17, Loss G: 4.493624210357666, Loss D: 0.02494063787162304\n",
      "Epoch 57/200, Batch 4/17, Loss G: 4.747176170349121, Loss D: 0.022815054282546043\n",
      "Epoch 57/200, Batch 5/17, Loss G: 4.5857319831848145, Loss D: 0.07126694917678833\n",
      "Epoch 57/200, Batch 6/17, Loss G: 4.28069543838501, Loss D: 0.11287723481655121\n",
      "Epoch 57/200, Batch 7/17, Loss G: 4.777671813964844, Loss D: 0.1328338235616684\n",
      "Epoch 57/200, Batch 8/17, Loss G: 4.4550862312316895, Loss D: 0.042150817811489105\n",
      "Epoch 57/200, Batch 9/17, Loss G: 4.587763786315918, Loss D: 0.029665950685739517\n",
      "Epoch 57/200, Batch 10/17, Loss G: 4.702363014221191, Loss D: 0.03990338370203972\n",
      "Epoch 57/200, Batch 11/17, Loss G: 4.650926113128662, Loss D: 0.024077072739601135\n",
      "Epoch 57/200, Batch 12/17, Loss G: 4.544247150421143, Loss D: 0.034163497388362885\n",
      "Epoch 57/200, Batch 13/17, Loss G: 4.419990539550781, Loss D: 0.029711950570344925\n",
      "Epoch 57/200, Batch 14/17, Loss G: 4.775699615478516, Loss D: 0.06878365576267242\n",
      "Epoch 57/200, Batch 15/17, Loss G: 4.312390327453613, Loss D: 0.1014050617814064\n",
      "Epoch 57/200, Batch 16/17, Loss G: 4.924071311950684, Loss D: 0.16888007521629333\n",
      "Epoch 58/200, Batch 0/17, Loss G: 4.155051231384277, Loss D: 0.10957314819097519\n",
      "Epoch 58/200, Batch 1/17, Loss G: 4.816983699798584, Loss D: 0.05239086225628853\n",
      "Epoch 58/200, Batch 2/17, Loss G: 4.493305206298828, Loss D: 0.019716158509254456\n",
      "Epoch 58/200, Batch 3/17, Loss G: 4.725385665893555, Loss D: 0.03800614923238754\n",
      "Epoch 58/200, Batch 4/17, Loss G: 4.699711799621582, Loss D: 0.03871415928006172\n",
      "Epoch 58/200, Batch 5/17, Loss G: 4.615086555480957, Loss D: 0.023505115881562233\n",
      "Epoch 58/200, Batch 6/17, Loss G: 4.68484354019165, Loss D: 0.016454435884952545\n",
      "Epoch 58/200, Batch 7/17, Loss G: 4.731021404266357, Loss D: 0.0364065021276474\n",
      "Epoch 58/200, Batch 8/17, Loss G: 4.410777568817139, Loss D: 0.023032907396554947\n",
      "Epoch 58/200, Batch 9/17, Loss G: 4.313465118408203, Loss D: 0.052455127239227295\n",
      "Epoch 58/200, Batch 10/17, Loss G: 4.833855628967285, Loss D: 0.09489970654249191\n",
      "Epoch 58/200, Batch 11/17, Loss G: 4.245345115661621, Loss D: 0.060967907309532166\n",
      "Epoch 58/200, Batch 12/17, Loss G: 4.5779709815979, Loss D: 0.02512660063803196\n",
      "Epoch 58/200, Batch 13/17, Loss G: 4.814296722412109, Loss D: 0.046688444912433624\n",
      "Epoch 58/200, Batch 14/17, Loss G: 4.435845375061035, Loss D: 0.03719015419483185\n",
      "Epoch 58/200, Batch 15/17, Loss G: 4.522608757019043, Loss D: 0.03966207429766655\n",
      "Epoch 58/200, Batch 16/17, Loss G: 4.721636772155762, Loss D: 0.06629559397697449\n",
      "Epoch 59/200, Batch 0/17, Loss G: 4.294964790344238, Loss D: 0.09977596253156662\n",
      "Epoch 59/200, Batch 1/17, Loss G: 5.071931838989258, Loss D: 0.056280359625816345\n",
      "Epoch 59/200, Batch 2/17, Loss G: 4.6162261962890625, Loss D: 0.025495991110801697\n",
      "Epoch 59/200, Batch 3/17, Loss G: 4.536201477050781, Loss D: 0.04497772082686424\n",
      "Epoch 59/200, Batch 4/17, Loss G: 4.689802169799805, Loss D: 0.05623665824532509\n",
      "Epoch 59/200, Batch 5/17, Loss G: 4.196445465087891, Loss D: 0.04825874790549278\n",
      "Epoch 59/200, Batch 6/17, Loss G: 4.583702087402344, Loss D: 0.05690108239650726\n",
      "Epoch 59/200, Batch 7/17, Loss G: 4.624375343322754, Loss D: 0.026019051671028137\n",
      "Epoch 59/200, Batch 8/17, Loss G: 4.538496971130371, Loss D: 0.02326684072613716\n",
      "Epoch 59/200, Batch 9/17, Loss G: 4.6200103759765625, Loss D: 0.017595650628209114\n",
      "Epoch 59/200, Batch 10/17, Loss G: 4.689504146575928, Loss D: 0.01210053265094757\n",
      "Epoch 59/200, Batch 11/17, Loss G: 4.630014896392822, Loss D: 0.0159552413970232\n",
      "Epoch 59/200, Batch 12/17, Loss G: 4.6172003746032715, Loss D: 0.022784966975450516\n",
      "Epoch 59/200, Batch 13/17, Loss G: 4.6794610023498535, Loss D: 0.0262936782091856\n",
      "Epoch 59/200, Batch 14/17, Loss G: 4.689068794250488, Loss D: 0.02881459891796112\n",
      "Epoch 59/200, Batch 15/17, Loss G: 4.648669242858887, Loss D: 0.03110640123486519\n",
      "Epoch 59/200, Batch 16/17, Loss G: 4.612388610839844, Loss D: 0.054673682898283005\n",
      "Epoch 60/200, Batch 0/17, Loss G: 3.9639153480529785, Loss D: 0.16089995205402374\n",
      "Epoch 60/200, Batch 1/17, Loss G: 4.720097541809082, Loss D: 0.1531338393688202\n",
      "Epoch 60/200, Batch 2/17, Loss G: 4.611539840698242, Loss D: 0.04386061429977417\n",
      "Epoch 60/200, Batch 3/17, Loss G: 4.2679595947265625, Loss D: 0.12515148520469666\n",
      "Epoch 60/200, Batch 4/17, Loss G: 4.806083679199219, Loss D: 0.11123504489660263\n",
      "Epoch 60/200, Batch 5/17, Loss G: 4.396450519561768, Loss D: 0.040849655866622925\n",
      "Epoch 60/200, Batch 6/17, Loss G: 4.376874923706055, Loss D: 0.04535777494311333\n",
      "Epoch 60/200, Batch 7/17, Loss G: 4.884721755981445, Loss D: 0.05425851047039032\n",
      "Epoch 60/200, Batch 8/17, Loss G: 4.657934665679932, Loss D: 0.03877025842666626\n",
      "Epoch 60/200, Batch 9/17, Loss G: 4.795385837554932, Loss D: 0.030553437769412994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200, Batch 10/17, Loss G: 4.52509069442749, Loss D: 0.05389007180929184\n",
      "Epoch 60/200, Batch 11/17, Loss G: 4.493714332580566, Loss D: 0.04038446396589279\n",
      "Epoch 60/200, Batch 12/17, Loss G: 4.584901809692383, Loss D: 0.03424305468797684\n",
      "Epoch 60/200, Batch 13/17, Loss G: 4.5572309494018555, Loss D: 0.04453463479876518\n",
      "Epoch 60/200, Batch 14/17, Loss G: 4.58359956741333, Loss D: 0.04114939644932747\n",
      "Epoch 60/200, Batch 15/17, Loss G: 4.8353271484375, Loss D: 0.05386105924844742\n",
      "Epoch 60/200, Batch 16/17, Loss G: 4.522895812988281, Loss D: 0.039382096379995346\n",
      "Epoch 61/200, Batch 0/17, Loss G: 4.573859691619873, Loss D: 0.027435384690761566\n",
      "Epoch 61/200, Batch 1/17, Loss G: 4.786754608154297, Loss D: 0.021375583484768867\n",
      "Epoch 61/200, Batch 2/17, Loss G: 4.6149396896362305, Loss D: 0.02637665346264839\n",
      "Epoch 61/200, Batch 3/17, Loss G: 4.4774088859558105, Loss D: 0.029987405985593796\n",
      "Epoch 61/200, Batch 4/17, Loss G: 4.86484956741333, Loss D: 0.010524101555347443\n",
      "Epoch 61/200, Batch 5/17, Loss G: 4.507369041442871, Loss D: 0.030127190053462982\n",
      "Epoch 61/200, Batch 6/17, Loss G: 4.681778907775879, Loss D: 0.02245159260928631\n",
      "Epoch 61/200, Batch 7/17, Loss G: 4.374757289886475, Loss D: 0.04160115867853165\n",
      "Epoch 61/200, Batch 8/17, Loss G: 4.699741363525391, Loss D: 0.02978506311774254\n",
      "Epoch 61/200, Batch 9/17, Loss G: 4.537342071533203, Loss D: 0.015200608409941196\n",
      "Epoch 61/200, Batch 10/17, Loss G: 4.552154541015625, Loss D: 0.024617912247776985\n",
      "Epoch 61/200, Batch 11/17, Loss G: 4.736353874206543, Loss D: 0.021555282175540924\n",
      "Epoch 61/200, Batch 12/17, Loss G: 4.453491687774658, Loss D: 0.0448945127427578\n",
      "Epoch 61/200, Batch 13/17, Loss G: 4.3059983253479, Loss D: 0.09574177116155624\n",
      "Epoch 61/200, Batch 14/17, Loss G: 4.655339241027832, Loss D: 0.18831545114517212\n",
      "Epoch 61/200, Batch 15/17, Loss G: 4.257680892944336, Loss D: 0.11151187866926193\n",
      "Epoch 61/200, Batch 16/17, Loss G: 4.917568206787109, Loss D: 0.027776779606938362\n",
      "Epoch 62/200, Batch 0/17, Loss G: 4.730976104736328, Loss D: 0.0186631977558136\n",
      "Epoch 62/200, Batch 1/17, Loss G: 4.4683756828308105, Loss D: 0.01990526169538498\n",
      "Epoch 62/200, Batch 2/17, Loss G: 4.780181884765625, Loss D: 0.02048288658261299\n",
      "Epoch 62/200, Batch 3/17, Loss G: 4.442133903503418, Loss D: 0.058417439460754395\n",
      "Epoch 62/200, Batch 4/17, Loss G: 4.318450927734375, Loss D: 0.052176717668771744\n",
      "Epoch 62/200, Batch 5/17, Loss G: 4.634655952453613, Loss D: 0.03086872026324272\n",
      "Epoch 62/200, Batch 6/17, Loss G: 4.702434539794922, Loss D: 0.02571013569831848\n",
      "Epoch 62/200, Batch 7/17, Loss G: 4.502958297729492, Loss D: 0.05060373246669769\n",
      "Epoch 62/200, Batch 8/17, Loss G: 4.46978759765625, Loss D: 0.01788594201207161\n",
      "Epoch 62/200, Batch 9/17, Loss G: 4.401191711425781, Loss D: 0.022122614085674286\n",
      "Epoch 62/200, Batch 10/17, Loss G: 4.471741199493408, Loss D: 0.0272566769272089\n",
      "Epoch 62/200, Batch 11/17, Loss G: 4.538462162017822, Loss D: 0.024031754583120346\n",
      "Epoch 62/200, Batch 12/17, Loss G: 4.506223678588867, Loss D: 0.03263822942972183\n",
      "Epoch 62/200, Batch 13/17, Loss G: 4.7314300537109375, Loss D: 0.02208622545003891\n",
      "Epoch 62/200, Batch 14/17, Loss G: 4.683691501617432, Loss D: 0.029652539640665054\n",
      "Epoch 62/200, Batch 15/17, Loss G: 4.389837741851807, Loss D: 0.04231631010770798\n",
      "Epoch 62/200, Batch 16/17, Loss G: 4.782161712646484, Loss D: 0.0671510398387909\n",
      "Epoch 63/200, Batch 0/17, Loss G: 4.3092570304870605, Loss D: 0.11399146914482117\n",
      "Epoch 63/200, Batch 1/17, Loss G: 4.767549514770508, Loss D: 0.2619968056678772\n",
      "Epoch 63/200, Batch 2/17, Loss G: 4.358138561248779, Loss D: 0.05242164433002472\n",
      "Epoch 63/200, Batch 3/17, Loss G: 4.741520881652832, Loss D: 0.04882799834012985\n",
      "Epoch 63/200, Batch 4/17, Loss G: 4.259303092956543, Loss D: 0.16919930279254913\n",
      "Epoch 63/200, Batch 5/17, Loss G: 4.75068998336792, Loss D: 0.41637521982192993\n",
      "Epoch 63/200, Batch 6/17, Loss G: 4.719297409057617, Loss D: 0.3134826421737671\n",
      "Epoch 63/200, Batch 7/17, Loss G: 4.221582412719727, Loss D: 0.16392502188682556\n",
      "Epoch 63/200, Batch 8/17, Loss G: 4.427343368530273, Loss D: 0.10826549679040909\n",
      "Epoch 63/200, Batch 9/17, Loss G: 4.89204216003418, Loss D: 0.11552546918392181\n",
      "Epoch 63/200, Batch 10/17, Loss G: 4.084713459014893, Loss D: 0.14543400704860687\n",
      "Epoch 63/200, Batch 11/17, Loss G: 4.3980207443237305, Loss D: 0.08464151620864868\n",
      "Epoch 63/200, Batch 12/17, Loss G: 4.115561485290527, Loss D: 0.07752475142478943\n",
      "Epoch 63/200, Batch 13/17, Loss G: 4.685332298278809, Loss D: 0.04782768711447716\n",
      "Epoch 63/200, Batch 14/17, Loss G: 4.470380783081055, Loss D: 0.11660804599523544\n",
      "Epoch 63/200, Batch 15/17, Loss G: 3.793522834777832, Loss D: 0.2439417541027069\n",
      "Epoch 63/200, Batch 16/17, Loss G: 4.453116416931152, Loss D: 0.145577073097229\n",
      "Epoch 64/200, Batch 0/17, Loss G: 4.53056526184082, Loss D: 0.10471983253955841\n",
      "Epoch 64/200, Batch 1/17, Loss G: 4.362886428833008, Loss D: 0.0929688885807991\n",
      "Epoch 64/200, Batch 2/17, Loss G: 4.485064506530762, Loss D: 0.07204500585794449\n",
      "Epoch 64/200, Batch 3/17, Loss G: 4.495761871337891, Loss D: 0.0625455230474472\n",
      "Epoch 64/200, Batch 4/17, Loss G: 4.102097511291504, Loss D: 0.10407400876283646\n",
      "Epoch 64/200, Batch 5/17, Loss G: 4.620938301086426, Loss D: 0.15022426843643188\n",
      "Epoch 64/200, Batch 6/17, Loss G: 4.28749942779541, Loss D: 0.057731062173843384\n",
      "Epoch 64/200, Batch 7/17, Loss G: 4.182886600494385, Loss D: 0.07780109345912933\n",
      "Epoch 64/200, Batch 8/17, Loss G: 4.814640045166016, Loss D: 0.07992306351661682\n",
      "Epoch 64/200, Batch 9/17, Loss G: 4.363805294036865, Loss D: 0.05624842643737793\n",
      "Epoch 64/200, Batch 10/17, Loss G: 4.915854454040527, Loss D: 0.027118459343910217\n",
      "Epoch 64/200, Batch 11/17, Loss G: 4.522844314575195, Loss D: 0.030496783554553986\n",
      "Epoch 64/200, Batch 12/17, Loss G: 4.394553184509277, Loss D: 0.04405553638935089\n",
      "Epoch 64/200, Batch 13/17, Loss G: 4.160482406616211, Loss D: 0.06041271239519119\n",
      "Epoch 64/200, Batch 14/17, Loss G: 4.491426944732666, Loss D: 0.06032063066959381\n",
      "Epoch 64/200, Batch 15/17, Loss G: 4.549863815307617, Loss D: 0.06031263247132301\n",
      "Epoch 64/200, Batch 16/17, Loss G: 4.065927505493164, Loss D: 0.11532753705978394\n",
      "Epoch 65/200, Batch 0/17, Loss G: 4.630274772644043, Loss D: 0.07413646578788757\n",
      "Epoch 65/200, Batch 1/17, Loss G: 4.5712103843688965, Loss D: 0.026228073984384537\n",
      "Epoch 65/200, Batch 2/17, Loss G: 4.343191146850586, Loss D: 0.05445612594485283\n",
      "Epoch 65/200, Batch 3/17, Loss G: 4.590914726257324, Loss D: 0.05174091458320618\n",
      "Epoch 65/200, Batch 4/17, Loss G: 4.712961196899414, Loss D: 0.06608632206916809\n",
      "Epoch 65/200, Batch 5/17, Loss G: 4.428656578063965, Loss D: 0.08957070112228394\n",
      "Epoch 65/200, Batch 6/17, Loss G: 4.800634384155273, Loss D: 0.14206594228744507\n",
      "Epoch 65/200, Batch 7/17, Loss G: 3.819131851196289, Loss D: 0.13755053281784058\n",
      "Epoch 65/200, Batch 8/17, Loss G: 4.422137260437012, Loss D: 0.03914392367005348\n",
      "Epoch 65/200, Batch 9/17, Loss G: 4.626636981964111, Loss D: 0.036860156804323196\n",
      "Epoch 65/200, Batch 10/17, Loss G: 4.405066013336182, Loss D: 0.0331822969019413\n",
      "Epoch 65/200, Batch 11/17, Loss G: 4.337417125701904, Loss D: 0.042508237063884735\n",
      "Epoch 65/200, Batch 12/17, Loss G: 4.810323238372803, Loss D: 0.024934135377407074\n",
      "Epoch 65/200, Batch 13/17, Loss G: 4.536303997039795, Loss D: 0.029118018224835396\n",
      "Epoch 65/200, Batch 14/17, Loss G: 4.5125837326049805, Loss D: 0.041290827095508575\n",
      "Epoch 65/200, Batch 15/17, Loss G: 4.358314037322998, Loss D: 0.040184494107961655\n",
      "Epoch 65/200, Batch 16/17, Loss G: 4.595768928527832, Loss D: 0.028850913047790527\n",
      "Epoch 66/200, Batch 0/17, Loss G: 4.352133274078369, Loss D: 0.03634633123874664\n",
      "Epoch 66/200, Batch 1/17, Loss G: 4.404589653015137, Loss D: 0.02799314633011818\n",
      "Epoch 66/200, Batch 2/17, Loss G: 4.486032485961914, Loss D: 0.033065374940633774\n",
      "Epoch 66/200, Batch 3/17, Loss G: 4.713744163513184, Loss D: 0.12052873522043228\n",
      "Epoch 66/200, Batch 4/17, Loss G: 4.050122261047363, Loss D: 0.18252548575401306\n",
      "Epoch 66/200, Batch 5/17, Loss G: 4.698153495788574, Loss D: 0.0735040158033371\n",
      "Epoch 66/200, Batch 6/17, Loss G: 4.6019768714904785, Loss D: 0.020514067262411118\n",
      "Epoch 66/200, Batch 7/17, Loss G: 4.245608806610107, Loss D: 0.042044222354888916\n",
      "Epoch 66/200, Batch 8/17, Loss G: 4.660564422607422, Loss D: 0.04288947582244873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200, Batch 9/17, Loss G: 4.5099592208862305, Loss D: 0.04119611904025078\n",
      "Epoch 66/200, Batch 10/17, Loss G: 4.321762561798096, Loss D: 0.048429060727357864\n",
      "Epoch 66/200, Batch 11/17, Loss G: 4.402548789978027, Loss D: 0.04558512195944786\n",
      "Epoch 66/200, Batch 12/17, Loss G: 4.661815643310547, Loss D: 0.051130469888448715\n",
      "Epoch 66/200, Batch 13/17, Loss G: 4.05723762512207, Loss D: 0.10759657621383667\n",
      "Epoch 66/200, Batch 14/17, Loss G: 4.6490702629089355, Loss D: 0.1960737705230713\n",
      "Epoch 66/200, Batch 15/17, Loss G: 3.988895893096924, Loss D: 0.11497889459133148\n",
      "Epoch 66/200, Batch 16/17, Loss G: 4.699736595153809, Loss D: 0.04807228222489357\n",
      "Epoch 67/200, Batch 0/17, Loss G: 4.283626556396484, Loss D: 0.05867858976125717\n",
      "Epoch 67/200, Batch 1/17, Loss G: 4.6324849128723145, Loss D: 0.06943368166685104\n",
      "Epoch 67/200, Batch 2/17, Loss G: 4.110683441162109, Loss D: 0.12341344356536865\n",
      "Epoch 67/200, Batch 3/17, Loss G: 4.546615123748779, Loss D: 0.21451899409294128\n",
      "Epoch 67/200, Batch 4/17, Loss G: 4.26216983795166, Loss D: 0.07001272588968277\n",
      "Epoch 67/200, Batch 5/17, Loss G: 4.348299503326416, Loss D: 0.04054177552461624\n",
      "Epoch 67/200, Batch 6/17, Loss G: 4.413093566894531, Loss D: 0.0695803314447403\n",
      "Epoch 67/200, Batch 7/17, Loss G: 4.18091344833374, Loss D: 0.07405205816030502\n",
      "Epoch 67/200, Batch 8/17, Loss G: 4.536181449890137, Loss D: 0.047127366065979004\n",
      "Epoch 67/200, Batch 9/17, Loss G: 4.362101078033447, Loss D: 0.04828904941678047\n",
      "Epoch 67/200, Batch 10/17, Loss G: 4.370935916900635, Loss D: 0.037092916667461395\n",
      "Epoch 67/200, Batch 11/17, Loss G: 4.6805338859558105, Loss D: 0.05617034062743187\n",
      "Epoch 67/200, Batch 12/17, Loss G: 4.363100528717041, Loss D: 0.04438472166657448\n",
      "Epoch 67/200, Batch 13/17, Loss G: 4.348966598510742, Loss D: 0.02637237310409546\n",
      "Epoch 67/200, Batch 14/17, Loss G: 4.4478044509887695, Loss D: 0.04118426889181137\n",
      "Epoch 67/200, Batch 15/17, Loss G: 4.533833980560303, Loss D: 0.02716202288866043\n",
      "Epoch 67/200, Batch 16/17, Loss G: 4.395261764526367, Loss D: 0.04023145139217377\n",
      "Epoch 68/200, Batch 0/17, Loss G: 4.420677661895752, Loss D: 0.055637769401073456\n",
      "Epoch 68/200, Batch 1/17, Loss G: 4.642214775085449, Loss D: 0.08339042961597443\n",
      "Epoch 68/200, Batch 2/17, Loss G: 4.208541393280029, Loss D: 0.0669737234711647\n",
      "Epoch 68/200, Batch 3/17, Loss G: 4.855423450469971, Loss D: 0.02948560193181038\n",
      "Epoch 68/200, Batch 4/17, Loss G: 4.549480438232422, Loss D: 0.03324054926633835\n",
      "Epoch 68/200, Batch 5/17, Loss G: 4.432708740234375, Loss D: 0.02057556062936783\n",
      "Epoch 68/200, Batch 6/17, Loss G: 4.35330867767334, Loss D: 0.053507253527641296\n",
      "Epoch 68/200, Batch 7/17, Loss G: 4.20103645324707, Loss D: 0.06437768787145615\n",
      "Epoch 68/200, Batch 8/17, Loss G: 4.763197898864746, Loss D: 0.08375655114650726\n",
      "Epoch 68/200, Batch 9/17, Loss G: 4.095451831817627, Loss D: 0.07070101797580719\n",
      "Epoch 68/200, Batch 10/17, Loss G: 4.625424861907959, Loss D: 0.048481687903404236\n",
      "Epoch 68/200, Batch 11/17, Loss G: 4.245022773742676, Loss D: 0.059235263615846634\n",
      "Epoch 68/200, Batch 12/17, Loss G: 4.625088691711426, Loss D: 0.056318026036024094\n",
      "Epoch 68/200, Batch 13/17, Loss G: 4.383827209472656, Loss D: 0.032024674117565155\n",
      "Epoch 68/200, Batch 14/17, Loss G: 4.276905059814453, Loss D: 0.03948107734322548\n",
      "Epoch 68/200, Batch 15/17, Loss G: 4.545755863189697, Loss D: 0.026726339012384415\n",
      "Epoch 68/200, Batch 16/17, Loss G: 4.497311115264893, Loss D: 0.025053180754184723\n",
      "Epoch 69/200, Batch 0/17, Loss G: 4.266875267028809, Loss D: 0.058929022401571274\n",
      "Epoch 69/200, Batch 1/17, Loss G: 4.650746822357178, Loss D: 0.04360707104206085\n",
      "Epoch 69/200, Batch 2/17, Loss G: 4.3805766105651855, Loss D: 0.02655009925365448\n",
      "Epoch 69/200, Batch 3/17, Loss G: 4.519986152648926, Loss D: 0.0352151095867157\n",
      "Epoch 69/200, Batch 4/17, Loss G: 4.523416996002197, Loss D: 0.04921526089310646\n",
      "Epoch 69/200, Batch 5/17, Loss G: 4.040768623352051, Loss D: 0.09115836024284363\n",
      "Epoch 69/200, Batch 6/17, Loss G: 4.615666389465332, Loss D: 0.12290332466363907\n",
      "Epoch 69/200, Batch 7/17, Loss G: 3.956921100616455, Loss D: 0.14306779205799103\n",
      "Epoch 69/200, Batch 8/17, Loss G: 4.574006080627441, Loss D: 0.07387488335371017\n",
      "Epoch 69/200, Batch 9/17, Loss G: 4.4045610427856445, Loss D: 0.0393717885017395\n",
      "Epoch 69/200, Batch 10/17, Loss G: 4.6133575439453125, Loss D: 0.018566980957984924\n",
      "Epoch 69/200, Batch 11/17, Loss G: 4.387084007263184, Loss D: 0.046839822083711624\n",
      "Epoch 69/200, Batch 12/17, Loss G: 4.678999900817871, Loss D: 0.08420521020889282\n",
      "Epoch 69/200, Batch 13/17, Loss G: 4.297168254852295, Loss D: 0.07739866524934769\n",
      "Epoch 69/200, Batch 14/17, Loss G: 4.900911808013916, Loss D: 0.021344926208257675\n",
      "Epoch 69/200, Batch 15/17, Loss G: 4.730753421783447, Loss D: 0.03546632081270218\n",
      "Epoch 69/200, Batch 16/17, Loss G: 4.44872522354126, Loss D: 0.032112300395965576\n",
      "Epoch 70/200, Batch 0/17, Loss G: 4.386430263519287, Loss D: 0.037281058728694916\n",
      "Epoch 70/200, Batch 1/17, Loss G: 4.665522575378418, Loss D: 0.04313309118151665\n",
      "Epoch 70/200, Batch 2/17, Loss G: 4.532961368560791, Loss D: 0.031594716012477875\n",
      "Epoch 70/200, Batch 3/17, Loss G: 4.373786926269531, Loss D: 0.02428976632654667\n",
      "Epoch 70/200, Batch 4/17, Loss G: 4.238511085510254, Loss D: 0.029415590688586235\n",
      "Epoch 70/200, Batch 5/17, Loss G: 4.220349311828613, Loss D: 0.0474160835146904\n",
      "Epoch 70/200, Batch 6/17, Loss G: 4.6390380859375, Loss D: 0.04473401978611946\n",
      "Epoch 70/200, Batch 7/17, Loss G: 4.459356784820557, Loss D: 0.03381288796663284\n",
      "Epoch 70/200, Batch 8/17, Loss G: 4.2986907958984375, Loss D: 0.038273852318525314\n",
      "Epoch 70/200, Batch 9/17, Loss G: 4.625632286071777, Loss D: 0.051003891974687576\n",
      "Epoch 70/200, Batch 10/17, Loss G: 4.494108200073242, Loss D: 0.06307530403137207\n",
      "Epoch 70/200, Batch 11/17, Loss G: 4.5057244300842285, Loss D: 0.05381377413868904\n",
      "Epoch 70/200, Batch 12/17, Loss G: 4.464151859283447, Loss D: 0.04375016316771507\n",
      "Epoch 70/200, Batch 13/17, Loss G: 4.751893997192383, Loss D: 0.022084752097725868\n",
      "Epoch 70/200, Batch 14/17, Loss G: 4.5286335945129395, Loss D: 0.024781476706266403\n",
      "Epoch 70/200, Batch 15/17, Loss G: 4.404058456420898, Loss D: 0.025728382170200348\n",
      "Epoch 70/200, Batch 16/17, Loss G: 4.365817546844482, Loss D: 0.013440622948110104\n",
      "Epoch 71/200, Batch 0/17, Loss G: 4.263876914978027, Loss D: 0.04029744490981102\n",
      "Epoch 71/200, Batch 1/17, Loss G: 4.843021392822266, Loss D: 0.06693451851606369\n",
      "Epoch 71/200, Batch 2/17, Loss G: 4.168362617492676, Loss D: 0.04735572636127472\n",
      "Epoch 71/200, Batch 3/17, Loss G: 4.583205223083496, Loss D: 0.023358482867479324\n",
      "Epoch 71/200, Batch 4/17, Loss G: 4.4497575759887695, Loss D: 0.05534831061959267\n",
      "Epoch 71/200, Batch 5/17, Loss G: 4.204239845275879, Loss D: 0.08479896187782288\n",
      "Epoch 71/200, Batch 6/17, Loss G: 4.540056228637695, Loss D: 0.053646113723516464\n",
      "Epoch 71/200, Batch 7/17, Loss G: 4.686917304992676, Loss D: 0.03407520055770874\n",
      "Epoch 71/200, Batch 8/17, Loss G: 4.222663879394531, Loss D: 0.05384449288249016\n",
      "Epoch 71/200, Batch 9/17, Loss G: 4.886715888977051, Loss D: 0.05194936692714691\n",
      "Epoch 71/200, Batch 10/17, Loss G: 4.52720832824707, Loss D: 0.017595242708921432\n",
      "Epoch 71/200, Batch 11/17, Loss G: 4.145722389221191, Loss D: 0.04209865257143974\n",
      "Epoch 71/200, Batch 12/17, Loss G: 4.645700454711914, Loss D: 0.10369376838207245\n",
      "Epoch 71/200, Batch 13/17, Loss G: 4.113609313964844, Loss D: 0.17479296028614044\n",
      "Epoch 71/200, Batch 14/17, Loss G: 4.577301979064941, Loss D: 0.08731767535209656\n",
      "Epoch 71/200, Batch 15/17, Loss G: 4.420782089233398, Loss D: 0.015740634873509407\n",
      "Epoch 71/200, Batch 16/17, Loss G: 4.02803897857666, Loss D: 0.1006670594215393\n",
      "Epoch 72/200, Batch 0/17, Loss G: 4.66302490234375, Loss D: 0.26031816005706787\n",
      "Epoch 72/200, Batch 1/17, Loss G: 4.23101282119751, Loss D: 0.09040611982345581\n",
      "Epoch 72/200, Batch 2/17, Loss G: 4.154870510101318, Loss D: 0.05665227025747299\n",
      "Epoch 72/200, Batch 3/17, Loss G: 4.932779312133789, Loss D: 0.2051209956407547\n",
      "Epoch 72/200, Batch 4/17, Loss G: 4.022590637207031, Loss D: 0.14010357856750488\n",
      "Epoch 72/200, Batch 5/17, Loss G: 4.772964000701904, Loss D: 0.0694478452205658\n",
      "Epoch 72/200, Batch 6/17, Loss G: 4.192007064819336, Loss D: 0.057839807122945786\n",
      "Epoch 72/200, Batch 7/17, Loss G: 4.356195449829102, Loss D: 0.1417624056339264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200, Batch 8/17, Loss G: 4.03173828125, Loss D: 0.1725517064332962\n",
      "Epoch 72/200, Batch 9/17, Loss G: 4.4998321533203125, Loss D: 0.1089840680360794\n",
      "Epoch 72/200, Batch 10/17, Loss G: 4.243256568908691, Loss D: 0.05873488262295723\n",
      "Epoch 72/200, Batch 11/17, Loss G: 4.275179862976074, Loss D: 0.05019453912973404\n",
      "Epoch 72/200, Batch 12/17, Loss G: 4.4980363845825195, Loss D: 0.10801084339618683\n",
      "Epoch 72/200, Batch 13/17, Loss G: 4.172476291656494, Loss D: 0.09389650076627731\n",
      "Epoch 72/200, Batch 14/17, Loss G: 4.549327850341797, Loss D: 0.04648056998848915\n",
      "Epoch 72/200, Batch 15/17, Loss G: 4.472647666931152, Loss D: 0.03064173460006714\n",
      "Epoch 72/200, Batch 16/17, Loss G: 4.5138258934021, Loss D: 0.03561439365148544\n",
      "Epoch 73/200, Batch 0/17, Loss G: 4.383922576904297, Loss D: 0.07350115478038788\n",
      "Epoch 73/200, Batch 1/17, Loss G: 4.172855854034424, Loss D: 0.049683522433042526\n",
      "Epoch 73/200, Batch 2/17, Loss G: 4.148107528686523, Loss D: 0.031471073627471924\n",
      "Epoch 73/200, Batch 3/17, Loss G: 4.634393692016602, Loss D: 0.08939781785011292\n",
      "Epoch 73/200, Batch 4/17, Loss G: 4.0044732093811035, Loss D: 0.16300654411315918\n",
      "Epoch 73/200, Batch 5/17, Loss G: 4.807938575744629, Loss D: 0.1009775921702385\n",
      "Epoch 73/200, Batch 6/17, Loss G: 4.149123191833496, Loss D: 0.039460912346839905\n",
      "Epoch 73/200, Batch 7/17, Loss G: 4.332900524139404, Loss D: 0.021175168454647064\n",
      "Epoch 73/200, Batch 8/17, Loss G: 4.4702863693237305, Loss D: 0.052267804741859436\n",
      "Epoch 73/200, Batch 9/17, Loss G: 4.337255477905273, Loss D: 0.058709628880023956\n",
      "Epoch 73/200, Batch 10/17, Loss G: 4.479003429412842, Loss D: 0.04815750941634178\n",
      "Epoch 73/200, Batch 11/17, Loss G: 4.318404197692871, Loss D: 0.04530976712703705\n",
      "Epoch 73/200, Batch 12/17, Loss G: 4.779219627380371, Loss D: 0.03388466313481331\n",
      "Epoch 73/200, Batch 13/17, Loss G: 4.346798419952393, Loss D: 0.019893549382686615\n",
      "Epoch 73/200, Batch 14/17, Loss G: 4.377842903137207, Loss D: 0.03755900263786316\n",
      "Epoch 73/200, Batch 15/17, Loss G: 4.206830978393555, Loss D: 0.044168099761009216\n",
      "Epoch 73/200, Batch 16/17, Loss G: 4.434469223022461, Loss D: 0.03318050876259804\n",
      "Epoch 74/200, Batch 0/17, Loss G: 4.2529072761535645, Loss D: 0.06025231257081032\n",
      "Epoch 74/200, Batch 1/17, Loss G: 4.367785453796387, Loss D: 0.07936713844537735\n",
      "Epoch 74/200, Batch 2/17, Loss G: 4.156943321228027, Loss D: 0.060313835740089417\n",
      "Epoch 74/200, Batch 3/17, Loss G: 4.642433166503906, Loss D: 0.025206688791513443\n",
      "Epoch 74/200, Batch 4/17, Loss G: 4.601492404937744, Loss D: 0.07749216258525848\n",
      "Epoch 74/200, Batch 5/17, Loss G: 3.865929126739502, Loss D: 0.127881720662117\n",
      "Epoch 74/200, Batch 6/17, Loss G: 4.691011905670166, Loss D: 0.09272345900535583\n",
      "Epoch 74/200, Batch 7/17, Loss G: 4.355975151062012, Loss D: 0.02018056809902191\n",
      "Epoch 74/200, Batch 8/17, Loss G: 4.310601234436035, Loss D: 0.03666972741484642\n",
      "Epoch 74/200, Batch 9/17, Loss G: 4.558931350708008, Loss D: 0.025453004986047745\n",
      "Epoch 74/200, Batch 10/17, Loss G: 4.350913047790527, Loss D: 0.01886581815779209\n",
      "Epoch 74/200, Batch 11/17, Loss G: 4.255600929260254, Loss D: 0.03176464885473251\n",
      "Epoch 74/200, Batch 12/17, Loss G: 4.2508225440979, Loss D: 0.039045631885528564\n",
      "Epoch 74/200, Batch 13/17, Loss G: 4.516836643218994, Loss D: 0.022522855550050735\n",
      "Epoch 74/200, Batch 14/17, Loss G: 4.343011379241943, Loss D: 0.061658825725317\n",
      "Epoch 74/200, Batch 15/17, Loss G: 4.001067161560059, Loss D: 0.06981014460325241\n",
      "Epoch 74/200, Batch 16/17, Loss G: 4.300539970397949, Loss D: 0.03701556473970413\n",
      "Epoch 75/200, Batch 0/17, Loss G: 4.4228105545043945, Loss D: 0.033162541687488556\n",
      "Epoch 75/200, Batch 1/17, Loss G: 4.180456161499023, Loss D: 0.059871673583984375\n",
      "Epoch 75/200, Batch 2/17, Loss G: 4.500368595123291, Loss D: 0.04633544757962227\n",
      "Epoch 75/200, Batch 3/17, Loss G: 4.419948577880859, Loss D: 0.022072438150644302\n",
      "Epoch 75/200, Batch 4/17, Loss G: 4.552411079406738, Loss D: 0.02487916871905327\n",
      "Epoch 75/200, Batch 5/17, Loss G: 4.655520439147949, Loss D: 0.022419987246394157\n",
      "Epoch 75/200, Batch 6/17, Loss G: 4.151561737060547, Loss D: 0.03125454857945442\n",
      "Epoch 75/200, Batch 7/17, Loss G: 4.5240068435668945, Loss D: 0.028964659199118614\n",
      "Epoch 75/200, Batch 8/17, Loss G: 4.485137939453125, Loss D: 0.041293781250715256\n",
      "Epoch 75/200, Batch 9/17, Loss G: 4.278296947479248, Loss D: 0.025424523279070854\n",
      "Epoch 75/200, Batch 10/17, Loss G: 4.562473773956299, Loss D: 0.021997950971126556\n",
      "Epoch 75/200, Batch 11/17, Loss G: 4.555389404296875, Loss D: 0.05479703098535538\n",
      "Epoch 75/200, Batch 12/17, Loss G: 3.771061420440674, Loss D: 0.17094793915748596\n",
      "Epoch 75/200, Batch 13/17, Loss G: 4.493418216705322, Loss D: 0.1529005914926529\n",
      "Epoch 75/200, Batch 14/17, Loss G: 4.299027919769287, Loss D: 0.07175786793231964\n",
      "Epoch 75/200, Batch 15/17, Loss G: 4.452579975128174, Loss D: 0.023489098995923996\n",
      "Epoch 75/200, Batch 16/17, Loss G: 4.680337905883789, Loss D: 0.03794224560260773\n",
      "Epoch 76/200, Batch 0/17, Loss G: 4.042061805725098, Loss D: 0.0732448548078537\n",
      "Epoch 76/200, Batch 1/17, Loss G: 4.328339576721191, Loss D: 0.11925403028726578\n",
      "Epoch 76/200, Batch 2/17, Loss G: 4.227841377258301, Loss D: 0.06398317962884903\n",
      "Epoch 76/200, Batch 3/17, Loss G: 4.5135698318481445, Loss D: 0.06181942671537399\n",
      "Epoch 76/200, Batch 4/17, Loss G: 4.4268035888671875, Loss D: 0.06854790449142456\n",
      "Epoch 76/200, Batch 5/17, Loss G: 4.411800861358643, Loss D: 0.08457177877426147\n",
      "Epoch 76/200, Batch 6/17, Loss G: 3.926837682723999, Loss D: 0.12199578434228897\n",
      "Epoch 76/200, Batch 7/17, Loss G: 4.646480560302734, Loss D: 0.16345854103565216\n",
      "Epoch 76/200, Batch 8/17, Loss G: 4.194091320037842, Loss D: 0.0209830142557621\n",
      "Epoch 76/200, Batch 9/17, Loss G: 3.925553321838379, Loss D: 0.09345883131027222\n",
      "Epoch 76/200, Batch 10/17, Loss G: 4.663843631744385, Loss D: 0.1989177018404007\n",
      "Epoch 76/200, Batch 11/17, Loss G: 4.056920528411865, Loss D: 0.10997861623764038\n",
      "Epoch 76/200, Batch 12/17, Loss G: 4.4638261795043945, Loss D: 0.0442577600479126\n",
      "Epoch 76/200, Batch 13/17, Loss G: 4.538694381713867, Loss D: 0.10713734477758408\n",
      "Epoch 76/200, Batch 14/17, Loss G: 3.8103268146514893, Loss D: 0.1874779760837555\n",
      "Epoch 76/200, Batch 15/17, Loss G: 4.386966228485107, Loss D: 0.04705975949764252\n",
      "Epoch 76/200, Batch 16/17, Loss G: 4.467498779296875, Loss D: 0.09183531254529953\n",
      "Epoch 77/200, Batch 0/17, Loss G: 3.5482358932495117, Loss D: 0.25909918546676636\n",
      "Epoch 77/200, Batch 1/17, Loss G: 4.536850929260254, Loss D: 0.10063125938177109\n",
      "Epoch 77/200, Batch 2/17, Loss G: 4.2256293296813965, Loss D: 0.05975063890218735\n",
      "Epoch 77/200, Batch 3/17, Loss G: 4.3466081619262695, Loss D: 0.07412570714950562\n",
      "Epoch 77/200, Batch 4/17, Loss G: 4.1887664794921875, Loss D: 0.10931500047445297\n",
      "Epoch 77/200, Batch 5/17, Loss G: 4.737459182739258, Loss D: 0.0395662896335125\n",
      "Epoch 77/200, Batch 6/17, Loss G: 4.328801155090332, Loss D: 0.06290960311889648\n",
      "Epoch 77/200, Batch 7/17, Loss G: 4.04466438293457, Loss D: 0.08912121504545212\n",
      "Epoch 77/200, Batch 8/17, Loss G: 4.591640472412109, Loss D: 0.14376896619796753\n",
      "Epoch 77/200, Batch 9/17, Loss G: 3.89756441116333, Loss D: 0.12814964354038239\n",
      "Epoch 77/200, Batch 10/17, Loss G: 4.683521747589111, Loss D: 0.06614772975444794\n",
      "Epoch 77/200, Batch 11/17, Loss G: 4.2899322509765625, Loss D: 0.036836955696344376\n",
      "Epoch 77/200, Batch 12/17, Loss G: 4.2662787437438965, Loss D: 0.04805232211947441\n",
      "Epoch 77/200, Batch 13/17, Loss G: 4.392453193664551, Loss D: 0.04286512732505798\n",
      "Epoch 77/200, Batch 14/17, Loss G: 4.285340785980225, Loss D: 0.05698544532060623\n",
      "Epoch 77/200, Batch 15/17, Loss G: 4.104971885681152, Loss D: 0.06881456822156906\n",
      "Epoch 77/200, Batch 16/17, Loss G: 4.664815425872803, Loss D: 0.06861565262079239\n",
      "Epoch 78/200, Batch 0/17, Loss G: 4.159779071807861, Loss D: 0.05632985010743141\n",
      "Epoch 78/200, Batch 1/17, Loss G: 4.491404056549072, Loss D: 0.03358309343457222\n",
      "Epoch 78/200, Batch 2/17, Loss G: 4.424213886260986, Loss D: 0.04687663912773132\n",
      "Epoch 78/200, Batch 3/17, Loss G: 4.35147762298584, Loss D: 0.027166258543729782\n",
      "Epoch 78/200, Batch 4/17, Loss G: 4.496611595153809, Loss D: 0.03946381062269211\n",
      "Epoch 78/200, Batch 5/17, Loss G: 4.254314422607422, Loss D: 0.06016235053539276\n",
      "Epoch 78/200, Batch 6/17, Loss G: 4.364367485046387, Loss D: 0.12773005664348602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200, Batch 7/17, Loss G: 3.708836793899536, Loss D: 0.16890785098075867\n",
      "Epoch 78/200, Batch 8/17, Loss G: 4.367725849151611, Loss D: 0.08462005853652954\n",
      "Epoch 78/200, Batch 9/17, Loss G: 4.18035364151001, Loss D: 0.054955944418907166\n",
      "Epoch 78/200, Batch 10/17, Loss G: 4.386807918548584, Loss D: 0.04453328996896744\n",
      "Epoch 78/200, Batch 11/17, Loss G: 4.258638381958008, Loss D: 0.04149762541055679\n",
      "Epoch 78/200, Batch 12/17, Loss G: 4.249216079711914, Loss D: 0.02511351928114891\n",
      "Epoch 78/200, Batch 13/17, Loss G: 4.187201976776123, Loss D: 0.03678332269191742\n",
      "Epoch 78/200, Batch 14/17, Loss G: 4.293637752532959, Loss D: 0.023397594690322876\n",
      "Epoch 78/200, Batch 15/17, Loss G: 4.3173418045043945, Loss D: 0.03650912642478943\n",
      "Epoch 78/200, Batch 16/17, Loss G: 4.506956577301025, Loss D: 0.03531583398580551\n",
      "Epoch 79/200, Batch 0/17, Loss G: 4.39448356628418, Loss D: 0.02198639139533043\n",
      "Epoch 79/200, Batch 1/17, Loss G: 4.3816046714782715, Loss D: 0.02614399790763855\n",
      "Epoch 79/200, Batch 2/17, Loss G: 4.3058600425720215, Loss D: 0.026072800159454346\n",
      "Epoch 79/200, Batch 3/17, Loss G: 4.371438980102539, Loss D: 0.04143105819821358\n",
      "Epoch 79/200, Batch 4/17, Loss G: 4.127469539642334, Loss D: 0.043607085943222046\n",
      "Epoch 79/200, Batch 5/17, Loss G: 4.512283802032471, Loss D: 0.0365171879529953\n",
      "Epoch 79/200, Batch 6/17, Loss G: 4.244536876678467, Loss D: 0.02016642317175865\n",
      "Epoch 79/200, Batch 7/17, Loss G: 4.31308126449585, Loss D: 0.028572876006364822\n",
      "Epoch 79/200, Batch 8/17, Loss G: 4.567878723144531, Loss D: 0.0350145623087883\n",
      "Epoch 79/200, Batch 9/17, Loss G: 4.212899208068848, Loss D: 0.02159520797431469\n",
      "Epoch 79/200, Batch 10/17, Loss G: 4.641910076141357, Loss D: 0.006646657362580299\n",
      "Epoch 79/200, Batch 11/17, Loss G: 4.654860496520996, Loss D: 0.014780164696276188\n",
      "Epoch 79/200, Batch 12/17, Loss G: 4.22914981842041, Loss D: 0.02281208522617817\n",
      "Epoch 79/200, Batch 13/17, Loss G: 4.553703308105469, Loss D: 0.011941488832235336\n",
      "Epoch 79/200, Batch 14/17, Loss G: 4.437281608581543, Loss D: 0.018328486010432243\n",
      "Epoch 79/200, Batch 15/17, Loss G: 4.354778289794922, Loss D: 0.02412758767604828\n",
      "Epoch 79/200, Batch 16/17, Loss G: 4.1704792976379395, Loss D: 0.020932231098413467\n",
      "Epoch 80/200, Batch 0/17, Loss G: 4.436526298522949, Loss D: 0.01604810543358326\n",
      "Epoch 80/200, Batch 1/17, Loss G: 4.344846725463867, Loss D: 0.02603091113269329\n",
      "Epoch 80/200, Batch 2/17, Loss G: 4.183408737182617, Loss D: 0.035933658480644226\n",
      "Epoch 80/200, Batch 3/17, Loss G: 4.502087593078613, Loss D: 0.07012706995010376\n",
      "Epoch 80/200, Batch 4/17, Loss G: 3.753882646560669, Loss D: 0.18093430995941162\n",
      "Epoch 80/200, Batch 5/17, Loss G: 4.750354290008545, Loss D: 0.12495139241218567\n",
      "Epoch 80/200, Batch 6/17, Loss G: 4.092597007751465, Loss D: 0.053065232932567596\n",
      "Epoch 80/200, Batch 7/17, Loss G: 4.5326690673828125, Loss D: 0.06865722686052322\n",
      "Epoch 80/200, Batch 8/17, Loss G: 4.143652439117432, Loss D: 0.047227755188941956\n",
      "Epoch 80/200, Batch 9/17, Loss G: 4.3708038330078125, Loss D: 0.0239282064139843\n",
      "Epoch 80/200, Batch 10/17, Loss G: 4.596918106079102, Loss D: 0.02988375723361969\n",
      "Epoch 80/200, Batch 11/17, Loss G: 4.224554538726807, Loss D: 0.037229184061288834\n",
      "Epoch 80/200, Batch 12/17, Loss G: 4.298213958740234, Loss D: 0.017538217827677727\n",
      "Epoch 80/200, Batch 13/17, Loss G: 4.190762042999268, Loss D: 0.04347086697816849\n",
      "Epoch 80/200, Batch 14/17, Loss G: 4.205077171325684, Loss D: 0.023902911692857742\n",
      "Epoch 80/200, Batch 15/17, Loss G: 4.532001972198486, Loss D: 0.03924275189638138\n",
      "Epoch 80/200, Batch 16/17, Loss G: 3.9391298294067383, Loss D: 0.16859301924705505\n",
      "Epoch 81/200, Batch 0/17, Loss G: 4.514107704162598, Loss D: 0.28843605518341064\n",
      "Epoch 81/200, Batch 1/17, Loss G: 4.314039707183838, Loss D: 0.0301311407238245\n",
      "Epoch 81/200, Batch 2/17, Loss G: 4.157905578613281, Loss D: 0.086208775639534\n",
      "Epoch 81/200, Batch 3/17, Loss G: 4.519944667816162, Loss D: 0.06403689831495285\n",
      "Epoch 81/200, Batch 4/17, Loss G: 4.235522270202637, Loss D: 0.058155979961156845\n",
      "Epoch 81/200, Batch 5/17, Loss G: 4.419766426086426, Loss D: 0.027776721864938736\n",
      "Epoch 81/200, Batch 6/17, Loss G: 4.148913860321045, Loss D: 0.03223252296447754\n",
      "Epoch 81/200, Batch 7/17, Loss G: 4.488127708435059, Loss D: 0.04719417914748192\n",
      "Epoch 81/200, Batch 8/17, Loss G: 4.081112384796143, Loss D: 0.08722290396690369\n",
      "Epoch 81/200, Batch 9/17, Loss G: 4.508549213409424, Loss D: 0.09886028617620468\n",
      "Epoch 81/200, Batch 10/17, Loss G: 4.039593696594238, Loss D: 0.05780598893761635\n",
      "Epoch 81/200, Batch 11/17, Loss G: 4.3575592041015625, Loss D: 0.04435988515615463\n",
      "Epoch 81/200, Batch 12/17, Loss G: 4.049731254577637, Loss D: 0.043056733906269073\n",
      "Epoch 81/200, Batch 13/17, Loss G: 4.223263263702393, Loss D: 0.02380264177918434\n",
      "Epoch 81/200, Batch 14/17, Loss G: 4.592262268066406, Loss D: 0.05090964585542679\n",
      "Epoch 81/200, Batch 15/17, Loss G: 3.758641242980957, Loss D: 0.13374482095241547\n",
      "Epoch 81/200, Batch 16/17, Loss G: 4.574933052062988, Loss D: 0.22235094010829926\n",
      "Epoch 82/200, Batch 0/17, Loss G: 3.9825973510742188, Loss D: 0.06751475483179092\n",
      "Epoch 82/200, Batch 1/17, Loss G: 4.233715057373047, Loss D: 0.09691400825977325\n",
      "Epoch 82/200, Batch 2/17, Loss G: 4.295845031738281, Loss D: 0.17719972133636475\n",
      "Epoch 82/200, Batch 3/17, Loss G: 4.209872722625732, Loss D: 0.13811728358268738\n",
      "Epoch 82/200, Batch 4/17, Loss G: 4.537506103515625, Loss D: 0.18485495448112488\n",
      "Epoch 82/200, Batch 5/17, Loss G: 3.9684762954711914, Loss D: 0.1540255844593048\n",
      "Epoch 82/200, Batch 6/17, Loss G: 4.271946907043457, Loss D: 0.061486225575208664\n",
      "Epoch 82/200, Batch 7/17, Loss G: 4.405702590942383, Loss D: 0.12758010625839233\n",
      "Epoch 82/200, Batch 8/17, Loss G: 3.961848258972168, Loss D: 0.10556669533252716\n",
      "Epoch 82/200, Batch 9/17, Loss G: 4.521370887756348, Loss D: 0.08564244210720062\n",
      "Epoch 82/200, Batch 10/17, Loss G: 4.247870922088623, Loss D: 0.06366467475891113\n",
      "Epoch 82/200, Batch 11/17, Loss G: 4.261199951171875, Loss D: 0.05279400944709778\n",
      "Epoch 82/200, Batch 12/17, Loss G: 4.237220287322998, Loss D: 0.0664525032043457\n",
      "Epoch 82/200, Batch 13/17, Loss G: 4.136303424835205, Loss D: 0.06819507479667664\n",
      "Epoch 82/200, Batch 14/17, Loss G: 4.537450790405273, Loss D: 0.0780060738325119\n",
      "Epoch 82/200, Batch 15/17, Loss G: 3.9259090423583984, Loss D: 0.07931433618068695\n",
      "Epoch 82/200, Batch 16/17, Loss G: 4.41281795501709, Loss D: 0.08091872185468674\n",
      "Epoch 83/200, Batch 0/17, Loss G: 3.98075008392334, Loss D: 0.060060542076826096\n",
      "Epoch 83/200, Batch 1/17, Loss G: 4.275676727294922, Loss D: 0.0384516566991806\n",
      "Epoch 83/200, Batch 2/17, Loss G: 4.294619083404541, Loss D: 0.09375656396150589\n",
      "Epoch 83/200, Batch 3/17, Loss G: 3.7655704021453857, Loss D: 0.18500854074954987\n",
      "Epoch 83/200, Batch 4/17, Loss G: 4.305993556976318, Loss D: 0.17777405679225922\n",
      "Epoch 83/200, Batch 5/17, Loss G: 3.8729186058044434, Loss D: 0.10828917473554611\n",
      "Epoch 83/200, Batch 6/17, Loss G: 4.416224479675293, Loss D: 0.05893488600850105\n",
      "Epoch 83/200, Batch 7/17, Loss G: 4.279819488525391, Loss D: 0.043946973979473114\n",
      "Epoch 83/200, Batch 8/17, Loss G: 4.166298866271973, Loss D: 0.054551709443330765\n",
      "Epoch 83/200, Batch 9/17, Loss G: 4.455876350402832, Loss D: 0.056589044630527496\n",
      "Epoch 83/200, Batch 10/17, Loss G: 4.3468427658081055, Loss D: 0.0535697266459465\n",
      "Epoch 83/200, Batch 11/17, Loss G: 4.419550895690918, Loss D: 0.04651593789458275\n",
      "Epoch 83/200, Batch 12/17, Loss G: 4.165903091430664, Loss D: 0.04483889788389206\n",
      "Epoch 83/200, Batch 13/17, Loss G: 4.617853164672852, Loss D: 0.06381787359714508\n",
      "Epoch 83/200, Batch 14/17, Loss G: 4.191504001617432, Loss D: 0.04624378681182861\n",
      "Epoch 83/200, Batch 15/17, Loss G: 4.21476411819458, Loss D: 0.033104777336120605\n",
      "Epoch 83/200, Batch 16/17, Loss G: 4.462207794189453, Loss D: 0.031085334718227386\n",
      "Epoch 84/200, Batch 0/17, Loss G: 4.206841945648193, Loss D: 0.04330400750041008\n",
      "Epoch 84/200, Batch 1/17, Loss G: 4.33485746383667, Loss D: 0.018852993845939636\n",
      "Epoch 84/200, Batch 2/17, Loss G: 4.439087390899658, Loss D: 0.06086603179574013\n",
      "Epoch 84/200, Batch 3/17, Loss G: 4.171767234802246, Loss D: 0.08297348022460938\n",
      "Epoch 84/200, Batch 4/17, Loss G: 4.427064895629883, Loss D: 0.0428026057779789\n",
      "Epoch 84/200, Batch 5/17, Loss G: 4.1999101638793945, Loss D: 0.03244408220052719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200, Batch 6/17, Loss G: 4.042165756225586, Loss D: 0.029512440785765648\n",
      "Epoch 84/200, Batch 7/17, Loss G: 4.3091349601745605, Loss D: 0.024149395525455475\n",
      "Epoch 84/200, Batch 8/17, Loss G: 4.118762016296387, Loss D: 0.039303187280893326\n",
      "Epoch 84/200, Batch 9/17, Loss G: 4.35178279876709, Loss D: 0.03846284747123718\n",
      "Epoch 84/200, Batch 10/17, Loss G: 4.129342555999756, Loss D: 0.03364870697259903\n",
      "Epoch 84/200, Batch 11/17, Loss G: 4.393329620361328, Loss D: 0.028365623205900192\n",
      "Epoch 84/200, Batch 12/17, Loss G: 4.570782661437988, Loss D: 0.06699700653553009\n",
      "Epoch 84/200, Batch 13/17, Loss G: 4.023499488830566, Loss D: 0.10960539430379868\n",
      "Epoch 84/200, Batch 14/17, Loss G: 4.543367862701416, Loss D: 0.049397584050893784\n",
      "Epoch 84/200, Batch 15/17, Loss G: 4.399037837982178, Loss D: 0.024874424561858177\n",
      "Epoch 84/200, Batch 16/17, Loss G: 4.3055100440979, Loss D: 0.03760372847318649\n",
      "Epoch 85/200, Batch 0/17, Loss G: 4.246799468994141, Loss D: 0.02541881427168846\n",
      "Epoch 85/200, Batch 1/17, Loss G: 4.542863845825195, Loss D: 0.023194927722215652\n",
      "Epoch 85/200, Batch 2/17, Loss G: 4.457847595214844, Loss D: 0.027387987822294235\n",
      "Epoch 85/200, Batch 3/17, Loss G: 4.212603569030762, Loss D: 0.027178602293133736\n",
      "Epoch 85/200, Batch 4/17, Loss G: 3.9881207942962646, Loss D: 0.026254305616021156\n",
      "Epoch 85/200, Batch 5/17, Loss G: 4.321916580200195, Loss D: 0.020738987252116203\n",
      "Epoch 85/200, Batch 6/17, Loss G: 4.5542378425598145, Loss D: 0.03864775970578194\n",
      "Epoch 85/200, Batch 7/17, Loss G: 4.059221267700195, Loss D: 0.03657028079032898\n",
      "Epoch 85/200, Batch 8/17, Loss G: 4.274487018585205, Loss D: 0.026497971266508102\n",
      "Epoch 85/200, Batch 9/17, Loss G: 4.194097518920898, Loss D: 0.03521711006760597\n",
      "Epoch 85/200, Batch 10/17, Loss G: 4.588672637939453, Loss D: 0.04441983625292778\n",
      "Epoch 85/200, Batch 11/17, Loss G: 4.077286243438721, Loss D: 0.07753141224384308\n",
      "Epoch 85/200, Batch 12/17, Loss G: 4.712462425231934, Loss D: 0.14329862594604492\n",
      "Epoch 85/200, Batch 13/17, Loss G: 3.9029924869537354, Loss D: 0.06126823276281357\n",
      "Epoch 85/200, Batch 14/17, Loss G: 4.275655269622803, Loss D: 0.025709165260195732\n",
      "Epoch 85/200, Batch 15/17, Loss G: 4.175308704376221, Loss D: 0.027493178844451904\n",
      "Epoch 85/200, Batch 16/17, Loss G: 4.736075401306152, Loss D: 0.06948445737361908\n",
      "Epoch 86/200, Batch 0/17, Loss G: 4.236319541931152, Loss D: 0.03400447219610214\n",
      "Epoch 86/200, Batch 1/17, Loss G: 4.188970565795898, Loss D: 0.045774661004543304\n",
      "Epoch 86/200, Batch 2/17, Loss G: 4.124129295349121, Loss D: 0.07494132965803146\n",
      "Epoch 86/200, Batch 3/17, Loss G: 4.3630805015563965, Loss D: 0.03949865326285362\n",
      "Epoch 86/200, Batch 4/17, Loss G: 4.2361345291137695, Loss D: 0.03698309138417244\n",
      "Epoch 86/200, Batch 5/17, Loss G: 4.5200066566467285, Loss D: 0.16250306367874146\n",
      "Epoch 86/200, Batch 6/17, Loss G: 3.775160074234009, Loss D: 0.13106568157672882\n",
      "Epoch 86/200, Batch 7/17, Loss G: 4.5240960121154785, Loss D: 0.19494256377220154\n",
      "Epoch 86/200, Batch 8/17, Loss G: 3.5975990295410156, Loss D: 0.25858673453330994\n",
      "Epoch 86/200, Batch 9/17, Loss G: 4.410485744476318, Loss D: 0.16364377737045288\n",
      "Epoch 86/200, Batch 10/17, Loss G: 3.7644901275634766, Loss D: 0.12327641248703003\n",
      "Epoch 86/200, Batch 11/17, Loss G: 4.078027248382568, Loss D: 0.07963094115257263\n",
      "Epoch 86/200, Batch 12/17, Loss G: 4.321857452392578, Loss D: 0.09746499359607697\n",
      "Epoch 86/200, Batch 13/17, Loss G: 3.9794936180114746, Loss D: 0.1169673427939415\n",
      "Epoch 86/200, Batch 14/17, Loss G: 4.40500020980835, Loss D: 0.07012645900249481\n",
      "Epoch 86/200, Batch 15/17, Loss G: 4.369513511657715, Loss D: 0.0708797425031662\n",
      "Epoch 86/200, Batch 16/17, Loss G: 4.097834587097168, Loss D: 0.0754416361451149\n",
      "Epoch 87/200, Batch 0/17, Loss G: 4.232623100280762, Loss D: 0.07604444772005081\n",
      "Epoch 87/200, Batch 1/17, Loss G: 4.31508731842041, Loss D: 0.1583622395992279\n",
      "Epoch 87/200, Batch 2/17, Loss G: 3.731825113296509, Loss D: 0.17193394899368286\n",
      "Epoch 87/200, Batch 3/17, Loss G: 4.571369171142578, Loss D: 0.07820460200309753\n",
      "Epoch 87/200, Batch 4/17, Loss G: 4.248698711395264, Loss D: 0.05839517340064049\n",
      "Epoch 87/200, Batch 5/17, Loss G: 4.029392242431641, Loss D: 0.058578141033649445\n",
      "Epoch 87/200, Batch 6/17, Loss G: 4.352885723114014, Loss D: 0.0642412081360817\n",
      "Epoch 87/200, Batch 7/17, Loss G: 3.9457812309265137, Loss D: 0.05294344946742058\n",
      "Epoch 87/200, Batch 8/17, Loss G: 4.245576858520508, Loss D: 0.08601178228855133\n",
      "Epoch 87/200, Batch 9/17, Loss G: 4.2149553298950195, Loss D: 0.028292730450630188\n",
      "Epoch 87/200, Batch 10/17, Loss G: 4.284233093261719, Loss D: 0.02715478278696537\n",
      "Epoch 87/200, Batch 11/17, Loss G: 4.51973819732666, Loss D: 0.039370931684970856\n",
      "Epoch 87/200, Batch 12/17, Loss G: 4.023552894592285, Loss D: 0.05195115879178047\n",
      "Epoch 87/200, Batch 13/17, Loss G: 4.4113640785217285, Loss D: 0.04009026661515236\n",
      "Epoch 87/200, Batch 14/17, Loss G: 4.127695083618164, Loss D: 0.03923402354121208\n",
      "Epoch 87/200, Batch 15/17, Loss G: 4.388205528259277, Loss D: 0.028589438647031784\n",
      "Epoch 87/200, Batch 16/17, Loss G: 4.338469505310059, Loss D: 0.026283133774995804\n",
      "Epoch 88/200, Batch 0/17, Loss G: 4.220948219299316, Loss D: 0.031633760780096054\n",
      "Epoch 88/200, Batch 1/17, Loss G: 4.185308933258057, Loss D: 0.045934610068798065\n",
      "Epoch 88/200, Batch 2/17, Loss G: 4.270033359527588, Loss D: 0.08320777118206024\n",
      "Epoch 88/200, Batch 3/17, Loss G: 3.691378116607666, Loss D: 0.14801466464996338\n",
      "Epoch 88/200, Batch 4/17, Loss G: 4.585743427276611, Loss D: 0.06627801805734634\n",
      "Epoch 88/200, Batch 5/17, Loss G: 4.50917911529541, Loss D: 0.03376191109418869\n",
      "Epoch 88/200, Batch 6/17, Loss G: 3.8273582458496094, Loss D: 0.10151813179254532\n",
      "Epoch 88/200, Batch 7/17, Loss G: 4.425257682800293, Loss D: 0.14921636879444122\n",
      "Epoch 88/200, Batch 8/17, Loss G: 3.863049030303955, Loss D: 0.09439265727996826\n",
      "Epoch 88/200, Batch 9/17, Loss G: 4.451752662658691, Loss D: 0.04467196390032768\n",
      "Epoch 88/200, Batch 10/17, Loss G: 4.266783237457275, Loss D: 0.03201648220419884\n",
      "Epoch 88/200, Batch 11/17, Loss G: 3.845025062561035, Loss D: 0.09354434907436371\n",
      "Epoch 88/200, Batch 12/17, Loss G: 4.3590593338012695, Loss D: 0.030462896451354027\n",
      "Epoch 88/200, Batch 13/17, Loss G: 4.432612419128418, Loss D: 0.0571921244263649\n",
      "Epoch 88/200, Batch 14/17, Loss G: 3.9771125316619873, Loss D: 0.10257091373205185\n",
      "Epoch 88/200, Batch 15/17, Loss G: 4.465362548828125, Loss D: 0.07703261822462082\n",
      "Epoch 88/200, Batch 16/17, Loss G: 3.810304880142212, Loss D: 0.07822544127702713\n",
      "Epoch 89/200, Batch 0/17, Loss G: 4.35471248626709, Loss D: 0.06400775164365768\n",
      "Epoch 89/200, Batch 1/17, Loss G: 4.051844120025635, Loss D: 0.06821140646934509\n",
      "Epoch 89/200, Batch 2/17, Loss G: 4.116942405700684, Loss D: 0.04738238453865051\n",
      "Epoch 89/200, Batch 3/17, Loss G: 4.0791826248168945, Loss D: 0.03576434403657913\n",
      "Epoch 89/200, Batch 4/17, Loss G: 4.281327247619629, Loss D: 0.059105850756168365\n",
      "Epoch 89/200, Batch 5/17, Loss G: 4.0046706199646, Loss D: 0.05377199500799179\n",
      "Epoch 89/200, Batch 6/17, Loss G: 4.324481010437012, Loss D: 0.051952168345451355\n",
      "Epoch 89/200, Batch 7/17, Loss G: 4.29532527923584, Loss D: 0.03679773211479187\n",
      "Epoch 89/200, Batch 8/17, Loss G: 4.560608386993408, Loss D: 0.016457153484225273\n",
      "Epoch 89/200, Batch 9/17, Loss G: 4.325554847717285, Loss D: 0.023410871624946594\n",
      "Epoch 89/200, Batch 10/17, Loss G: 4.239497184753418, Loss D: 0.024340834468603134\n",
      "Epoch 89/200, Batch 11/17, Loss G: 4.367961883544922, Loss D: 0.034255500882864\n",
      "Epoch 89/200, Batch 12/17, Loss G: 4.341979503631592, Loss D: 0.06168683245778084\n",
      "Epoch 89/200, Batch 13/17, Loss G: 4.104726791381836, Loss D: 0.03732186555862427\n",
      "Epoch 89/200, Batch 14/17, Loss G: 4.116374492645264, Loss D: 0.051410526037216187\n",
      "Epoch 89/200, Batch 15/17, Loss G: 4.572114944458008, Loss D: 0.08452514559030533\n",
      "Epoch 89/200, Batch 16/17, Loss G: 4.31456184387207, Loss D: 0.03283258527517319\n",
      "Epoch 90/200, Batch 0/17, Loss G: 4.182114601135254, Loss D: 0.04032432287931442\n",
      "Epoch 90/200, Batch 1/17, Loss G: 4.726053237915039, Loss D: 0.06109454482793808\n",
      "Epoch 90/200, Batch 2/17, Loss G: 4.311241626739502, Loss D: 0.06014976277947426\n",
      "Epoch 90/200, Batch 3/17, Loss G: 4.275426864624023, Loss D: 0.039646148681640625\n",
      "Epoch 90/200, Batch 4/17, Loss G: 4.145298004150391, Loss D: 0.06634719669818878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Batch 5/17, Loss G: 3.9598913192749023, Loss D: 0.10401248186826706\n",
      "Epoch 90/200, Batch 6/17, Loss G: 4.433563232421875, Loss D: 0.08207691460847855\n",
      "Epoch 90/200, Batch 7/17, Loss G: 4.387851715087891, Loss D: 0.018706271424889565\n",
      "Epoch 90/200, Batch 8/17, Loss G: 4.103478908538818, Loss D: 0.029022572562098503\n",
      "Epoch 90/200, Batch 9/17, Loss G: 4.222254753112793, Loss D: 0.01482415571808815\n",
      "Epoch 90/200, Batch 10/17, Loss G: 4.270974159240723, Loss D: 0.025572314858436584\n",
      "Epoch 90/200, Batch 11/17, Loss G: 4.11189079284668, Loss D: 0.01769321598112583\n",
      "Epoch 90/200, Batch 12/17, Loss G: 4.303735256195068, Loss D: 0.04176098853349686\n",
      "Epoch 90/200, Batch 13/17, Loss G: 3.9023375511169434, Loss D: 0.06915248185396194\n",
      "Epoch 90/200, Batch 14/17, Loss G: 4.310643196105957, Loss D: 0.10676292330026627\n",
      "Epoch 90/200, Batch 15/17, Loss G: 4.088743209838867, Loss D: 0.03712981939315796\n",
      "Epoch 90/200, Batch 16/17, Loss G: 4.224895477294922, Loss D: 0.06690961122512817\n",
      "Epoch 91/200, Batch 0/17, Loss G: 4.689452648162842, Loss D: 0.04905231297016144\n",
      "Epoch 91/200, Batch 1/17, Loss G: 4.261973857879639, Loss D: 0.02667444571852684\n",
      "Epoch 91/200, Batch 2/17, Loss G: 4.320921421051025, Loss D: 0.021365437656641006\n",
      "Epoch 91/200, Batch 3/17, Loss G: 4.127747535705566, Loss D: 0.02847164496779442\n",
      "Epoch 91/200, Batch 4/17, Loss G: 4.307610511779785, Loss D: 0.01134676206856966\n",
      "Epoch 91/200, Batch 5/17, Loss G: 4.17805814743042, Loss D: 0.029524337500333786\n",
      "Epoch 91/200, Batch 6/17, Loss G: 4.365267276763916, Loss D: 0.048265546560287476\n",
      "Epoch 91/200, Batch 7/17, Loss G: 4.278257369995117, Loss D: 0.007061408832669258\n",
      "Epoch 91/200, Batch 8/17, Loss G: 3.8780622482299805, Loss D: 0.09779270738363266\n",
      "Epoch 91/200, Batch 9/17, Loss G: 4.400486946105957, Loss D: 0.07020409405231476\n",
      "Epoch 91/200, Batch 10/17, Loss G: 4.208327770233154, Loss D: 0.018881872296333313\n",
      "Epoch 91/200, Batch 11/17, Loss G: 4.401986122131348, Loss D: 0.010849221609532833\n",
      "Epoch 91/200, Batch 12/17, Loss G: 3.97344970703125, Loss D: 0.08451029658317566\n",
      "Epoch 91/200, Batch 13/17, Loss G: 4.628553867340088, Loss D: 0.1249890998005867\n",
      "Epoch 91/200, Batch 14/17, Loss G: 4.0709428787231445, Loss D: 0.03501509875059128\n",
      "Epoch 91/200, Batch 15/17, Loss G: 4.298655033111572, Loss D: 0.027649955824017525\n",
      "Epoch 91/200, Batch 16/17, Loss G: 4.51837158203125, Loss D: 0.06654712557792664\n",
      "Epoch 92/200, Batch 0/17, Loss G: 3.8154776096343994, Loss D: 0.1310921162366867\n",
      "Epoch 92/200, Batch 1/17, Loss G: 4.545364856719971, Loss D: 0.055926427245140076\n",
      "Epoch 92/200, Batch 2/17, Loss G: 4.17971134185791, Loss D: 0.03181435167789459\n",
      "Epoch 92/200, Batch 3/17, Loss G: 3.8745083808898926, Loss D: 0.0657433271408081\n",
      "Epoch 92/200, Batch 4/17, Loss G: 4.413202285766602, Loss D: 0.13527028262615204\n",
      "Epoch 92/200, Batch 5/17, Loss G: 3.8471951484680176, Loss D: 0.1259162276983261\n",
      "Epoch 92/200, Batch 6/17, Loss G: 4.3643927574157715, Loss D: 0.06251394748687744\n",
      "Epoch 92/200, Batch 7/17, Loss G: 3.9423725605010986, Loss D: 0.034574106335639954\n",
      "Epoch 92/200, Batch 8/17, Loss G: 4.184160232543945, Loss D: 0.05423019826412201\n",
      "Epoch 92/200, Batch 9/17, Loss G: 4.382075309753418, Loss D: 0.08675380051136017\n",
      "Epoch 92/200, Batch 10/17, Loss G: 3.7732863426208496, Loss D: 0.12189535796642303\n",
      "Epoch 92/200, Batch 11/17, Loss G: 4.118754863739014, Loss D: 0.06453991681337357\n",
      "Epoch 92/200, Batch 12/17, Loss G: 4.035301208496094, Loss D: 0.04630869999527931\n",
      "Epoch 92/200, Batch 13/17, Loss G: 4.174135208129883, Loss D: 0.03265318274497986\n",
      "Epoch 92/200, Batch 14/17, Loss G: 4.242043972015381, Loss D: 0.02316112071275711\n",
      "Epoch 92/200, Batch 15/17, Loss G: 4.2571821212768555, Loss D: 0.027448300272226334\n",
      "Epoch 92/200, Batch 16/17, Loss G: 4.194220542907715, Loss D: 0.01633312553167343\n",
      "Epoch 93/200, Batch 0/17, Loss G: 4.294958591461182, Loss D: 0.047876209020614624\n",
      "Epoch 93/200, Batch 1/17, Loss G: 4.516260147094727, Loss D: 0.03715058043599129\n",
      "Epoch 93/200, Batch 2/17, Loss G: 4.277942657470703, Loss D: 0.022692862898111343\n",
      "Epoch 93/200, Batch 3/17, Loss G: 4.224546432495117, Loss D: 0.01714959740638733\n",
      "Epoch 93/200, Batch 4/17, Loss G: 4.192361831665039, Loss D: 0.025443004444241524\n",
      "Epoch 93/200, Batch 5/17, Loss G: 4.22993278503418, Loss D: 0.028375186026096344\n",
      "Epoch 93/200, Batch 6/17, Loss G: 4.1255927085876465, Loss D: 0.027079159393906593\n",
      "Epoch 93/200, Batch 7/17, Loss G: 4.303261756896973, Loss D: 0.029724737629294395\n",
      "Epoch 93/200, Batch 8/17, Loss G: 4.041426658630371, Loss D: 0.021100888028740883\n",
      "Epoch 93/200, Batch 9/17, Loss G: 4.151323318481445, Loss D: 0.03039867803454399\n",
      "Epoch 93/200, Batch 10/17, Loss G: 4.43080997467041, Loss D: 0.036970485001802444\n",
      "Epoch 93/200, Batch 11/17, Loss G: 4.204409599304199, Loss D: 0.023810308426618576\n",
      "Epoch 93/200, Batch 12/17, Loss G: 4.147831439971924, Loss D: 0.03361035883426666\n",
      "Epoch 93/200, Batch 13/17, Loss G: 3.9764952659606934, Loss D: 0.028695611283183098\n",
      "Epoch 93/200, Batch 14/17, Loss G: 4.3044281005859375, Loss D: 0.03270931914448738\n",
      "Epoch 93/200, Batch 15/17, Loss G: 3.998337507247925, Loss D: 0.0379413366317749\n",
      "Epoch 93/200, Batch 16/17, Loss G: 4.53176736831665, Loss D: 0.021824056282639503\n",
      "Epoch 94/200, Batch 0/17, Loss G: 4.329548358917236, Loss D: 0.028178665786981583\n",
      "Epoch 94/200, Batch 1/17, Loss G: 4.38924503326416, Loss D: 0.021434839814901352\n",
      "Epoch 94/200, Batch 2/17, Loss G: 3.971909999847412, Loss D: 0.052398812025785446\n",
      "Epoch 94/200, Batch 3/17, Loss G: 4.461572170257568, Loss D: 0.07352256774902344\n",
      "Epoch 94/200, Batch 4/17, Loss G: 3.8926196098327637, Loss D: 0.09342940151691437\n",
      "Epoch 94/200, Batch 5/17, Loss G: 4.505260467529297, Loss D: 0.06926842778921127\n",
      "Epoch 94/200, Batch 6/17, Loss G: 4.072423458099365, Loss D: 0.02673250250518322\n",
      "Epoch 94/200, Batch 7/17, Loss G: 3.990800619125366, Loss D: 0.055741243064403534\n",
      "Epoch 94/200, Batch 8/17, Loss G: 4.334233283996582, Loss D: 0.03482309356331825\n",
      "Epoch 94/200, Batch 9/17, Loss G: 4.09207820892334, Loss D: 0.024836471304297447\n",
      "Epoch 94/200, Batch 10/17, Loss G: 4.151697158813477, Loss D: 0.025592179968953133\n",
      "Epoch 94/200, Batch 11/17, Loss G: 4.156449794769287, Loss D: 0.026707889512181282\n",
      "Epoch 94/200, Batch 12/17, Loss G: 4.40592098236084, Loss D: 0.04046617075800896\n",
      "Epoch 94/200, Batch 13/17, Loss G: 4.113451957702637, Loss D: 0.04380457103252411\n",
      "Epoch 94/200, Batch 14/17, Loss G: 4.071777820587158, Loss D: 0.01482315268367529\n",
      "Epoch 94/200, Batch 15/17, Loss G: 4.104879856109619, Loss D: 0.029401201754808426\n",
      "Epoch 94/200, Batch 16/17, Loss G: 4.361440658569336, Loss D: 0.05846962705254555\n",
      "Epoch 95/200, Batch 0/17, Loss G: 3.814819812774658, Loss D: 0.10291020572185516\n",
      "Epoch 95/200, Batch 1/17, Loss G: 4.292947769165039, Loss D: 0.07136386632919312\n",
      "Epoch 95/200, Batch 2/17, Loss G: 4.131015777587891, Loss D: 0.0514206662774086\n",
      "Epoch 95/200, Batch 3/17, Loss G: 4.439034461975098, Loss D: 0.029751630499958992\n",
      "Epoch 95/200, Batch 4/17, Loss G: 4.204424858093262, Loss D: 0.03999853879213333\n",
      "Epoch 95/200, Batch 5/17, Loss G: 3.6868505477905273, Loss D: 0.11321281641721725\n",
      "Epoch 95/200, Batch 6/17, Loss G: 4.52985954284668, Loss D: 0.13487237691879272\n",
      "Epoch 95/200, Batch 7/17, Loss G: 3.805839776992798, Loss D: 0.07623632252216339\n",
      "Epoch 95/200, Batch 8/17, Loss G: 4.412128925323486, Loss D: 0.04603123664855957\n",
      "Epoch 95/200, Batch 9/17, Loss G: 4.132266044616699, Loss D: 0.07669573277235031\n",
      "Epoch 95/200, Batch 10/17, Loss G: 4.391327857971191, Loss D: 0.11610110104084015\n",
      "Epoch 95/200, Batch 11/17, Loss G: 3.874619960784912, Loss D: 0.09243955463171005\n",
      "Epoch 95/200, Batch 12/17, Loss G: 4.360451698303223, Loss D: 0.03326244652271271\n",
      "Epoch 95/200, Batch 13/17, Loss G: 4.302008628845215, Loss D: 0.04221005737781525\n",
      "Epoch 95/200, Batch 14/17, Loss G: 3.6342787742614746, Loss D: 0.12217142432928085\n",
      "Epoch 95/200, Batch 15/17, Loss G: 4.527956485748291, Loss D: 0.13587410748004913\n",
      "Epoch 95/200, Batch 16/17, Loss G: 4.0505828857421875, Loss D: 0.020642582327127457\n",
      "Epoch 96/200, Batch 0/17, Loss G: 4.092584609985352, Loss D: 0.0692208930850029\n",
      "Epoch 96/200, Batch 1/17, Loss G: 4.530979156494141, Loss D: 0.12658372521400452\n",
      "Epoch 96/200, Batch 2/17, Loss G: 3.834348678588867, Loss D: 0.11151003837585449\n",
      "Epoch 96/200, Batch 3/17, Loss G: 4.431098461151123, Loss D: 0.04144756495952606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200, Batch 4/17, Loss G: 4.283860206604004, Loss D: 0.03496495634317398\n",
      "Epoch 96/200, Batch 5/17, Loss G: 3.7062747478485107, Loss D: 0.11911308765411377\n",
      "Epoch 96/200, Batch 6/17, Loss G: 4.244586944580078, Loss D: 0.0756869986653328\n",
      "Epoch 96/200, Batch 7/17, Loss G: 4.060276985168457, Loss D: 0.026909785345196724\n",
      "Epoch 96/200, Batch 8/17, Loss G: 4.2427215576171875, Loss D: 0.02248140051960945\n",
      "Epoch 96/200, Batch 9/17, Loss G: 4.070123195648193, Loss D: 0.028305182233452797\n",
      "Epoch 96/200, Batch 10/17, Loss G: 4.400833606719971, Loss D: 0.06370316445827484\n",
      "Epoch 96/200, Batch 11/17, Loss G: 3.8204245567321777, Loss D: 0.10183435678482056\n",
      "Epoch 96/200, Batch 12/17, Loss G: 4.329982757568359, Loss D: 0.09225855022668839\n",
      "Epoch 96/200, Batch 13/17, Loss G: 4.256911277770996, Loss D: 0.006861565634608269\n",
      "Epoch 96/200, Batch 14/17, Loss G: 4.207152366638184, Loss D: 0.028124595060944557\n",
      "Epoch 96/200, Batch 15/17, Loss G: 3.9794087409973145, Loss D: 0.04706670343875885\n",
      "Epoch 96/200, Batch 16/17, Loss G: 4.33138370513916, Loss D: 0.11288297921419144\n",
      "Epoch 97/200, Batch 0/17, Loss G: 3.9054765701293945, Loss D: 0.11318200081586838\n",
      "Epoch 97/200, Batch 1/17, Loss G: 4.59731388092041, Loss D: 0.05109502375125885\n",
      "Epoch 97/200, Batch 2/17, Loss G: 4.079527854919434, Loss D: 0.04381963610649109\n",
      "Epoch 97/200, Batch 3/17, Loss G: 4.07737922668457, Loss D: 0.05785393342375755\n",
      "Epoch 97/200, Batch 4/17, Loss G: 4.3946943283081055, Loss D: 0.07497262209653854\n",
      "Epoch 97/200, Batch 5/17, Loss G: 3.997087001800537, Loss D: 0.05503591150045395\n",
      "Epoch 97/200, Batch 6/17, Loss G: 4.167346954345703, Loss D: 0.02955441176891327\n",
      "Epoch 97/200, Batch 7/17, Loss G: 4.15762996673584, Loss D: 0.03697151690721512\n",
      "Epoch 97/200, Batch 8/17, Loss G: 4.114418029785156, Loss D: 0.03482120484113693\n",
      "Epoch 97/200, Batch 9/17, Loss G: 3.9865641593933105, Loss D: 0.03320038691163063\n",
      "Epoch 97/200, Batch 10/17, Loss G: 4.378438949584961, Loss D: 0.05782903730869293\n",
      "Epoch 97/200, Batch 11/17, Loss G: 4.174795150756836, Loss D: 0.04354679957032204\n",
      "Epoch 97/200, Batch 12/17, Loss G: 4.452908515930176, Loss D: 0.030483342707157135\n",
      "Epoch 97/200, Batch 13/17, Loss G: 4.002435684204102, Loss D: 0.02532307803630829\n",
      "Epoch 97/200, Batch 14/17, Loss G: 4.33343505859375, Loss D: 0.01135240402072668\n",
      "Epoch 97/200, Batch 15/17, Loss G: 4.342520713806152, Loss D: 0.007864650338888168\n",
      "Epoch 97/200, Batch 16/17, Loss G: 3.8401458263397217, Loss D: 0.01916372962296009\n",
      "Epoch 98/200, Batch 0/17, Loss G: 4.330570697784424, Loss D: 0.004479990806430578\n",
      "Epoch 98/200, Batch 1/17, Loss G: 4.474405765533447, Loss D: 0.013595263473689556\n",
      "Epoch 98/200, Batch 2/17, Loss G: 4.034859657287598, Loss D: 0.013140639290213585\n",
      "Epoch 98/200, Batch 3/17, Loss G: 4.354635238647461, Loss D: 0.006942925974726677\n",
      "Epoch 98/200, Batch 4/17, Loss G: 4.269862174987793, Loss D: 0.01135377585887909\n",
      "Epoch 98/200, Batch 5/17, Loss G: 4.143977642059326, Loss D: 0.013748356141149998\n",
      "Epoch 98/200, Batch 6/17, Loss G: 4.171130180358887, Loss D: 0.02224012091755867\n",
      "Epoch 98/200, Batch 7/17, Loss G: 4.300188064575195, Loss D: 0.01568886637687683\n",
      "Epoch 98/200, Batch 8/17, Loss G: 4.112227916717529, Loss D: 0.019423313438892365\n",
      "Epoch 98/200, Batch 9/17, Loss G: 4.149447441101074, Loss D: 0.020956356078386307\n",
      "Epoch 98/200, Batch 10/17, Loss G: 4.227411270141602, Loss D: 0.02300897054374218\n",
      "Epoch 98/200, Batch 11/17, Loss G: 4.2285003662109375, Loss D: 0.03936266899108887\n",
      "Epoch 98/200, Batch 12/17, Loss G: 3.832573890686035, Loss D: 0.09637224674224854\n",
      "Epoch 98/200, Batch 13/17, Loss G: 4.451106071472168, Loss D: 0.15630611777305603\n",
      "Epoch 98/200, Batch 14/17, Loss G: 3.824613571166992, Loss D: 0.05074787884950638\n",
      "Epoch 98/200, Batch 15/17, Loss G: 4.193469047546387, Loss D: 0.01839599944651127\n",
      "Epoch 98/200, Batch 16/17, Loss G: 4.124355316162109, Loss D: 0.02395925298333168\n",
      "Epoch 99/200, Batch 0/17, Loss G: 4.242903232574463, Loss D: 0.02057170495390892\n",
      "Epoch 99/200, Batch 1/17, Loss G: 4.193573474884033, Loss D: 0.035107698291540146\n",
      "Epoch 99/200, Batch 2/17, Loss G: 4.31424617767334, Loss D: 0.012847225181758404\n",
      "Epoch 99/200, Batch 3/17, Loss G: 4.076625347137451, Loss D: 0.02350766398012638\n",
      "Epoch 99/200, Batch 4/17, Loss G: 4.184237480163574, Loss D: 0.023812975734472275\n",
      "Epoch 99/200, Batch 5/17, Loss G: 4.235300540924072, Loss D: 0.062155887484550476\n",
      "Epoch 99/200, Batch 6/17, Loss G: 3.6960110664367676, Loss D: 0.12231346219778061\n",
      "Epoch 99/200, Batch 7/17, Loss G: 4.258846759796143, Loss D: 0.08067013323307037\n",
      "Epoch 99/200, Batch 8/17, Loss G: 3.973053455352783, Loss D: 0.04903549700975418\n",
      "Epoch 99/200, Batch 9/17, Loss G: 4.291249752044678, Loss D: 0.07114316523075104\n",
      "Epoch 99/200, Batch 10/17, Loss G: 3.7814626693725586, Loss D: 0.10909105837345123\n",
      "Epoch 99/200, Batch 11/17, Loss G: 4.362539291381836, Loss D: 0.10146954655647278\n",
      "Epoch 99/200, Batch 12/17, Loss G: 3.8445656299591064, Loss D: 0.05575566738843918\n",
      "Epoch 99/200, Batch 13/17, Loss G: 4.146080017089844, Loss D: 0.024580348283052444\n",
      "Epoch 99/200, Batch 14/17, Loss G: 4.359034538269043, Loss D: 0.07191243767738342\n",
      "Epoch 99/200, Batch 15/17, Loss G: 3.820563793182373, Loss D: 0.08642083406448364\n",
      "Epoch 99/200, Batch 16/17, Loss G: 4.399197578430176, Loss D: 0.047505926340818405\n",
      "Epoch 100/200, Batch 0/17, Loss G: 4.060954570770264, Loss D: 0.06335361301898956\n",
      "Epoch 100/200, Batch 1/17, Loss G: 4.188359260559082, Loss D: 0.0691152885556221\n",
      "Epoch 100/200, Batch 2/17, Loss G: 4.155240058898926, Loss D: 0.036634936928749084\n",
      "Epoch 100/200, Batch 3/17, Loss G: 3.982247829437256, Loss D: 0.04713406413793564\n",
      "Epoch 100/200, Batch 4/17, Loss G: 4.548280715942383, Loss D: 0.04420826956629753\n",
      "Epoch 100/200, Batch 5/17, Loss G: 4.100958824157715, Loss D: 0.03690820559859276\n",
      "Epoch 100/200, Batch 6/17, Loss G: 3.7509920597076416, Loss D: 0.05449666082859039\n",
      "Epoch 100/200, Batch 7/17, Loss G: 4.375858306884766, Loss D: 0.040354207158088684\n",
      "Epoch 100/200, Batch 8/17, Loss G: 3.966949939727783, Loss D: 0.034125976264476776\n",
      "Epoch 100/200, Batch 9/17, Loss G: 4.191712379455566, Loss D: 0.05200306326150894\n",
      "Epoch 100/200, Batch 10/17, Loss G: 4.303481101989746, Loss D: 0.028140872716903687\n",
      "Epoch 100/200, Batch 11/17, Loss G: 4.466244697570801, Loss D: 0.017766479402780533\n",
      "Epoch 100/200, Batch 12/17, Loss G: 4.537040710449219, Loss D: 0.02844059281051159\n",
      "Epoch 100/200, Batch 13/17, Loss G: 4.046249866485596, Loss D: 0.025577928870916367\n",
      "Epoch 100/200, Batch 14/17, Loss G: 4.094781875610352, Loss D: 0.03981512784957886\n",
      "Epoch 100/200, Batch 15/17, Loss G: 3.7464864253997803, Loss D: 0.06112228333950043\n",
      "Epoch 100/200, Batch 16/17, Loss G: 4.324348449707031, Loss D: 0.1590714305639267\n",
      "Epoch 101/200, Batch 0/17, Loss G: 3.5738630294799805, Loss D: 0.23778551816940308\n",
      "Epoch 101/200, Batch 1/17, Loss G: 4.418108940124512, Loss D: 0.0502554215490818\n",
      "Epoch 101/200, Batch 2/17, Loss G: 4.152128219604492, Loss D: 0.033268772065639496\n",
      "Epoch 101/200, Batch 3/17, Loss G: 4.006659507751465, Loss D: 0.06040431931614876\n",
      "Epoch 101/200, Batch 4/17, Loss G: 4.278519153594971, Loss D: 0.05825754255056381\n",
      "Epoch 101/200, Batch 5/17, Loss G: 4.1165571212768555, Loss D: 0.03446538373827934\n",
      "Epoch 101/200, Batch 6/17, Loss G: 4.169155120849609, Loss D: 0.047777190804481506\n",
      "Epoch 101/200, Batch 7/17, Loss G: 4.1223602294921875, Loss D: 0.040425583720207214\n",
      "Epoch 101/200, Batch 8/17, Loss G: 4.080617427825928, Loss D: 0.04330956190824509\n",
      "Epoch 101/200, Batch 9/17, Loss G: 4.0106658935546875, Loss D: 0.03519689664244652\n",
      "Epoch 101/200, Batch 10/17, Loss G: 4.199989318847656, Loss D: 0.021539652720093727\n",
      "Epoch 101/200, Batch 11/17, Loss G: 3.9818079471588135, Loss D: 0.06217711791396141\n",
      "Epoch 101/200, Batch 12/17, Loss G: 3.7353262901306152, Loss D: 0.0779709592461586\n",
      "Epoch 101/200, Batch 13/17, Loss G: 4.327838897705078, Loss D: 0.056321777403354645\n",
      "Epoch 101/200, Batch 14/17, Loss G: 4.021856784820557, Loss D: 0.04656660184264183\n",
      "Epoch 101/200, Batch 15/17, Loss G: 4.396528720855713, Loss D: 0.02330269291996956\n",
      "Epoch 101/200, Batch 16/17, Loss G: 4.295670032501221, Loss D: 0.010522536002099514\n",
      "Epoch 102/200, Batch 0/17, Loss G: 4.066215991973877, Loss D: 0.02152024209499359\n",
      "Epoch 102/200, Batch 1/17, Loss G: 4.357276916503906, Loss D: 0.016377709805965424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200, Batch 2/17, Loss G: 4.112037181854248, Loss D: 0.028584104031324387\n",
      "Epoch 102/200, Batch 3/17, Loss G: 4.119906425476074, Loss D: 0.022609593346714973\n",
      "Epoch 102/200, Batch 4/17, Loss G: 4.1419477462768555, Loss D: 0.014978116378188133\n",
      "Epoch 102/200, Batch 5/17, Loss G: 4.01636266708374, Loss D: 0.02810237556695938\n",
      "Epoch 102/200, Batch 6/17, Loss G: 4.2448296546936035, Loss D: 0.07261266559362411\n",
      "Epoch 102/200, Batch 7/17, Loss G: 3.7868003845214844, Loss D: 0.1508733332157135\n",
      "Epoch 102/200, Batch 8/17, Loss G: 4.344077110290527, Loss D: 0.1738176941871643\n",
      "Epoch 102/200, Batch 9/17, Loss G: 4.145959854125977, Loss D: 0.06341997534036636\n",
      "Epoch 102/200, Batch 10/17, Loss G: 4.2611083984375, Loss D: 0.03559262305498123\n",
      "Epoch 102/200, Batch 11/17, Loss G: 4.067653179168701, Loss D: 0.08217266201972961\n",
      "Epoch 102/200, Batch 12/17, Loss G: 3.628203868865967, Loss D: 0.1962600201368332\n",
      "Epoch 102/200, Batch 13/17, Loss G: 4.300239086151123, Loss D: 0.13157512247562408\n",
      "Epoch 102/200, Batch 14/17, Loss G: 3.8752856254577637, Loss D: 0.05283363536000252\n",
      "Epoch 102/200, Batch 15/17, Loss G: 4.034049987792969, Loss D: 0.04026753455400467\n",
      "Epoch 102/200, Batch 16/17, Loss G: 4.079135894775391, Loss D: 0.06006667762994766\n",
      "Epoch 103/200, Batch 0/17, Loss G: 3.9770565032958984, Loss D: 0.10595250129699707\n",
      "Epoch 103/200, Batch 1/17, Loss G: 4.248940467834473, Loss D: 0.03897662088274956\n",
      "Epoch 103/200, Batch 2/17, Loss G: 4.203032970428467, Loss D: 0.1405111849308014\n",
      "Epoch 103/200, Batch 3/17, Loss G: 3.513576030731201, Loss D: 0.26097291707992554\n",
      "Epoch 103/200, Batch 4/17, Loss G: 4.322258949279785, Loss D: 0.052681390196084976\n",
      "Epoch 103/200, Batch 5/17, Loss G: 4.0793232917785645, Loss D: 0.043755896389484406\n",
      "Epoch 103/200, Batch 6/17, Loss G: 3.9111900329589844, Loss D: 0.04723836109042168\n",
      "Epoch 103/200, Batch 7/17, Loss G: 4.14138126373291, Loss D: 0.044217754155397415\n",
      "Epoch 103/200, Batch 8/17, Loss G: 3.775770664215088, Loss D: 0.0334794744849205\n",
      "Epoch 103/200, Batch 9/17, Loss G: 3.985901355743408, Loss D: 0.041499003767967224\n",
      "Epoch 103/200, Batch 10/17, Loss G: 4.173253059387207, Loss D: 0.04763957858085632\n",
      "Epoch 103/200, Batch 11/17, Loss G: 3.9504027366638184, Loss D: 0.03148150444030762\n",
      "Epoch 103/200, Batch 12/17, Loss G: 4.022066116333008, Loss D: 0.04483676329255104\n",
      "Epoch 103/200, Batch 13/17, Loss G: 4.324075698852539, Loss D: 0.04703319072723389\n",
      "Epoch 103/200, Batch 14/17, Loss G: 3.8985280990600586, Loss D: 0.043761685490608215\n",
      "Epoch 103/200, Batch 15/17, Loss G: 4.211029052734375, Loss D: 0.037036024034023285\n",
      "Epoch 103/200, Batch 16/17, Loss G: 4.026081085205078, Loss D: 0.02276398241519928\n",
      "Epoch 104/200, Batch 0/17, Loss G: 4.12986946105957, Loss D: 0.023904351517558098\n",
      "Epoch 104/200, Batch 1/17, Loss G: 4.067278861999512, Loss D: 0.03590000420808792\n",
      "Epoch 104/200, Batch 2/17, Loss G: 4.24739933013916, Loss D: 0.0195469930768013\n",
      "Epoch 104/200, Batch 3/17, Loss G: 3.895794630050659, Loss D: 0.038271017372608185\n",
      "Epoch 104/200, Batch 4/17, Loss G: 4.3008856773376465, Loss D: 0.0340452678501606\n",
      "Epoch 104/200, Batch 5/17, Loss G: 4.113483428955078, Loss D: 0.028155149891972542\n",
      "Epoch 104/200, Batch 6/17, Loss G: 4.309042930603027, Loss D: 0.015956919640302658\n",
      "Epoch 104/200, Batch 7/17, Loss G: 4.0968122482299805, Loss D: 0.04499334841966629\n",
      "Epoch 104/200, Batch 8/17, Loss G: 3.743722438812256, Loss D: 0.06500155478715897\n",
      "Epoch 104/200, Batch 9/17, Loss G: 4.308615684509277, Loss D: 0.0368385910987854\n",
      "Epoch 104/200, Batch 10/17, Loss G: 4.313189506530762, Loss D: 0.02123969979584217\n",
      "Epoch 104/200, Batch 11/17, Loss G: 3.9867429733276367, Loss D: 0.019944481551647186\n",
      "Epoch 104/200, Batch 12/17, Loss G: 4.455225944519043, Loss D: 0.025037497282028198\n",
      "Epoch 104/200, Batch 13/17, Loss G: 4.161710262298584, Loss D: 0.02501869574189186\n",
      "Epoch 104/200, Batch 14/17, Loss G: 3.92004656791687, Loss D: 0.03312546759843826\n",
      "Epoch 104/200, Batch 15/17, Loss G: 4.0348334312438965, Loss D: 0.026520784944295883\n",
      "Epoch 104/200, Batch 16/17, Loss G: 4.183333396911621, Loss D: 0.01529219001531601\n",
      "Epoch 105/200, Batch 0/17, Loss G: 4.0257062911987305, Loss D: 0.018160399049520493\n",
      "Epoch 105/200, Batch 1/17, Loss G: 4.228809356689453, Loss D: 0.04518648236989975\n",
      "Epoch 105/200, Batch 2/17, Loss G: 3.7372970581054688, Loss D: 0.1027381494641304\n",
      "Epoch 105/200, Batch 3/17, Loss G: 4.232025146484375, Loss D: 0.08919385075569153\n",
      "Epoch 105/200, Batch 4/17, Loss G: 3.939699172973633, Loss D: 0.0465245321393013\n",
      "Epoch 105/200, Batch 5/17, Loss G: 4.005101680755615, Loss D: 0.026201879605650902\n",
      "Epoch 105/200, Batch 6/17, Loss G: 4.13332462310791, Loss D: 0.029646340757608414\n",
      "Epoch 105/200, Batch 7/17, Loss G: 4.125730037689209, Loss D: 0.022115733474493027\n",
      "Epoch 105/200, Batch 8/17, Loss G: 4.130266189575195, Loss D: 0.029011493548750877\n",
      "Epoch 105/200, Batch 9/17, Loss G: 4.031155586242676, Loss D: 0.029647523537278175\n",
      "Epoch 105/200, Batch 10/17, Loss G: 4.178064346313477, Loss D: 0.02782050520181656\n",
      "Epoch 105/200, Batch 11/17, Loss G: 4.060210227966309, Loss D: 0.03516165167093277\n",
      "Epoch 105/200, Batch 12/17, Loss G: 4.013035774230957, Loss D: 0.02438012883067131\n",
      "Epoch 105/200, Batch 13/17, Loss G: 4.284093856811523, Loss D: 0.0802905410528183\n",
      "Epoch 105/200, Batch 14/17, Loss G: 3.69012451171875, Loss D: 0.164344921708107\n",
      "Epoch 105/200, Batch 15/17, Loss G: 4.352885723114014, Loss D: 0.06918200850486755\n",
      "Epoch 105/200, Batch 16/17, Loss G: 4.114017486572266, Loss D: 0.06985719501972198\n",
      "Epoch 106/200, Batch 0/17, Loss G: 3.6177334785461426, Loss D: 0.12203782051801682\n",
      "Epoch 106/200, Batch 1/17, Loss G: 4.001351356506348, Loss D: 0.05296488106250763\n",
      "Epoch 106/200, Batch 2/17, Loss G: 3.9766592979431152, Loss D: 0.04604057967662811\n",
      "Epoch 106/200, Batch 3/17, Loss G: 3.9710068702697754, Loss D: 0.040107302367687225\n",
      "Epoch 106/200, Batch 4/17, Loss G: 4.12156867980957, Loss D: 0.04157055541872978\n",
      "Epoch 106/200, Batch 5/17, Loss G: 4.0118207931518555, Loss D: 0.052249614149332047\n",
      "Epoch 106/200, Batch 6/17, Loss G: 4.255005836486816, Loss D: 0.06078198924660683\n",
      "Epoch 106/200, Batch 7/17, Loss G: 3.975137948989868, Loss D: 0.048009492456912994\n",
      "Epoch 106/200, Batch 8/17, Loss G: 4.081383228302002, Loss D: 0.04501394182443619\n",
      "Epoch 106/200, Batch 9/17, Loss G: 4.134593963623047, Loss D: 0.034053899347782135\n",
      "Epoch 106/200, Batch 10/17, Loss G: 4.220272541046143, Loss D: 0.03452711179852486\n",
      "Epoch 106/200, Batch 11/17, Loss G: 4.1981611251831055, Loss D: 0.03207911550998688\n",
      "Epoch 106/200, Batch 12/17, Loss G: 4.06683874130249, Loss D: 0.02941085398197174\n",
      "Epoch 106/200, Batch 13/17, Loss G: 4.311450958251953, Loss D: 0.03340375795960426\n",
      "Epoch 106/200, Batch 14/17, Loss G: 4.158705234527588, Loss D: 0.016031993553042412\n",
      "Epoch 106/200, Batch 15/17, Loss G: 4.248331546783447, Loss D: 0.03254789113998413\n",
      "Epoch 106/200, Batch 16/17, Loss G: 4.216342926025391, Loss D: 0.018590105697512627\n",
      "Epoch 107/200, Batch 0/17, Loss G: 4.369148254394531, Loss D: 0.03321589156985283\n",
      "Epoch 107/200, Batch 1/17, Loss G: 4.116225242614746, Loss D: 0.021335631608963013\n",
      "Epoch 107/200, Batch 2/17, Loss G: 4.041224479675293, Loss D: 0.02900196798145771\n",
      "Epoch 107/200, Batch 3/17, Loss G: 3.8288865089416504, Loss D: 0.024400589987635612\n",
      "Epoch 107/200, Batch 4/17, Loss G: 4.458145618438721, Loss D: 0.01102476753294468\n",
      "Epoch 107/200, Batch 5/17, Loss G: 4.341166019439697, Loss D: 0.031676094979047775\n",
      "Epoch 107/200, Batch 6/17, Loss G: 4.018131732940674, Loss D: 0.06050143390893936\n",
      "Epoch 107/200, Batch 7/17, Loss G: 4.426178932189941, Loss D: 0.06566847860813141\n",
      "Epoch 107/200, Batch 8/17, Loss G: 4.04422664642334, Loss D: 0.047624893486499786\n",
      "Epoch 107/200, Batch 9/17, Loss G: 4.121822357177734, Loss D: 0.021057436242699623\n",
      "Epoch 107/200, Batch 10/17, Loss G: 4.025475978851318, Loss D: 0.03451044484972954\n",
      "Epoch 107/200, Batch 11/17, Loss G: 4.258317947387695, Loss D: 0.055865924805402756\n",
      "Epoch 107/200, Batch 12/17, Loss G: 3.7327752113342285, Loss D: 0.062139369547367096\n",
      "Epoch 107/200, Batch 13/17, Loss G: 4.310087203979492, Loss D: 0.02778562903404236\n",
      "Epoch 107/200, Batch 14/17, Loss G: 4.237142562866211, Loss D: 0.03564893826842308\n",
      "Epoch 107/200, Batch 15/17, Loss G: 4.001007080078125, Loss D: 0.04039722681045532\n",
      "Epoch 107/200, Batch 16/17, Loss G: 4.191742420196533, Loss D: 0.016316721215844154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200, Batch 0/17, Loss G: 4.249886512756348, Loss D: 0.0482855886220932\n",
      "Epoch 108/200, Batch 1/17, Loss G: 4.082515239715576, Loss D: 0.06046440079808235\n",
      "Epoch 108/200, Batch 2/17, Loss G: 4.256699562072754, Loss D: 0.05051204189658165\n",
      "Epoch 108/200, Batch 3/17, Loss G: 3.982179641723633, Loss D: 0.08313442021608353\n",
      "Epoch 108/200, Batch 4/17, Loss G: 4.324463844299316, Loss D: 0.07039862871170044\n",
      "Epoch 108/200, Batch 5/17, Loss G: 4.096120834350586, Loss D: 0.02253265678882599\n",
      "Epoch 108/200, Batch 6/17, Loss G: 4.031119346618652, Loss D: 0.022720303386449814\n",
      "Epoch 108/200, Batch 7/17, Loss G: 4.080117702484131, Loss D: 0.024445321410894394\n",
      "Epoch 108/200, Batch 8/17, Loss G: 4.185783386230469, Loss D: 0.03797269985079765\n",
      "Epoch 108/200, Batch 9/17, Loss G: 3.8609654903411865, Loss D: 0.05107366293668747\n",
      "Epoch 108/200, Batch 10/17, Loss G: 4.2453389167785645, Loss D: 0.08584361523389816\n",
      "Epoch 108/200, Batch 11/17, Loss G: 3.6837973594665527, Loss D: 0.0942310094833374\n",
      "Epoch 108/200, Batch 12/17, Loss G: 3.9139928817749023, Loss D: 0.031105393543839455\n",
      "Epoch 108/200, Batch 13/17, Loss G: 4.1349568367004395, Loss D: 0.11287003010511398\n",
      "Epoch 108/200, Batch 14/17, Loss G: 3.5077428817749023, Loss D: 0.17721156775951385\n",
      "Epoch 108/200, Batch 15/17, Loss G: 4.245494842529297, Loss D: 0.1387614756822586\n",
      "Epoch 108/200, Batch 16/17, Loss G: 3.9912290573120117, Loss D: 0.06990592926740646\n",
      "Epoch 109/200, Batch 0/17, Loss G: 4.164407730102539, Loss D: 0.04758990556001663\n",
      "Epoch 109/200, Batch 1/17, Loss G: 3.9988183975219727, Loss D: 0.05775551497936249\n",
      "Epoch 109/200, Batch 2/17, Loss G: 4.051898956298828, Loss D: 0.06588464975357056\n",
      "Epoch 109/200, Batch 3/17, Loss G: 4.148380756378174, Loss D: 0.029493646696209908\n",
      "Epoch 109/200, Batch 4/17, Loss G: 4.138919830322266, Loss D: 0.04373935982584953\n",
      "Epoch 109/200, Batch 5/17, Loss G: 4.133893013000488, Loss D: 0.036269500851631165\n",
      "Epoch 109/200, Batch 6/17, Loss G: 4.005236625671387, Loss D: 0.037989016622304916\n",
      "Epoch 109/200, Batch 7/17, Loss G: 4.23366641998291, Loss D: 0.04826188087463379\n",
      "Epoch 109/200, Batch 8/17, Loss G: 3.911071538925171, Loss D: 0.07624534517526627\n",
      "Epoch 109/200, Batch 9/17, Loss G: 4.3760600090026855, Loss D: 0.17325881123542786\n",
      "Epoch 109/200, Batch 10/17, Loss G: 3.4991941452026367, Loss D: 0.22367486357688904\n",
      "Epoch 109/200, Batch 11/17, Loss G: 4.040732383728027, Loss D: 0.0663672685623169\n",
      "Epoch 109/200, Batch 12/17, Loss G: 4.086713790893555, Loss D: 0.03553486242890358\n",
      "Epoch 109/200, Batch 13/17, Loss G: 3.95536732673645, Loss D: 0.05534658581018448\n",
      "Epoch 109/200, Batch 14/17, Loss G: 3.7706785202026367, Loss D: 0.09920188784599304\n",
      "Epoch 109/200, Batch 15/17, Loss G: 4.188506126403809, Loss D: 0.07806307822465897\n",
      "Epoch 109/200, Batch 16/17, Loss G: 3.8598551750183105, Loss D: 0.03331135958433151\n",
      "Epoch 110/200, Batch 0/17, Loss G: 3.8393373489379883, Loss D: 0.06465814262628555\n",
      "Epoch 110/200, Batch 1/17, Loss G: 4.350689888000488, Loss D: 0.057802263647317886\n",
      "Epoch 110/200, Batch 2/17, Loss G: 4.120037078857422, Loss D: 0.03519034385681152\n",
      "Epoch 110/200, Batch 3/17, Loss G: 3.8322372436523438, Loss D: 0.036059338599443436\n",
      "Epoch 110/200, Batch 4/17, Loss G: 4.09416389465332, Loss D: 0.02751101925969124\n",
      "Epoch 110/200, Batch 5/17, Loss G: 4.114719867706299, Loss D: 0.037224434316158295\n",
      "Epoch 110/200, Batch 6/17, Loss G: 4.034273624420166, Loss D: 0.04706364870071411\n",
      "Epoch 110/200, Batch 7/17, Loss G: 3.9196019172668457, Loss D: 0.030309319496154785\n",
      "Epoch 110/200, Batch 8/17, Loss G: 4.137353897094727, Loss D: 0.0242873877286911\n",
      "Epoch 110/200, Batch 9/17, Loss G: 3.9475862979888916, Loss D: 0.021163303405046463\n",
      "Epoch 110/200, Batch 10/17, Loss G: 3.8939146995544434, Loss D: 0.031204387545585632\n",
      "Epoch 110/200, Batch 11/17, Loss G: 3.9265928268432617, Loss D: 0.03946666046977043\n",
      "Epoch 110/200, Batch 12/17, Loss G: 4.142831802368164, Loss D: 0.04243846237659454\n",
      "Epoch 110/200, Batch 13/17, Loss G: 4.163861274719238, Loss D: 0.026638424023985863\n",
      "Epoch 110/200, Batch 14/17, Loss G: 4.10682487487793, Loss D: 0.04165691137313843\n",
      "Epoch 110/200, Batch 15/17, Loss G: 4.0568108558654785, Loss D: 0.027843229472637177\n",
      "Epoch 110/200, Batch 16/17, Loss G: 4.161098480224609, Loss D: 0.029588349163532257\n",
      "Epoch 111/200, Batch 0/17, Loss G: 4.057182312011719, Loss D: 0.036306142807006836\n",
      "Epoch 111/200, Batch 1/17, Loss G: 4.329979419708252, Loss D: 0.03877534344792366\n",
      "Epoch 111/200, Batch 2/17, Loss G: 4.031052589416504, Loss D: 0.03371381014585495\n",
      "Epoch 111/200, Batch 3/17, Loss G: 4.06575345993042, Loss D: 0.016133295372128487\n",
      "Epoch 111/200, Batch 4/17, Loss G: 4.0870208740234375, Loss D: 0.023733116686344147\n",
      "Epoch 111/200, Batch 5/17, Loss G: 4.364845275878906, Loss D: 0.01621011272072792\n",
      "Epoch 111/200, Batch 6/17, Loss G: 4.21806001663208, Loss D: 0.013379907235503197\n",
      "Epoch 111/200, Batch 7/17, Loss G: 4.033086776733398, Loss D: 0.037032339721918106\n",
      "Epoch 111/200, Batch 8/17, Loss G: 4.062172889709473, Loss D: 0.03755445405840874\n",
      "Epoch 111/200, Batch 9/17, Loss G: 3.87406849861145, Loss D: 0.031037090346217155\n",
      "Epoch 111/200, Batch 10/17, Loss G: 4.0275492668151855, Loss D: 0.007509450428187847\n",
      "Epoch 111/200, Batch 11/17, Loss G: 4.074833869934082, Loss D: 0.06369470804929733\n",
      "Epoch 111/200, Batch 12/17, Loss G: 3.6110620498657227, Loss D: 0.0964663103222847\n",
      "Epoch 111/200, Batch 13/17, Loss G: 4.195858955383301, Loss D: 0.03785412013530731\n",
      "Epoch 111/200, Batch 14/17, Loss G: 4.136491298675537, Loss D: 0.03510725498199463\n",
      "Epoch 111/200, Batch 15/17, Loss G: 3.842921495437622, Loss D: 0.03932453319430351\n",
      "Epoch 111/200, Batch 16/17, Loss G: 4.456457138061523, Loss D: 0.03721604496240616\n",
      "Epoch 112/200, Batch 0/17, Loss G: 3.978543281555176, Loss D: 0.028570853173732758\n",
      "Epoch 112/200, Batch 1/17, Loss G: 4.148494720458984, Loss D: 0.013673864305019379\n",
      "Epoch 112/200, Batch 2/17, Loss G: 4.068542957305908, Loss D: 0.036757633090019226\n",
      "Epoch 112/200, Batch 3/17, Loss G: 4.173487663269043, Loss D: 0.017644375562667847\n",
      "Epoch 112/200, Batch 4/17, Loss G: 4.033393859863281, Loss D: 0.01972200721502304\n",
      "Epoch 112/200, Batch 5/17, Loss G: 4.178693771362305, Loss D: 0.029092147946357727\n",
      "Epoch 112/200, Batch 6/17, Loss G: 4.007473468780518, Loss D: 0.03010694682598114\n",
      "Epoch 112/200, Batch 7/17, Loss G: 3.9659461975097656, Loss D: 0.04524819552898407\n",
      "Epoch 112/200, Batch 8/17, Loss G: 4.282432556152344, Loss D: 0.03394651412963867\n",
      "Epoch 112/200, Batch 9/17, Loss G: 4.153922080993652, Loss D: 0.024933576583862305\n",
      "Epoch 112/200, Batch 10/17, Loss G: 4.03853702545166, Loss D: 0.01914699748158455\n",
      "Epoch 112/200, Batch 11/17, Loss G: 4.32977294921875, Loss D: 0.010874859057366848\n",
      "Epoch 112/200, Batch 12/17, Loss G: 4.052894115447998, Loss D: 0.02153986319899559\n",
      "Epoch 112/200, Batch 13/17, Loss G: 4.015732765197754, Loss D: 0.022633014246821404\n",
      "Epoch 112/200, Batch 14/17, Loss G: 4.1373796463012695, Loss D: 0.019093334674835205\n",
      "Epoch 112/200, Batch 15/17, Loss G: 4.084731101989746, Loss D: 0.026289254426956177\n",
      "Epoch 112/200, Batch 16/17, Loss G: 3.8585996627807617, Loss D: 0.018356524407863617\n",
      "Epoch 113/200, Batch 0/17, Loss G: 3.784489631652832, Loss D: 0.05485111474990845\n",
      "Epoch 113/200, Batch 1/17, Loss G: 4.327561378479004, Loss D: 0.13226747512817383\n",
      "Epoch 113/200, Batch 2/17, Loss G: 3.897367238998413, Loss D: 0.06250562518835068\n",
      "Epoch 113/200, Batch 3/17, Loss G: 4.320813179016113, Loss D: 0.012044393457472324\n",
      "Epoch 113/200, Batch 4/17, Loss G: 4.295734405517578, Loss D: 0.013870306313037872\n",
      "Epoch 113/200, Batch 5/17, Loss G: 4.212994575500488, Loss D: 0.021927429363131523\n",
      "Epoch 113/200, Batch 6/17, Loss G: 4.108784198760986, Loss D: 0.005270735360682011\n",
      "Epoch 113/200, Batch 7/17, Loss G: 4.257578372955322, Loss D: 0.02166619338095188\n",
      "Epoch 113/200, Batch 8/17, Loss G: 3.782866954803467, Loss D: 0.029726071283221245\n",
      "Epoch 113/200, Batch 9/17, Loss G: 4.291327476501465, Loss D: 0.013191434554755688\n",
      "Epoch 113/200, Batch 10/17, Loss G: 4.005821228027344, Loss D: 0.02715258300304413\n",
      "Epoch 113/200, Batch 11/17, Loss G: 3.776721715927124, Loss D: 0.05277413874864578\n",
      "Epoch 113/200, Batch 12/17, Loss G: 4.116103649139404, Loss D: 0.0637831836938858\n",
      "Epoch 113/200, Batch 13/17, Loss G: 3.971576690673828, Loss D: 0.0245219673961401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200, Batch 14/17, Loss G: 4.065136432647705, Loss D: 0.032249193638563156\n",
      "Epoch 113/200, Batch 15/17, Loss G: 4.281941890716553, Loss D: 0.07609574496746063\n",
      "Epoch 113/200, Batch 16/17, Loss G: 3.5628786087036133, Loss D: 0.108824722468853\n",
      "Epoch 114/200, Batch 0/17, Loss G: 4.337141036987305, Loss D: 0.15932650864124298\n",
      "Epoch 114/200, Batch 1/17, Loss G: 3.782594680786133, Loss D: 0.10244110226631165\n",
      "Epoch 114/200, Batch 2/17, Loss G: 4.114084720611572, Loss D: 0.06153494492173195\n",
      "Epoch 114/200, Batch 3/17, Loss G: 3.9501709938049316, Loss D: 0.04071403294801712\n",
      "Epoch 114/200, Batch 4/17, Loss G: 3.863389015197754, Loss D: 0.05205681174993515\n",
      "Epoch 114/200, Batch 5/17, Loss G: 3.9036598205566406, Loss D: 0.04807662591338158\n",
      "Epoch 114/200, Batch 6/17, Loss G: 4.038956165313721, Loss D: 0.03439510241150856\n",
      "Epoch 114/200, Batch 7/17, Loss G: 4.068412780761719, Loss D: 0.04227543622255325\n",
      "Epoch 114/200, Batch 8/17, Loss G: 3.962728977203369, Loss D: 0.030694259330630302\n",
      "Epoch 114/200, Batch 9/17, Loss G: 3.9671542644500732, Loss D: 0.024143435060977936\n",
      "Epoch 114/200, Batch 10/17, Loss G: 3.9924325942993164, Loss D: 0.023709921166300774\n",
      "Epoch 114/200, Batch 11/17, Loss G: 4.295403957366943, Loss D: 0.043164219707250595\n",
      "Epoch 114/200, Batch 12/17, Loss G: 3.9539804458618164, Loss D: 0.04948749393224716\n",
      "Epoch 114/200, Batch 13/17, Loss G: 4.155627250671387, Loss D: 0.038718197494745255\n",
      "Epoch 114/200, Batch 14/17, Loss G: 3.8341543674468994, Loss D: 0.03475651890039444\n",
      "Epoch 114/200, Batch 15/17, Loss G: 4.077966213226318, Loss D: 0.020568378269672394\n",
      "Epoch 114/200, Batch 16/17, Loss G: 4.236755847930908, Loss D: 0.02301771752536297\n",
      "Epoch 115/200, Batch 0/17, Loss G: 4.005533218383789, Loss D: 0.010205048136413097\n",
      "Epoch 115/200, Batch 1/17, Loss G: 3.806455135345459, Loss D: 0.03092673234641552\n",
      "Epoch 115/200, Batch 2/17, Loss G: 4.209904670715332, Loss D: 0.028084373101592064\n",
      "Epoch 115/200, Batch 3/17, Loss G: 4.081705093383789, Loss D: 0.010924928821623325\n",
      "Epoch 115/200, Batch 4/17, Loss G: 4.127501964569092, Loss D: 0.011440632864832878\n",
      "Epoch 115/200, Batch 5/17, Loss G: 4.000422477722168, Loss D: 0.018700091168284416\n",
      "Epoch 115/200, Batch 6/17, Loss G: 4.18496036529541, Loss D: 0.022320309653878212\n",
      "Epoch 115/200, Batch 7/17, Loss G: 4.0350799560546875, Loss D: 0.018128443509340286\n",
      "Epoch 115/200, Batch 8/17, Loss G: 4.019021034240723, Loss D: 0.0210601594299078\n",
      "Epoch 115/200, Batch 9/17, Loss G: 4.049384117126465, Loss D: 0.019000310450792313\n",
      "Epoch 115/200, Batch 10/17, Loss G: 4.293706893920898, Loss D: 0.01937228813767433\n",
      "Epoch 115/200, Batch 11/17, Loss G: 4.141310691833496, Loss D: 0.01456928439438343\n",
      "Epoch 115/200, Batch 12/17, Loss G: 4.16689920425415, Loss D: 0.028979729861021042\n",
      "Epoch 115/200, Batch 13/17, Loss G: 3.77626371383667, Loss D: 0.0887787938117981\n",
      "Epoch 115/200, Batch 14/17, Loss G: 4.264514923095703, Loss D: 0.18858292698860168\n",
      "Epoch 115/200, Batch 15/17, Loss G: 3.9796142578125, Loss D: 0.044158048927783966\n",
      "Epoch 115/200, Batch 16/17, Loss G: 3.6139111518859863, Loss D: 0.06460069119930267\n",
      "Epoch 116/200, Batch 0/17, Loss G: 4.131557464599609, Loss D: 0.26165854930877686\n",
      "Epoch 116/200, Batch 1/17, Loss G: 3.633225440979004, Loss D: 0.1458532065153122\n",
      "Epoch 116/200, Batch 2/17, Loss G: 4.098624229431152, Loss D: 0.04233405366539955\n",
      "Epoch 116/200, Batch 3/17, Loss G: 3.9377264976501465, Loss D: 0.052629075944423676\n",
      "Epoch 116/200, Batch 4/17, Loss G: 3.88893723487854, Loss D: 0.09678411483764648\n",
      "Epoch 116/200, Batch 5/17, Loss G: 4.4860639572143555, Loss D: 0.2021564543247223\n",
      "Epoch 116/200, Batch 6/17, Loss G: 3.7922706604003906, Loss D: 0.13968108594417572\n",
      "Epoch 116/200, Batch 7/17, Loss G: 4.308435440063477, Loss D: 0.052376821637153625\n",
      "Epoch 116/200, Batch 8/17, Loss G: 4.06555700302124, Loss D: 0.073231041431427\n",
      "Epoch 116/200, Batch 9/17, Loss G: 3.6281347274780273, Loss D: 0.1714661419391632\n",
      "Epoch 116/200, Batch 10/17, Loss G: 4.174830436706543, Loss D: 0.11819923669099808\n",
      "Epoch 116/200, Batch 11/17, Loss G: 3.8032479286193848, Loss D: 0.05438242107629776\n",
      "Epoch 116/200, Batch 12/17, Loss G: 3.869313955307007, Loss D: 0.05576781928539276\n",
      "Epoch 116/200, Batch 13/17, Loss G: 4.102513313293457, Loss D: 0.03432932123541832\n",
      "Epoch 116/200, Batch 14/17, Loss G: 4.02377986907959, Loss D: 0.04236524552106857\n",
      "Epoch 116/200, Batch 15/17, Loss G: 4.017026424407959, Loss D: 0.03902822732925415\n",
      "Epoch 116/200, Batch 16/17, Loss G: 4.038758754730225, Loss D: 0.11108934879302979\n",
      "Epoch 117/200, Batch 0/17, Loss G: 3.39947772026062, Loss D: 0.12109153717756271\n",
      "Epoch 117/200, Batch 1/17, Loss G: 4.031132698059082, Loss D: 0.043119918555021286\n",
      "Epoch 117/200, Batch 2/17, Loss G: 4.120865821838379, Loss D: 0.04622533172369003\n",
      "Epoch 117/200, Batch 3/17, Loss G: 3.9681262969970703, Loss D: 0.05020701885223389\n",
      "Epoch 117/200, Batch 4/17, Loss G: 3.810141086578369, Loss D: 0.049555011093616486\n",
      "Epoch 117/200, Batch 5/17, Loss G: 3.9737000465393066, Loss D: 0.035526446998119354\n",
      "Epoch 117/200, Batch 6/17, Loss G: 4.330820083618164, Loss D: 0.039648473262786865\n",
      "Epoch 117/200, Batch 7/17, Loss G: 3.9521803855895996, Loss D: 0.05141275376081467\n",
      "Epoch 117/200, Batch 8/17, Loss G: 4.08182430267334, Loss D: 0.03321389481425285\n",
      "Epoch 117/200, Batch 9/17, Loss G: 3.961894989013672, Loss D: 0.028268754482269287\n",
      "Epoch 117/200, Batch 10/17, Loss G: 3.8981094360351562, Loss D: 0.02540125697851181\n",
      "Epoch 117/200, Batch 11/17, Loss G: 4.119886875152588, Loss D: 0.018528487533330917\n",
      "Epoch 117/200, Batch 12/17, Loss G: 4.147802829742432, Loss D: 0.05890602245926857\n",
      "Epoch 117/200, Batch 13/17, Loss G: 3.935863494873047, Loss D: 0.07220832258462906\n",
      "Epoch 117/200, Batch 14/17, Loss G: 4.092474460601807, Loss D: 0.017560236155986786\n",
      "Epoch 117/200, Batch 15/17, Loss G: 4.299027442932129, Loss D: 0.049018774181604385\n",
      "Epoch 117/200, Batch 16/17, Loss G: 3.7956671714782715, Loss D: 0.04853866249322891\n",
      "Epoch 118/200, Batch 0/17, Loss G: 4.074107646942139, Loss D: 0.01462705060839653\n",
      "Epoch 118/200, Batch 1/17, Loss G: 4.263286590576172, Loss D: 0.044653650373220444\n",
      "Epoch 118/200, Batch 2/17, Loss G: 3.8199968338012695, Loss D: 0.03628942370414734\n",
      "Epoch 118/200, Batch 3/17, Loss G: 3.997291326522827, Loss D: 0.02533016726374626\n",
      "Epoch 118/200, Batch 4/17, Loss G: 4.184571266174316, Loss D: 0.01794431544840336\n",
      "Epoch 118/200, Batch 5/17, Loss G: 4.1098785400390625, Loss D: 0.020210932940244675\n",
      "Epoch 118/200, Batch 6/17, Loss G: 4.286710262298584, Loss D: 0.03133515268564224\n",
      "Epoch 118/200, Batch 7/17, Loss G: 4.0194878578186035, Loss D: 0.023874621838331223\n",
      "Epoch 118/200, Batch 8/17, Loss G: 4.105860233306885, Loss D: 0.028527066111564636\n",
      "Epoch 118/200, Batch 9/17, Loss G: 3.838353157043457, Loss D: 0.021726828068494797\n",
      "Epoch 118/200, Batch 10/17, Loss G: 4.27640438079834, Loss D: 0.03838218003511429\n",
      "Epoch 118/200, Batch 11/17, Loss G: 3.977795124053955, Loss D: 0.04166966304183006\n",
      "Epoch 118/200, Batch 12/17, Loss G: 4.1414031982421875, Loss D: 0.03032911941409111\n",
      "Epoch 118/200, Batch 13/17, Loss G: 4.042349338531494, Loss D: 0.031472738832235336\n",
      "Epoch 118/200, Batch 14/17, Loss G: 3.98954176902771, Loss D: 0.014480166137218475\n",
      "Epoch 118/200, Batch 15/17, Loss G: 3.944389581680298, Loss D: 0.02257090061903\n",
      "Epoch 118/200, Batch 16/17, Loss G: 4.069616317749023, Loss D: 0.017887910827994347\n",
      "Epoch 119/200, Batch 0/17, Loss G: 3.9569334983825684, Loss D: 0.016636565327644348\n",
      "Epoch 119/200, Batch 1/17, Loss G: 4.052199363708496, Loss D: 0.025544747710227966\n",
      "Epoch 119/200, Batch 2/17, Loss G: 4.069550514221191, Loss D: 0.024331970140337944\n",
      "Epoch 119/200, Batch 3/17, Loss G: 3.962783098220825, Loss D: 0.03163925185799599\n",
      "Epoch 119/200, Batch 4/17, Loss G: 4.090954303741455, Loss D: 0.038432445377111435\n",
      "Epoch 119/200, Batch 5/17, Loss G: 3.888681173324585, Loss D: 0.0457819327712059\n",
      "Epoch 119/200, Batch 6/17, Loss G: 4.236525535583496, Loss D: 0.021243881434202194\n",
      "Epoch 119/200, Batch 7/17, Loss G: 4.06762170791626, Loss D: 0.030181488022208214\n",
      "Epoch 119/200, Batch 8/17, Loss G: 3.6111819744110107, Loss D: 0.05304010212421417\n",
      "Epoch 119/200, Batch 9/17, Loss G: 4.188694953918457, Loss D: 0.02837819792330265\n",
      "Epoch 119/200, Batch 10/17, Loss G: 4.178608417510986, Loss D: 0.015579473227262497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200, Batch 11/17, Loss G: 4.122475624084473, Loss D: 0.017201360315084457\n",
      "Epoch 119/200, Batch 12/17, Loss G: 4.100874423980713, Loss D: 0.01982303149998188\n",
      "Epoch 119/200, Batch 13/17, Loss G: 4.186491966247559, Loss D: 0.019383953884243965\n",
      "Epoch 119/200, Batch 14/17, Loss G: 4.159006118774414, Loss D: 0.017933588474988937\n",
      "Epoch 119/200, Batch 15/17, Loss G: 4.024715900421143, Loss D: 0.009576261043548584\n",
      "Epoch 119/200, Batch 16/17, Loss G: 4.006995677947998, Loss D: 0.018894415348768234\n",
      "Epoch 120/200, Batch 0/17, Loss G: 4.04368782043457, Loss D: 0.019847944378852844\n",
      "Epoch 120/200, Batch 1/17, Loss G: 3.9551267623901367, Loss D: 0.013230854645371437\n",
      "Epoch 120/200, Batch 2/17, Loss G: 3.9047164916992188, Loss D: 0.014860756695270538\n",
      "Epoch 120/200, Batch 3/17, Loss G: 4.148375988006592, Loss D: 0.02414870820939541\n",
      "Epoch 120/200, Batch 4/17, Loss G: 3.869865894317627, Loss D: 0.03385981172323227\n",
      "Epoch 120/200, Batch 5/17, Loss G: 4.247208595275879, Loss D: 0.016693195328116417\n",
      "Epoch 120/200, Batch 6/17, Loss G: 3.95120906829834, Loss D: 0.0389634445309639\n",
      "Epoch 120/200, Batch 7/17, Loss G: 3.7964768409729004, Loss D: 0.05563991144299507\n",
      "Epoch 120/200, Batch 8/17, Loss G: 4.332392692565918, Loss D: 0.013024172745645046\n",
      "Epoch 120/200, Batch 9/17, Loss G: 4.05293607711792, Loss D: 0.05900850519537926\n",
      "Epoch 120/200, Batch 10/17, Loss G: 3.6834001541137695, Loss D: 0.10834299027919769\n",
      "Epoch 120/200, Batch 11/17, Loss G: 4.257957935333252, Loss D: 0.08626270294189453\n",
      "Epoch 120/200, Batch 12/17, Loss G: 3.6718835830688477, Loss D: 0.09810178726911545\n",
      "Epoch 120/200, Batch 13/17, Loss G: 4.245964527130127, Loss D: 0.09077541530132294\n",
      "Epoch 120/200, Batch 14/17, Loss G: 3.8184080123901367, Loss D: 0.03454314172267914\n",
      "Epoch 120/200, Batch 15/17, Loss G: 4.174775123596191, Loss D: 0.02684735879302025\n",
      "Epoch 120/200, Batch 16/17, Loss G: 3.9125006198883057, Loss D: 0.048444557934999466\n",
      "Epoch 121/200, Batch 0/17, Loss G: 3.674398183822632, Loss D: 0.039601460099220276\n",
      "Epoch 121/200, Batch 1/17, Loss G: 4.000725746154785, Loss D: 0.034651558846235275\n",
      "Epoch 121/200, Batch 2/17, Loss G: 3.9392781257629395, Loss D: 0.03360439091920853\n",
      "Epoch 121/200, Batch 3/17, Loss G: 3.7773537635803223, Loss D: 0.02732163853943348\n",
      "Epoch 121/200, Batch 4/17, Loss G: 4.144048690795898, Loss D: 0.03738570213317871\n",
      "Epoch 121/200, Batch 5/17, Loss G: 3.936690330505371, Loss D: 0.036269936710596085\n",
      "Epoch 121/200, Batch 6/17, Loss G: 4.224477767944336, Loss D: 0.030337434262037277\n",
      "Epoch 121/200, Batch 7/17, Loss G: 3.8150436878204346, Loss D: 0.02935904636979103\n",
      "Epoch 121/200, Batch 8/17, Loss G: 4.093057155609131, Loss D: 0.005884783808141947\n",
      "Epoch 121/200, Batch 9/17, Loss G: 4.110917091369629, Loss D: 0.006219821982085705\n",
      "Epoch 121/200, Batch 10/17, Loss G: 4.107503414154053, Loss D: 0.004296381026506424\n",
      "Epoch 121/200, Batch 11/17, Loss G: 3.9690024852752686, Loss D: 0.036910705268383026\n",
      "Epoch 121/200, Batch 12/17, Loss G: 4.494571685791016, Loss D: 0.020676389336586\n",
      "Epoch 121/200, Batch 13/17, Loss G: 4.106294631958008, Loss D: 0.01697411574423313\n",
      "Epoch 121/200, Batch 14/17, Loss G: 3.8490190505981445, Loss D: 0.03385079279541969\n",
      "Epoch 121/200, Batch 15/17, Loss G: 4.2341203689575195, Loss D: 0.015936629846692085\n",
      "Epoch 121/200, Batch 16/17, Loss G: 4.220386505126953, Loss D: 0.03996676206588745\n",
      "Epoch 122/200, Batch 0/17, Loss G: 3.620779037475586, Loss D: 0.08792366087436676\n",
      "Epoch 122/200, Batch 1/17, Loss G: 4.331409454345703, Loss D: 0.06571844220161438\n",
      "Epoch 122/200, Batch 2/17, Loss G: 3.9546918869018555, Loss D: 0.023144738748669624\n",
      "Epoch 122/200, Batch 3/17, Loss G: 4.005589485168457, Loss D: 0.03987470641732216\n",
      "Epoch 122/200, Batch 4/17, Loss G: 4.092377662658691, Loss D: 0.03192151337862015\n",
      "Epoch 122/200, Batch 5/17, Loss G: 3.9158830642700195, Loss D: 0.04165569692850113\n",
      "Epoch 122/200, Batch 6/17, Loss G: 3.884373903274536, Loss D: 0.027582498267292976\n",
      "Epoch 122/200, Batch 7/17, Loss G: 4.179935455322266, Loss D: 0.03871678188443184\n",
      "Epoch 122/200, Batch 8/17, Loss G: 3.985410213470459, Loss D: 0.04145585000514984\n",
      "Epoch 122/200, Batch 9/17, Loss G: 4.043977737426758, Loss D: 0.014495481736958027\n",
      "Epoch 122/200, Batch 10/17, Loss G: 3.9942686557769775, Loss D: 0.016625171527266502\n",
      "Epoch 122/200, Batch 11/17, Loss G: 4.209142208099365, Loss D: 0.03912830352783203\n",
      "Epoch 122/200, Batch 12/17, Loss G: 3.716181755065918, Loss D: 0.0503825768828392\n",
      "Epoch 122/200, Batch 13/17, Loss G: 4.117398738861084, Loss D: 0.019167479127645493\n",
      "Epoch 122/200, Batch 14/17, Loss G: 4.1885552406311035, Loss D: 0.09923883527517319\n",
      "Epoch 122/200, Batch 15/17, Loss G: 3.3894400596618652, Loss D: 0.20843273401260376\n",
      "Epoch 122/200, Batch 16/17, Loss G: 4.305967330932617, Loss D: 0.26027363538742065\n",
      "Epoch 123/200, Batch 0/17, Loss G: 3.6899638175964355, Loss D: 0.10733203589916229\n",
      "Epoch 123/200, Batch 1/17, Loss G: 3.8904495239257812, Loss D: 0.052941784262657166\n",
      "Epoch 123/200, Batch 2/17, Loss G: 4.100229263305664, Loss D: 0.10747803747653961\n",
      "Epoch 123/200, Batch 3/17, Loss G: 3.3703768253326416, Loss D: 0.18522147834300995\n",
      "Epoch 123/200, Batch 4/17, Loss G: 3.983896255493164, Loss D: 0.06736667454242706\n",
      "Epoch 123/200, Batch 5/17, Loss G: 4.072308540344238, Loss D: 0.04620436206459999\n",
      "Epoch 123/200, Batch 6/17, Loss G: 3.5636720657348633, Loss D: 0.08336697518825531\n",
      "Epoch 123/200, Batch 7/17, Loss G: 4.099434852600098, Loss D: 0.05459451675415039\n",
      "Epoch 123/200, Batch 8/17, Loss G: 3.8871192932128906, Loss D: 0.03657735139131546\n",
      "Epoch 123/200, Batch 9/17, Loss G: 3.933138370513916, Loss D: 0.04108472913503647\n",
      "Epoch 123/200, Batch 10/17, Loss G: 4.164896011352539, Loss D: 0.047349005937576294\n",
      "Epoch 123/200, Batch 11/17, Loss G: 3.8830442428588867, Loss D: 0.046379879117012024\n",
      "Epoch 123/200, Batch 12/17, Loss G: 4.17711067199707, Loss D: 0.06668628007173538\n",
      "Epoch 123/200, Batch 13/17, Loss G: 3.916597366333008, Loss D: 0.06900966912508011\n",
      "Epoch 123/200, Batch 14/17, Loss G: 4.119528770446777, Loss D: 0.03453449159860611\n",
      "Epoch 123/200, Batch 15/17, Loss G: 3.931276321411133, Loss D: 0.02823309600353241\n",
      "Epoch 123/200, Batch 16/17, Loss G: 3.787477970123291, Loss D: 0.034988246858119965\n",
      "Epoch 124/200, Batch 0/17, Loss G: 4.144761085510254, Loss D: 0.01969367265701294\n",
      "Epoch 124/200, Batch 1/17, Loss G: 4.104284286499023, Loss D: 0.020231401547789574\n",
      "Epoch 124/200, Batch 2/17, Loss G: 4.0418853759765625, Loss D: 0.028833314776420593\n",
      "Epoch 124/200, Batch 3/17, Loss G: 4.003604412078857, Loss D: 0.04210260882973671\n",
      "Epoch 124/200, Batch 4/17, Loss G: 3.664639472961426, Loss D: 0.06563218683004379\n",
      "Epoch 124/200, Batch 5/17, Loss G: 4.299785614013672, Loss D: 0.04416441544890404\n",
      "Epoch 124/200, Batch 6/17, Loss G: 3.7878849506378174, Loss D: 0.027741223573684692\n",
      "Epoch 124/200, Batch 7/17, Loss G: 3.713192939758301, Loss D: 0.03467768803238869\n",
      "Epoch 124/200, Batch 8/17, Loss G: 4.128891944885254, Loss D: 0.030950428918004036\n",
      "Epoch 124/200, Batch 9/17, Loss G: 4.035709381103516, Loss D: 0.022480813786387444\n",
      "Epoch 124/200, Batch 10/17, Loss G: 4.087713718414307, Loss D: 0.024417763575911522\n",
      "Epoch 124/200, Batch 11/17, Loss G: 3.953094005584717, Loss D: 0.023790501058101654\n",
      "Epoch 124/200, Batch 12/17, Loss G: 4.091826438903809, Loss D: 0.014962416142225266\n",
      "Epoch 124/200, Batch 13/17, Loss G: 4.266302585601807, Loss D: 0.014055490493774414\n",
      "Epoch 124/200, Batch 14/17, Loss G: 3.901060104370117, Loss D: 0.017384391278028488\n",
      "Epoch 124/200, Batch 15/17, Loss G: 3.868316173553467, Loss D: 0.01792275533080101\n",
      "Epoch 124/200, Batch 16/17, Loss G: 4.172703742980957, Loss D: 0.006953088566660881\n",
      "Epoch 125/200, Batch 0/17, Loss G: 4.155184268951416, Loss D: 0.015043088234961033\n",
      "Epoch 125/200, Batch 1/17, Loss G: 4.111130714416504, Loss D: 0.008801480755209923\n",
      "Epoch 125/200, Batch 2/17, Loss G: 4.121323585510254, Loss D: 0.019930042326450348\n",
      "Epoch 125/200, Batch 3/17, Loss G: 4.063409805297852, Loss D: 0.025619229301810265\n",
      "Epoch 125/200, Batch 4/17, Loss G: 3.726846694946289, Loss D: 0.02169298753142357\n",
      "Epoch 125/200, Batch 5/17, Loss G: 3.899158000946045, Loss D: 0.015053385868668556\n",
      "Epoch 125/200, Batch 6/17, Loss G: 4.208300590515137, Loss D: 0.027302749454975128\n",
      "Epoch 125/200, Batch 7/17, Loss G: 3.944016218185425, Loss D: 0.021809857338666916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200, Batch 8/17, Loss G: 3.921751022338867, Loss D: 0.017412565648555756\n",
      "Epoch 125/200, Batch 9/17, Loss G: 4.170870780944824, Loss D: 0.030787892639636993\n",
      "Epoch 125/200, Batch 10/17, Loss G: 3.9110615253448486, Loss D: 0.014941972680389881\n",
      "Epoch 125/200, Batch 11/17, Loss G: 3.894740343093872, Loss D: 0.027140986174345016\n",
      "Epoch 125/200, Batch 12/17, Loss G: 4.435401439666748, Loss D: 0.08260234445333481\n",
      "Epoch 125/200, Batch 13/17, Loss G: 3.4532313346862793, Loss D: 0.1327352374792099\n",
      "Epoch 125/200, Batch 14/17, Loss G: 4.231269836425781, Loss D: 0.038972433656454086\n",
      "Epoch 125/200, Batch 15/17, Loss G: 4.12601375579834, Loss D: 0.027010275050997734\n",
      "Epoch 125/200, Batch 16/17, Loss G: 3.77779483795166, Loss D: 0.0233771950006485\n",
      "Epoch 126/200, Batch 0/17, Loss G: 3.8307271003723145, Loss D: 0.05720217153429985\n",
      "Epoch 126/200, Batch 1/17, Loss G: 4.259917259216309, Loss D: 0.049379002302885056\n",
      "Epoch 126/200, Batch 2/17, Loss G: 3.9566574096679688, Loss D: 0.03501921147108078\n",
      "Epoch 126/200, Batch 3/17, Loss G: 3.8696556091308594, Loss D: 0.026617929339408875\n",
      "Epoch 126/200, Batch 4/17, Loss G: 3.8451006412506104, Loss D: 0.03364361450076103\n",
      "Epoch 126/200, Batch 5/17, Loss G: 4.066054344177246, Loss D: 0.01918339915573597\n",
      "Epoch 126/200, Batch 6/17, Loss G: 3.9017333984375, Loss D: 0.03988495096564293\n",
      "Epoch 126/200, Batch 7/17, Loss G: 4.00634765625, Loss D: 0.05303078517317772\n",
      "Epoch 126/200, Batch 8/17, Loss G: 4.098911285400391, Loss D: 0.020763587206602097\n",
      "Epoch 126/200, Batch 9/17, Loss G: 3.9360194206237793, Loss D: 0.0391857735812664\n",
      "Epoch 126/200, Batch 10/17, Loss G: 3.809267520904541, Loss D: 0.05400571599602699\n",
      "Epoch 126/200, Batch 11/17, Loss G: 3.92921781539917, Loss D: 0.01748531311750412\n",
      "Epoch 126/200, Batch 12/17, Loss G: 3.90999698638916, Loss D: 0.032750047743320465\n",
      "Epoch 126/200, Batch 13/17, Loss G: 3.8650646209716797, Loss D: 0.02733665332198143\n",
      "Epoch 126/200, Batch 14/17, Loss G: 4.032536506652832, Loss D: 0.011728912591934204\n",
      "Epoch 126/200, Batch 15/17, Loss G: 4.0854058265686035, Loss D: 0.022125549614429474\n",
      "Epoch 126/200, Batch 16/17, Loss G: 4.077641010284424, Loss D: 0.028879281133413315\n",
      "Epoch 127/200, Batch 0/17, Loss G: 4.04840612411499, Loss D: 0.011535273864865303\n",
      "Epoch 127/200, Batch 1/17, Loss G: 4.191128730773926, Loss D: 0.020473690703511238\n",
      "Epoch 127/200, Batch 2/17, Loss G: 4.025027275085449, Loss D: 0.03993307054042816\n",
      "Epoch 127/200, Batch 3/17, Loss G: 3.707270622253418, Loss D: 0.05778047814965248\n",
      "Epoch 127/200, Batch 4/17, Loss G: 3.919174909591675, Loss D: 0.02899862453341484\n",
      "Epoch 127/200, Batch 5/17, Loss G: 4.079355716705322, Loss D: 0.029643256217241287\n",
      "Epoch 127/200, Batch 6/17, Loss G: 3.9529361724853516, Loss D: 0.03660806640982628\n",
      "Epoch 127/200, Batch 7/17, Loss G: 4.0116705894470215, Loss D: 0.040637485682964325\n",
      "Epoch 127/200, Batch 8/17, Loss G: 3.7367444038391113, Loss D: 0.0338328592479229\n",
      "Epoch 127/200, Batch 9/17, Loss G: 4.026185035705566, Loss D: 0.01821894571185112\n",
      "Epoch 127/200, Batch 10/17, Loss G: 4.138857841491699, Loss D: 0.022438379004597664\n",
      "Epoch 127/200, Batch 11/17, Loss G: 3.9276070594787598, Loss D: 0.018476534634828568\n",
      "Epoch 127/200, Batch 12/17, Loss G: 3.932755708694458, Loss D: 0.01516631804406643\n",
      "Epoch 127/200, Batch 13/17, Loss G: 3.997502088546753, Loss D: 0.013994005508720875\n",
      "Epoch 127/200, Batch 14/17, Loss G: 3.964240074157715, Loss D: 0.026449495926499367\n",
      "Epoch 127/200, Batch 15/17, Loss G: 3.70757794380188, Loss D: 0.06887298822402954\n",
      "Epoch 127/200, Batch 16/17, Loss G: 4.094511032104492, Loss D: 0.10305788367986679\n",
      "Epoch 128/200, Batch 0/17, Loss G: 3.356489896774292, Loss D: 0.20439377427101135\n",
      "Epoch 128/200, Batch 1/17, Loss G: 4.086000919342041, Loss D: 0.042224034667015076\n",
      "Epoch 128/200, Batch 2/17, Loss G: 4.085160732269287, Loss D: 0.028368914499878883\n",
      "Epoch 128/200, Batch 3/17, Loss G: 3.5661325454711914, Loss D: 0.07530330866575241\n",
      "Epoch 128/200, Batch 4/17, Loss G: 4.155746936798096, Loss D: 0.09525298327207565\n",
      "Epoch 128/200, Batch 5/17, Loss G: 3.7170848846435547, Loss D: 0.06975611299276352\n",
      "Epoch 128/200, Batch 6/17, Loss G: 4.043770790100098, Loss D: 0.03493291884660721\n",
      "Epoch 128/200, Batch 7/17, Loss G: 3.959713935852051, Loss D: 0.036864664405584335\n",
      "Epoch 128/200, Batch 8/17, Loss G: 3.7627718448638916, Loss D: 0.05349951237440109\n",
      "Epoch 128/200, Batch 9/17, Loss G: 4.085352897644043, Loss D: 0.04276615008711815\n",
      "Epoch 128/200, Batch 10/17, Loss G: 3.965390205383301, Loss D: 0.028513135388493538\n",
      "Epoch 128/200, Batch 11/17, Loss G: 3.795011043548584, Loss D: 0.023035138845443726\n",
      "Epoch 128/200, Batch 12/17, Loss G: 4.2348456382751465, Loss D: 0.026076005771756172\n",
      "Epoch 128/200, Batch 13/17, Loss G: 3.754772424697876, Loss D: 0.05556053668260574\n",
      "Epoch 128/200, Batch 14/17, Loss G: 4.182669639587402, Loss D: 0.09509346634149551\n",
      "Epoch 128/200, Batch 15/17, Loss G: 3.601688861846924, Loss D: 0.06704253703355789\n",
      "Epoch 128/200, Batch 16/17, Loss G: 4.177223205566406, Loss D: 0.020486999303102493\n",
      "Epoch 129/200, Batch 0/17, Loss G: 4.12014102935791, Loss D: 0.030214104801416397\n",
      "Epoch 129/200, Batch 1/17, Loss G: 4.24350643157959, Loss D: 0.017446408048272133\n",
      "Epoch 129/200, Batch 2/17, Loss G: 3.9431509971618652, Loss D: 0.0341535359621048\n",
      "Epoch 129/200, Batch 3/17, Loss G: 3.9321532249450684, Loss D: 0.030431091785430908\n",
      "Epoch 129/200, Batch 4/17, Loss G: 3.8395488262176514, Loss D: 0.027246667072176933\n",
      "Epoch 129/200, Batch 5/17, Loss G: 3.9314308166503906, Loss D: 0.025174111127853394\n",
      "Epoch 129/200, Batch 6/17, Loss G: 3.8190360069274902, Loss D: 0.013536033220589161\n",
      "Epoch 129/200, Batch 7/17, Loss G: 3.9256982803344727, Loss D: 0.025830652564764023\n",
      "Epoch 129/200, Batch 8/17, Loss G: 4.087686061859131, Loss D: 0.014608452096581459\n",
      "Epoch 129/200, Batch 9/17, Loss G: 4.244385719299316, Loss D: 0.033255815505981445\n",
      "Epoch 129/200, Batch 10/17, Loss G: 3.8942489624023438, Loss D: 0.048469580709934235\n",
      "Epoch 129/200, Batch 11/17, Loss G: 4.015255451202393, Loss D: 0.04182228073477745\n",
      "Epoch 129/200, Batch 12/17, Loss G: 3.9381046295166016, Loss D: 0.02098137140274048\n",
      "Epoch 129/200, Batch 13/17, Loss G: 3.7451624870300293, Loss D: 0.04073699191212654\n",
      "Epoch 129/200, Batch 14/17, Loss G: 4.118283271789551, Loss D: 0.05841904878616333\n",
      "Epoch 129/200, Batch 15/17, Loss G: 3.7858738899230957, Loss D: 0.0431675910949707\n",
      "Epoch 129/200, Batch 16/17, Loss G: 3.9876980781555176, Loss D: 0.022960655391216278\n",
      "Epoch 130/200, Batch 0/17, Loss G: 3.779799699783325, Loss D: 0.026780683547258377\n",
      "Epoch 130/200, Batch 1/17, Loss G: 4.106268882751465, Loss D: 0.021550288423895836\n",
      "Epoch 130/200, Batch 2/17, Loss G: 4.217405319213867, Loss D: 0.014786443673074245\n",
      "Epoch 130/200, Batch 3/17, Loss G: 3.9953839778900146, Loss D: 0.019390324130654335\n",
      "Epoch 130/200, Batch 4/17, Loss G: 4.050718784332275, Loss D: 0.011981918476521969\n",
      "Epoch 130/200, Batch 5/17, Loss G: 3.857919692993164, Loss D: 0.035919755697250366\n",
      "Epoch 130/200, Batch 6/17, Loss G: 4.200150489807129, Loss D: 0.051419928669929504\n",
      "Epoch 130/200, Batch 7/17, Loss G: 3.8879709243774414, Loss D: 0.011954737827181816\n",
      "Epoch 130/200, Batch 8/17, Loss G: 3.956077814102173, Loss D: 0.0550558902323246\n",
      "Epoch 130/200, Batch 9/17, Loss G: 4.2259297370910645, Loss D: 0.052626222372055054\n",
      "Epoch 130/200, Batch 10/17, Loss G: 3.731637954711914, Loss D: 0.01992473565042019\n",
      "Epoch 130/200, Batch 11/17, Loss G: 3.99802303314209, Loss D: 0.010180192068219185\n",
      "Epoch 130/200, Batch 12/17, Loss G: 3.9371910095214844, Loss D: 0.015947403386235237\n",
      "Epoch 130/200, Batch 13/17, Loss G: 3.9787466526031494, Loss D: 0.050680071115493774\n",
      "Epoch 130/200, Batch 14/17, Loss G: 3.6499385833740234, Loss D: 0.09200521558523178\n",
      "Epoch 130/200, Batch 15/17, Loss G: 4.109997749328613, Loss D: 0.051037661731243134\n",
      "Epoch 130/200, Batch 16/17, Loss G: 4.013246059417725, Loss D: 0.03255530074238777\n",
      "Epoch 131/200, Batch 0/17, Loss G: 3.857811689376831, Loss D: 0.053610917180776596\n",
      "Epoch 131/200, Batch 1/17, Loss G: 4.150537967681885, Loss D: 0.020959312096238136\n",
      "Epoch 131/200, Batch 2/17, Loss G: 4.014770984649658, Loss D: 0.04420386999845505\n",
      "Epoch 131/200, Batch 3/17, Loss G: 3.8236660957336426, Loss D: 0.04974324256181717\n",
      "Epoch 131/200, Batch 4/17, Loss G: 3.9904513359069824, Loss D: 0.02374555915594101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200, Batch 5/17, Loss G: 4.115374565124512, Loss D: 0.02916920930147171\n",
      "Epoch 131/200, Batch 6/17, Loss G: 3.8692667484283447, Loss D: 0.018415579572319984\n",
      "Epoch 131/200, Batch 7/17, Loss G: 3.8717188835144043, Loss D: 0.022886550053954124\n",
      "Epoch 131/200, Batch 8/17, Loss G: 4.147595405578613, Loss D: 0.022873077541589737\n",
      "Epoch 131/200, Batch 9/17, Loss G: 3.902207136154175, Loss D: 0.01626085489988327\n",
      "Epoch 131/200, Batch 10/17, Loss G: 4.046056270599365, Loss D: 0.032234229147434235\n",
      "Epoch 131/200, Batch 11/17, Loss G: 4.124180316925049, Loss D: 0.01672903262078762\n",
      "Epoch 131/200, Batch 12/17, Loss G: 3.918808937072754, Loss D: 0.024699246510863304\n",
      "Epoch 131/200, Batch 13/17, Loss G: 3.965453863143921, Loss D: 0.0172733124345541\n",
      "Epoch 131/200, Batch 14/17, Loss G: 3.872861623764038, Loss D: 0.014426121488213539\n",
      "Epoch 131/200, Batch 15/17, Loss G: 3.879030704498291, Loss D: 0.012535547837615013\n",
      "Epoch 131/200, Batch 16/17, Loss G: 4.059455871582031, Loss D: 0.03036775439977646\n",
      "Epoch 132/200, Batch 0/17, Loss G: 3.729397773742676, Loss D: 0.05104600638151169\n",
      "Epoch 132/200, Batch 1/17, Loss G: 4.0643157958984375, Loss D: 0.02160034328699112\n",
      "Epoch 132/200, Batch 2/17, Loss G: 3.887442111968994, Loss D: 0.0288829542696476\n",
      "Epoch 132/200, Batch 3/17, Loss G: 3.563988447189331, Loss D: 0.07029435783624649\n",
      "Epoch 132/200, Batch 4/17, Loss G: 4.015153884887695, Loss D: 0.08794933557510376\n",
      "Epoch 132/200, Batch 5/17, Loss G: 3.7558367252349854, Loss D: 0.03878885507583618\n",
      "Epoch 132/200, Batch 6/17, Loss G: 4.064642906188965, Loss D: 0.026630504056811333\n",
      "Epoch 132/200, Batch 7/17, Loss G: 3.7939062118530273, Loss D: 0.03934244439005852\n",
      "Epoch 132/200, Batch 8/17, Loss G: 4.236525535583496, Loss D: 0.06649366021156311\n",
      "Epoch 132/200, Batch 9/17, Loss G: 3.6697988510131836, Loss D: 0.06363172829151154\n",
      "Epoch 132/200, Batch 10/17, Loss G: 4.02846622467041, Loss D: 0.021853432059288025\n",
      "Epoch 132/200, Batch 11/17, Loss G: 4.10573148727417, Loss D: 0.03316125646233559\n",
      "Epoch 132/200, Batch 12/17, Loss G: 3.7463037967681885, Loss D: 0.032269299030303955\n",
      "Epoch 132/200, Batch 13/17, Loss G: 3.849180221557617, Loss D: 0.05018988996744156\n",
      "Epoch 132/200, Batch 14/17, Loss G: 3.499499797821045, Loss D: 0.07431676238775253\n",
      "Epoch 132/200, Batch 15/17, Loss G: 4.400391578674316, Loss D: 0.09796246886253357\n",
      "Epoch 132/200, Batch 16/17, Loss G: 3.8088653087615967, Loss D: 0.042020007967948914\n",
      "Epoch 133/200, Batch 0/17, Loss G: 4.1051530838012695, Loss D: 0.01719990000128746\n",
      "Epoch 133/200, Batch 1/17, Loss G: 4.053865909576416, Loss D: 0.026471659541130066\n",
      "Epoch 133/200, Batch 2/17, Loss G: 4.056407928466797, Loss D: 0.03502121567726135\n",
      "Epoch 133/200, Batch 3/17, Loss G: 3.735863208770752, Loss D: 0.06637651473283768\n",
      "Epoch 133/200, Batch 4/17, Loss G: 4.289918422698975, Loss D: 0.03623463213443756\n",
      "Epoch 133/200, Batch 5/17, Loss G: 3.8459630012512207, Loss D: 0.033825647085905075\n",
      "Epoch 133/200, Batch 6/17, Loss G: 3.9877450466156006, Loss D: 0.02262350730597973\n",
      "Epoch 133/200, Batch 7/17, Loss G: 3.9368600845336914, Loss D: 0.018288109451532364\n",
      "Epoch 133/200, Batch 8/17, Loss G: 3.9549224376678467, Loss D: 0.01367315836250782\n",
      "Epoch 133/200, Batch 9/17, Loss G: 4.077777862548828, Loss D: 0.014324036426842213\n",
      "Epoch 133/200, Batch 10/17, Loss G: 3.9899260997772217, Loss D: 0.037507396191358566\n",
      "Epoch 133/200, Batch 11/17, Loss G: 3.604220390319824, Loss D: 0.04215436056256294\n",
      "Epoch 133/200, Batch 12/17, Loss G: 4.175623416900635, Loss D: 0.020392850041389465\n",
      "Epoch 133/200, Batch 13/17, Loss G: 3.817251682281494, Loss D: 0.018766961991786957\n",
      "Epoch 133/200, Batch 14/17, Loss G: 3.7229814529418945, Loss D: 0.028888089582324028\n",
      "Epoch 133/200, Batch 15/17, Loss G: 4.087297439575195, Loss D: 0.027338476851582527\n",
      "Epoch 133/200, Batch 16/17, Loss G: 4.1223015785217285, Loss D: 0.021768664941191673\n",
      "Epoch 134/200, Batch 0/17, Loss G: 3.9986751079559326, Loss D: 0.01625664532184601\n",
      "Epoch 134/200, Batch 1/17, Loss G: 3.9954190254211426, Loss D: 0.02529168874025345\n",
      "Epoch 134/200, Batch 2/17, Loss G: 4.010663032531738, Loss D: 0.025429382920265198\n",
      "Epoch 134/200, Batch 3/17, Loss G: 4.164178371429443, Loss D: 0.012071123346686363\n",
      "Epoch 134/200, Batch 4/17, Loss G: 3.976269245147705, Loss D: 0.00731914397329092\n",
      "Epoch 134/200, Batch 5/17, Loss G: 3.985057830810547, Loss D: 0.011171778663992882\n",
      "Epoch 134/200, Batch 6/17, Loss G: 3.817225456237793, Loss D: 0.013538751751184464\n",
      "Epoch 134/200, Batch 7/17, Loss G: 3.9139404296875, Loss D: 0.016315288841724396\n",
      "Epoch 134/200, Batch 8/17, Loss G: 4.001497268676758, Loss D: 0.015462908893823624\n",
      "Epoch 134/200, Batch 9/17, Loss G: 4.040131568908691, Loss D: 0.015118861570954323\n",
      "Epoch 134/200, Batch 10/17, Loss G: 3.920459270477295, Loss D: 0.010272214189171791\n",
      "Epoch 134/200, Batch 11/17, Loss G: 3.8170175552368164, Loss D: 0.016466092318296432\n",
      "Epoch 134/200, Batch 12/17, Loss G: 3.9886300563812256, Loss D: 0.009156711399555206\n",
      "Epoch 134/200, Batch 13/17, Loss G: 3.981778621673584, Loss D: 0.011281438171863556\n",
      "Epoch 134/200, Batch 14/17, Loss G: 3.8171463012695312, Loss D: 0.013699447736144066\n",
      "Epoch 134/200, Batch 15/17, Loss G: 3.8844187259674072, Loss D: 0.016276605427265167\n",
      "Epoch 134/200, Batch 16/17, Loss G: 4.230936527252197, Loss D: 0.009917388670146465\n",
      "Epoch 135/200, Batch 0/17, Loss G: 4.075030326843262, Loss D: 0.012880775146186352\n",
      "Epoch 135/200, Batch 1/17, Loss G: 3.9441466331481934, Loss D: 0.009481063112616539\n",
      "Epoch 135/200, Batch 2/17, Loss G: 4.005459308624268, Loss D: 0.014184841886162758\n",
      "Epoch 135/200, Batch 3/17, Loss G: 3.9692983627319336, Loss D: 0.01142185740172863\n",
      "Epoch 135/200, Batch 4/17, Loss G: 3.7285513877868652, Loss D: 0.016105271875858307\n",
      "Epoch 135/200, Batch 5/17, Loss G: 4.02861213684082, Loss D: 0.01144466269761324\n",
      "Epoch 135/200, Batch 6/17, Loss G: 3.858447551727295, Loss D: 0.011835649609565735\n",
      "Epoch 135/200, Batch 7/17, Loss G: 3.8719539642333984, Loss D: 0.015278246253728867\n",
      "Epoch 135/200, Batch 8/17, Loss G: 4.0401201248168945, Loss D: 0.008389394730329514\n",
      "Epoch 135/200, Batch 9/17, Loss G: 3.9960246086120605, Loss D: 0.010161375626921654\n",
      "Epoch 135/200, Batch 10/17, Loss G: 3.9204812049865723, Loss D: 0.01435037236660719\n",
      "Epoch 135/200, Batch 11/17, Loss G: 3.937295913696289, Loss D: 0.03037514165043831\n",
      "Epoch 135/200, Batch 12/17, Loss G: 4.196399688720703, Loss D: 0.015287571586668491\n",
      "Epoch 135/200, Batch 13/17, Loss G: 3.9762415885925293, Loss D: 0.023424800485372543\n",
      "Epoch 135/200, Batch 14/17, Loss G: 3.9200994968414307, Loss D: 0.019270744174718857\n",
      "Epoch 135/200, Batch 15/17, Loss G: 3.903092384338379, Loss D: 0.014129819348454475\n",
      "Epoch 135/200, Batch 16/17, Loss G: 4.093927383422852, Loss D: 0.0037407665513455868\n",
      "Epoch 136/200, Batch 0/17, Loss G: 4.22837495803833, Loss D: 0.012251942418515682\n",
      "Epoch 136/200, Batch 1/17, Loss G: 4.103777885437012, Loss D: 0.009101303294301033\n",
      "Epoch 136/200, Batch 2/17, Loss G: 4.01934814453125, Loss D: 0.018407201394438744\n",
      "Epoch 136/200, Batch 3/17, Loss G: 4.124370574951172, Loss D: 0.02151748724281788\n",
      "Epoch 136/200, Batch 4/17, Loss G: 3.819666862487793, Loss D: 0.018406063318252563\n",
      "Epoch 136/200, Batch 5/17, Loss G: 4.010949611663818, Loss D: 0.010799062438309193\n",
      "Epoch 136/200, Batch 6/17, Loss G: 3.9978325366973877, Loss D: 0.010225687175989151\n",
      "Epoch 136/200, Batch 7/17, Loss G: 3.8894495964050293, Loss D: 0.012094985693693161\n",
      "Epoch 136/200, Batch 8/17, Loss G: 3.7531707286834717, Loss D: 0.011455485597252846\n",
      "Epoch 136/200, Batch 9/17, Loss G: 3.8090662956237793, Loss D: 0.018958278000354767\n",
      "Epoch 136/200, Batch 10/17, Loss G: 3.781688690185547, Loss D: 0.009757310152053833\n",
      "Epoch 136/200, Batch 11/17, Loss G: 3.7698535919189453, Loss D: 0.02072080783545971\n",
      "Epoch 136/200, Batch 12/17, Loss G: 4.280439376831055, Loss D: 0.08072543889284134\n",
      "Epoch 136/200, Batch 13/17, Loss G: 3.321312427520752, Loss D: 0.18022161722183228\n",
      "Epoch 136/200, Batch 14/17, Loss G: 4.1039910316467285, Loss D: 0.18651673197746277\n",
      "Epoch 136/200, Batch 15/17, Loss G: 3.749239444732666, Loss D: 0.07237014919519424\n",
      "Epoch 136/200, Batch 16/17, Loss G: 3.7029857635498047, Loss D: 0.08533267676830292\n",
      "Epoch 137/200, Batch 0/17, Loss G: 4.0699052810668945, Loss D: 0.2938849627971649\n",
      "Epoch 137/200, Batch 1/17, Loss G: 3.689479351043701, Loss D: 0.17159348726272583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200, Batch 2/17, Loss G: 3.7951111793518066, Loss D: 0.11131750047206879\n",
      "Epoch 137/200, Batch 3/17, Loss G: 4.107514381408691, Loss D: 0.17208774387836456\n",
      "Epoch 137/200, Batch 4/17, Loss G: 3.499152660369873, Loss D: 0.08634882420301437\n",
      "Epoch 137/200, Batch 5/17, Loss G: 4.075031757354736, Loss D: 0.04295479133725166\n",
      "Epoch 137/200, Batch 6/17, Loss G: 3.754485845565796, Loss D: 0.04125373438000679\n",
      "Epoch 137/200, Batch 7/17, Loss G: 3.900939464569092, Loss D: 0.05175682529807091\n",
      "Epoch 137/200, Batch 8/17, Loss G: 4.212841987609863, Loss D: 0.04329633340239525\n",
      "Epoch 137/200, Batch 9/17, Loss G: 3.5831120014190674, Loss D: 0.08066029101610184\n",
      "Epoch 137/200, Batch 10/17, Loss G: 4.015236854553223, Loss D: 0.08700229972600937\n",
      "Epoch 137/200, Batch 11/17, Loss G: 3.7324652671813965, Loss D: 0.08379662781953812\n",
      "Epoch 137/200, Batch 12/17, Loss G: 4.058523654937744, Loss D: 0.028219569474458694\n",
      "Epoch 137/200, Batch 13/17, Loss G: 3.9356508255004883, Loss D: 0.057670529931783676\n",
      "Epoch 137/200, Batch 14/17, Loss G: 3.75616455078125, Loss D: 0.051489926874637604\n",
      "Epoch 137/200, Batch 15/17, Loss G: 3.8718438148498535, Loss D: 0.05259527638554573\n",
      "Epoch 137/200, Batch 16/17, Loss G: 3.9564528465270996, Loss D: 0.043553318828344345\n",
      "Epoch 138/200, Batch 0/17, Loss G: 3.9011597633361816, Loss D: 0.050470903515815735\n",
      "Epoch 138/200, Batch 1/17, Loss G: 3.8012256622314453, Loss D: 0.021073954179883003\n",
      "Epoch 138/200, Batch 2/17, Loss G: 3.743969440460205, Loss D: 0.02268870547413826\n",
      "Epoch 138/200, Batch 3/17, Loss G: 3.9350452423095703, Loss D: 0.025951575487852097\n",
      "Epoch 138/200, Batch 4/17, Loss G: 3.8514604568481445, Loss D: 0.02099820412695408\n",
      "Epoch 138/200, Batch 5/17, Loss G: 3.804464340209961, Loss D: 0.024475611746311188\n",
      "Epoch 138/200, Batch 6/17, Loss G: 3.7164244651794434, Loss D: 0.02762470580637455\n",
      "Epoch 138/200, Batch 7/17, Loss G: 3.680844306945801, Loss D: 0.06229947879910469\n",
      "Epoch 138/200, Batch 8/17, Loss G: 4.167999267578125, Loss D: 0.04442795366048813\n",
      "Epoch 138/200, Batch 9/17, Loss G: 4.01157283782959, Loss D: 0.023686328902840614\n",
      "Epoch 138/200, Batch 10/17, Loss G: 3.6920642852783203, Loss D: 0.048436541110277176\n",
      "Epoch 138/200, Batch 11/17, Loss G: 4.109433174133301, Loss D: 0.04597270116209984\n",
      "Epoch 138/200, Batch 12/17, Loss G: 3.79134202003479, Loss D: 0.03839118033647537\n",
      "Epoch 138/200, Batch 13/17, Loss G: 3.943974018096924, Loss D: 0.023603199049830437\n",
      "Epoch 138/200, Batch 14/17, Loss G: 4.019641876220703, Loss D: 0.0212382972240448\n",
      "Epoch 138/200, Batch 15/17, Loss G: 4.008735179901123, Loss D: 0.014422062784433365\n",
      "Epoch 138/200, Batch 16/17, Loss G: 4.258863925933838, Loss D: 0.019029518589377403\n",
      "Epoch 139/200, Batch 0/17, Loss G: 3.987377405166626, Loss D: 0.027472814545035362\n",
      "Epoch 139/200, Batch 1/17, Loss G: 3.9410452842712402, Loss D: 0.01634971983730793\n",
      "Epoch 139/200, Batch 2/17, Loss G: 3.920480489730835, Loss D: 0.014014310203492641\n",
      "Epoch 139/200, Batch 3/17, Loss G: 3.7312707901000977, Loss D: 0.015099097043275833\n",
      "Epoch 139/200, Batch 4/17, Loss G: 4.10648250579834, Loss D: 0.010146629065275192\n",
      "Epoch 139/200, Batch 5/17, Loss G: 3.9167423248291016, Loss D: 0.010218384675681591\n",
      "Epoch 139/200, Batch 6/17, Loss G: 3.986553907394409, Loss D: 0.016678210347890854\n",
      "Epoch 139/200, Batch 7/17, Loss G: 3.8673810958862305, Loss D: 0.012842883355915546\n",
      "Epoch 139/200, Batch 8/17, Loss G: 4.0000810623168945, Loss D: 0.012946106493473053\n",
      "Epoch 139/200, Batch 9/17, Loss G: 3.9584879875183105, Loss D: 0.013958174735307693\n",
      "Epoch 139/200, Batch 10/17, Loss G: 4.086906433105469, Loss D: 0.0150301493704319\n",
      "Epoch 139/200, Batch 11/17, Loss G: 3.9713635444641113, Loss D: 0.013648134656250477\n",
      "Epoch 139/200, Batch 12/17, Loss G: 4.110696792602539, Loss D: 0.006098324898630381\n",
      "Epoch 139/200, Batch 13/17, Loss G: 3.921234130859375, Loss D: 0.018945783376693726\n",
      "Epoch 139/200, Batch 14/17, Loss G: 4.109145164489746, Loss D: 0.015584107488393784\n",
      "Epoch 139/200, Batch 15/17, Loss G: 3.815061330795288, Loss D: 0.018076295033097267\n",
      "Epoch 139/200, Batch 16/17, Loss G: 4.048440933227539, Loss D: 0.013070141896605492\n",
      "Epoch 140/200, Batch 0/17, Loss G: 4.081092357635498, Loss D: 0.008870178833603859\n",
      "Epoch 140/200, Batch 1/17, Loss G: 4.053168296813965, Loss D: 0.01419655978679657\n",
      "Epoch 140/200, Batch 2/17, Loss G: 4.002971649169922, Loss D: 0.015739833936095238\n",
      "Epoch 140/200, Batch 3/17, Loss G: 3.963988780975342, Loss D: 0.012749062851071358\n",
      "Epoch 140/200, Batch 4/17, Loss G: 4.028326034545898, Loss D: 0.018947860226035118\n",
      "Epoch 140/200, Batch 5/17, Loss G: 4.075547218322754, Loss D: 0.007092840503901243\n",
      "Epoch 140/200, Batch 6/17, Loss G: 4.004883766174316, Loss D: 0.01164222601801157\n",
      "Epoch 140/200, Batch 7/17, Loss G: 3.795172929763794, Loss D: 0.022963080555200577\n",
      "Epoch 140/200, Batch 8/17, Loss G: 3.5700225830078125, Loss D: 0.0469270683825016\n",
      "Epoch 140/200, Batch 9/17, Loss G: 3.9236650466918945, Loss D: 0.04723097011446953\n",
      "Epoch 140/200, Batch 10/17, Loss G: 3.934478998184204, Loss D: 0.01970074325799942\n",
      "Epoch 140/200, Batch 11/17, Loss G: 3.815002918243408, Loss D: 0.021345358341932297\n",
      "Epoch 140/200, Batch 12/17, Loss G: 3.884195327758789, Loss D: 0.00951143354177475\n",
      "Epoch 140/200, Batch 13/17, Loss G: 3.8955636024475098, Loss D: 0.014162637293338776\n",
      "Epoch 140/200, Batch 14/17, Loss G: 3.987112045288086, Loss D: 0.00852247141301632\n",
      "Epoch 140/200, Batch 15/17, Loss G: 3.832512855529785, Loss D: 0.012095113284885883\n",
      "Epoch 140/200, Batch 16/17, Loss G: 4.307280540466309, Loss D: 0.0097560565918684\n",
      "Epoch 141/200, Batch 0/17, Loss G: 3.9878640174865723, Loss D: 0.01874731481075287\n",
      "Epoch 141/200, Batch 1/17, Loss G: 4.072378158569336, Loss D: 0.013566249050199986\n",
      "Epoch 141/200, Batch 2/17, Loss G: 3.963733673095703, Loss D: 0.011852556839585304\n",
      "Epoch 141/200, Batch 3/17, Loss G: 3.9301657676696777, Loss D: 0.015554416924715042\n",
      "Epoch 141/200, Batch 4/17, Loss G: 4.039707183837891, Loss D: 0.006673519965261221\n",
      "Epoch 141/200, Batch 5/17, Loss G: 4.036736011505127, Loss D: 0.012700963765382767\n",
      "Epoch 141/200, Batch 6/17, Loss G: 3.9057881832122803, Loss D: 0.0123518705368042\n",
      "Epoch 141/200, Batch 7/17, Loss G: 3.9812870025634766, Loss D: 0.009702338837087154\n",
      "Epoch 141/200, Batch 8/17, Loss G: 3.9884250164031982, Loss D: 0.009051545523107052\n",
      "Epoch 141/200, Batch 9/17, Loss G: 3.911261796951294, Loss D: 0.008842459879815578\n",
      "Epoch 141/200, Batch 10/17, Loss G: 3.89939546585083, Loss D: 0.013110574334859848\n",
      "Epoch 141/200, Batch 11/17, Loss G: 3.823755979537964, Loss D: 0.012986011803150177\n",
      "Epoch 141/200, Batch 12/17, Loss G: 3.95766019821167, Loss D: 0.006929044146090746\n",
      "Epoch 141/200, Batch 13/17, Loss G: 3.95424222946167, Loss D: 0.015276925638318062\n",
      "Epoch 141/200, Batch 14/17, Loss G: 3.7651827335357666, Loss D: 0.018941311165690422\n",
      "Epoch 141/200, Batch 15/17, Loss G: 3.901444435119629, Loss D: 0.006059229839593172\n",
      "Epoch 141/200, Batch 16/17, Loss G: 4.019591331481934, Loss D: 0.010774563066661358\n",
      "Epoch 142/200, Batch 0/17, Loss G: 3.950286865234375, Loss D: 0.008247187361121178\n",
      "Epoch 142/200, Batch 1/17, Loss G: 4.0651960372924805, Loss D: 0.009957079775631428\n",
      "Epoch 142/200, Batch 2/17, Loss G: 4.100503921508789, Loss D: 0.008134957402944565\n",
      "Epoch 142/200, Batch 3/17, Loss G: 3.88966965675354, Loss D: 0.007453250698745251\n",
      "Epoch 142/200, Batch 4/17, Loss G: 4.012986183166504, Loss D: 0.01454160362482071\n",
      "Epoch 142/200, Batch 5/17, Loss G: 3.9770426750183105, Loss D: 0.010476524941623211\n",
      "Epoch 142/200, Batch 6/17, Loss G: 4.082359313964844, Loss D: 0.009614466689527035\n",
      "Epoch 142/200, Batch 7/17, Loss G: 3.9519848823547363, Loss D: 0.009189864620566368\n",
      "Epoch 142/200, Batch 8/17, Loss G: 3.8612823486328125, Loss D: 0.011996246874332428\n",
      "Epoch 142/200, Batch 9/17, Loss G: 3.959660053253174, Loss D: 0.00832926295697689\n",
      "Epoch 142/200, Batch 10/17, Loss G: 3.8736233711242676, Loss D: 0.012820041738450527\n",
      "Epoch 142/200, Batch 11/17, Loss G: 3.85794734954834, Loss D: 0.009265944361686707\n",
      "Epoch 142/200, Batch 12/17, Loss G: 3.867476463317871, Loss D: 0.008256873115897179\n",
      "Epoch 142/200, Batch 13/17, Loss G: 3.701702117919922, Loss D: 0.01881948672235012\n",
      "Epoch 142/200, Batch 14/17, Loss G: 3.984783887863159, Loss D: 0.026312487199902534\n",
      "Epoch 142/200, Batch 15/17, Loss G: 3.879765033721924, Loss D: 0.01406667660921812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200, Batch 16/17, Loss G: 3.934162139892578, Loss D: 0.0165284164249897\n",
      "Epoch 143/200, Batch 0/17, Loss G: 3.8876278400421143, Loss D: 0.008569917641580105\n",
      "Epoch 143/200, Batch 1/17, Loss G: 3.882500410079956, Loss D: 0.007419932633638382\n",
      "Epoch 143/200, Batch 2/17, Loss G: 3.7373600006103516, Loss D: 0.019248807802796364\n",
      "Epoch 143/200, Batch 3/17, Loss G: 4.126277923583984, Loss D: 0.011452613398432732\n",
      "Epoch 143/200, Batch 4/17, Loss G: 4.013216018676758, Loss D: 0.008807375095784664\n",
      "Epoch 143/200, Batch 5/17, Loss G: 4.014180660247803, Loss D: 0.011013612151145935\n",
      "Epoch 143/200, Batch 6/17, Loss G: 3.9713010787963867, Loss D: 0.007312513887882233\n",
      "Epoch 143/200, Batch 7/17, Loss G: 3.9557204246520996, Loss D: 0.007019825279712677\n",
      "Epoch 143/200, Batch 8/17, Loss G: 4.051868438720703, Loss D: 0.0072249723598361015\n",
      "Epoch 143/200, Batch 9/17, Loss G: 3.84867000579834, Loss D: 0.012073939666152\n",
      "Epoch 143/200, Batch 10/17, Loss G: 3.987093925476074, Loss D: 0.016531266272068024\n",
      "Epoch 143/200, Batch 11/17, Loss G: 4.038900375366211, Loss D: 0.010958493687212467\n",
      "Epoch 143/200, Batch 12/17, Loss G: 4.01865816116333, Loss D: 0.007358334492892027\n",
      "Epoch 143/200, Batch 13/17, Loss G: 3.853562831878662, Loss D: 0.011496856808662415\n",
      "Epoch 143/200, Batch 14/17, Loss G: 4.008716583251953, Loss D: 0.012433042749762535\n",
      "Epoch 143/200, Batch 15/17, Loss G: 3.8307716846466064, Loss D: 0.014826594851911068\n",
      "Epoch 143/200, Batch 16/17, Loss G: 3.647035598754883, Loss D: 0.019631311297416687\n",
      "Epoch 144/200, Batch 0/17, Loss G: 4.125097751617432, Loss D: 0.011270474642515182\n",
      "Epoch 144/200, Batch 1/17, Loss G: 4.024544715881348, Loss D: 0.007436288986355066\n",
      "Epoch 144/200, Batch 2/17, Loss G: 3.7866158485412598, Loss D: 0.007291526533663273\n",
      "Epoch 144/200, Batch 3/17, Loss G: 4.0173468589782715, Loss D: 0.006276359315961599\n",
      "Epoch 144/200, Batch 4/17, Loss G: 3.874565601348877, Loss D: 0.014806602150201797\n",
      "Epoch 144/200, Batch 5/17, Loss G: 3.782378911972046, Loss D: 0.005869651213288307\n",
      "Epoch 144/200, Batch 6/17, Loss G: 3.8511743545532227, Loss D: 0.014771375805139542\n",
      "Epoch 144/200, Batch 7/17, Loss G: 3.9384071826934814, Loss D: 0.018071044236421585\n",
      "Epoch 144/200, Batch 8/17, Loss G: 3.9242329597473145, Loss D: 0.016543664038181305\n",
      "Epoch 144/200, Batch 9/17, Loss G: 3.9088077545166016, Loss D: 0.016116177663207054\n",
      "Epoch 144/200, Batch 10/17, Loss G: 4.139350891113281, Loss D: 0.02257055975496769\n",
      "Epoch 144/200, Batch 11/17, Loss G: 3.7961764335632324, Loss D: 0.017870590090751648\n",
      "Epoch 144/200, Batch 12/17, Loss G: 4.324980735778809, Loss D: 0.014407021924853325\n",
      "Epoch 144/200, Batch 13/17, Loss G: 3.831263542175293, Loss D: 0.01885669305920601\n",
      "Epoch 144/200, Batch 14/17, Loss G: 3.983335256576538, Loss D: 0.007950208149850368\n",
      "Epoch 144/200, Batch 15/17, Loss G: 3.858297348022461, Loss D: 0.00717420456930995\n",
      "Epoch 144/200, Batch 16/17, Loss G: 3.9531662464141846, Loss D: 0.017974495887756348\n",
      "Epoch 145/200, Batch 0/17, Loss G: 3.6094095706939697, Loss D: 0.06781165301799774\n",
      "Epoch 145/200, Batch 1/17, Loss G: 4.043758392333984, Loss D: 0.04247855767607689\n",
      "Epoch 145/200, Batch 2/17, Loss G: 4.097729206085205, Loss D: 0.008426208980381489\n",
      "Epoch 145/200, Batch 3/17, Loss G: 3.60076904296875, Loss D: 0.09539084136486053\n",
      "Epoch 145/200, Batch 4/17, Loss G: 4.163414001464844, Loss D: 0.25306349992752075\n",
      "Epoch 145/200, Batch 5/17, Loss G: 3.5478415489196777, Loss D: 0.11546671390533447\n",
      "Epoch 145/200, Batch 6/17, Loss G: 3.6451072692871094, Loss D: 0.12451346963644028\n",
      "Epoch 145/200, Batch 7/17, Loss G: 3.7000176906585693, Loss D: 0.08647327870130539\n",
      "Epoch 145/200, Batch 8/17, Loss G: 3.599853277206421, Loss D: 0.3349626064300537\n",
      "Epoch 145/200, Batch 9/17, Loss G: 3.7044734954833984, Loss D: 0.19548195600509644\n",
      "Epoch 145/200, Batch 10/17, Loss G: 3.4831748008728027, Loss D: 0.2354639321565628\n",
      "Epoch 145/200, Batch 11/17, Loss G: 3.996453285217285, Loss D: 0.21220694482326508\n",
      "Epoch 145/200, Batch 12/17, Loss G: 3.700226068496704, Loss D: 0.11111189424991608\n",
      "Epoch 145/200, Batch 13/17, Loss G: 3.726083755493164, Loss D: 0.09402637928724289\n",
      "Epoch 145/200, Batch 14/17, Loss G: 4.168718338012695, Loss D: 0.0501386821269989\n",
      "Epoch 145/200, Batch 15/17, Loss G: 3.6689958572387695, Loss D: 0.08936399221420288\n",
      "Epoch 145/200, Batch 16/17, Loss G: 4.0017852783203125, Loss D: 0.2020677924156189\n",
      "Epoch 146/200, Batch 0/17, Loss G: 3.3283157348632812, Loss D: 0.2527744174003601\n",
      "Epoch 146/200, Batch 1/17, Loss G: 3.9264466762542725, Loss D: 0.06786046922206879\n",
      "Epoch 146/200, Batch 2/17, Loss G: 4.158985614776611, Loss D: 0.05042387172579765\n",
      "Epoch 146/200, Batch 3/17, Loss G: 3.804410934448242, Loss D: 0.05197151005268097\n",
      "Epoch 146/200, Batch 4/17, Loss G: 3.707653522491455, Loss D: 0.059727590531110764\n",
      "Epoch 146/200, Batch 5/17, Loss G: 3.9838757514953613, Loss D: 0.09221164137125015\n",
      "Epoch 146/200, Batch 6/17, Loss G: 3.5297799110412598, Loss D: 0.0680948793888092\n",
      "Epoch 146/200, Batch 7/17, Loss G: 3.803175449371338, Loss D: 0.04053439199924469\n",
      "Epoch 146/200, Batch 8/17, Loss G: 4.1450018882751465, Loss D: 0.12572136521339417\n",
      "Epoch 146/200, Batch 9/17, Loss G: 3.23152494430542, Loss D: 0.2080589234828949\n",
      "Epoch 146/200, Batch 10/17, Loss G: 3.9223105907440186, Loss D: 0.04069723188877106\n",
      "Epoch 146/200, Batch 11/17, Loss G: 3.964564561843872, Loss D: 0.055874768644571304\n",
      "Epoch 146/200, Batch 12/17, Loss G: 3.59847354888916, Loss D: 0.0520988404750824\n",
      "Epoch 146/200, Batch 13/17, Loss G: 3.776162624359131, Loss D: 0.04587506130337715\n",
      "Epoch 146/200, Batch 14/17, Loss G: 3.757098913192749, Loss D: 0.0393662229180336\n",
      "Epoch 146/200, Batch 15/17, Loss G: 3.6560544967651367, Loss D: 0.038322314620018005\n",
      "Epoch 146/200, Batch 16/17, Loss G: 4.060171604156494, Loss D: 0.0803612619638443\n",
      "Epoch 147/200, Batch 0/17, Loss G: 3.551264762878418, Loss D: 0.11355079710483551\n",
      "Epoch 147/200, Batch 1/17, Loss G: 4.072369575500488, Loss D: 0.04451340436935425\n",
      "Epoch 147/200, Batch 2/17, Loss G: 3.922868490219116, Loss D: 0.024693787097930908\n",
      "Epoch 147/200, Batch 3/17, Loss G: 3.723651647567749, Loss D: 0.036569349467754364\n",
      "Epoch 147/200, Batch 4/17, Loss G: 3.6458005905151367, Loss D: 0.05744541063904762\n",
      "Epoch 147/200, Batch 5/17, Loss G: 3.9754371643066406, Loss D: 0.03198782354593277\n",
      "Epoch 147/200, Batch 6/17, Loss G: 3.8226871490478516, Loss D: 0.018892519176006317\n",
      "Epoch 147/200, Batch 7/17, Loss G: 3.854249954223633, Loss D: 0.0266653411090374\n",
      "Epoch 147/200, Batch 8/17, Loss G: 3.8960015773773193, Loss D: 0.024125145748257637\n",
      "Epoch 147/200, Batch 9/17, Loss G: 3.8234598636627197, Loss D: 0.02646789886057377\n",
      "Epoch 147/200, Batch 10/17, Loss G: 3.8540549278259277, Loss D: 0.016464492306113243\n",
      "Epoch 147/200, Batch 11/17, Loss G: 3.834782123565674, Loss D: 0.017624272033572197\n",
      "Epoch 147/200, Batch 12/17, Loss G: 3.8701822757720947, Loss D: 0.02553432248532772\n",
      "Epoch 147/200, Batch 13/17, Loss G: 4.074028015136719, Loss D: 0.030727490782737732\n",
      "Epoch 147/200, Batch 14/17, Loss G: 3.724233627319336, Loss D: 0.03962397202849388\n",
      "Epoch 147/200, Batch 15/17, Loss G: 3.9955573081970215, Loss D: 0.013641232624650002\n",
      "Epoch 147/200, Batch 16/17, Loss G: 4.238777160644531, Loss D: 0.02153414860367775\n",
      "Epoch 148/200, Batch 0/17, Loss G: 3.955477237701416, Loss D: 0.015157314017415047\n",
      "Epoch 148/200, Batch 1/17, Loss G: 3.978816270828247, Loss D: 0.013075262308120728\n",
      "Epoch 148/200, Batch 2/17, Loss G: 3.9322197437286377, Loss D: 0.020289480686187744\n",
      "Epoch 148/200, Batch 3/17, Loss G: 3.7119665145874023, Loss D: 0.01708592101931572\n",
      "Epoch 148/200, Batch 4/17, Loss G: 4.118801593780518, Loss D: 0.013299115002155304\n",
      "Epoch 148/200, Batch 5/17, Loss G: 3.9228744506835938, Loss D: 0.015135244466364384\n",
      "Epoch 148/200, Batch 6/17, Loss G: 3.769804000854492, Loss D: 0.01396750845015049\n",
      "Epoch 148/200, Batch 7/17, Loss G: 4.003398895263672, Loss D: 0.013440901413559914\n",
      "Epoch 148/200, Batch 8/17, Loss G: 3.854897975921631, Loss D: 0.00850637722760439\n",
      "Epoch 148/200, Batch 9/17, Loss G: 4.049258232116699, Loss D: 0.010599376633763313\n",
      "Epoch 148/200, Batch 10/17, Loss G: 3.8879036903381348, Loss D: 0.013482904061675072\n",
      "Epoch 148/200, Batch 11/17, Loss G: 3.8559060096740723, Loss D: 0.020567825064063072\n",
      "Epoch 148/200, Batch 12/17, Loss G: 3.896986484527588, Loss D: 0.012895218096673489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200, Batch 13/17, Loss G: 3.890042543411255, Loss D: 0.012236231006681919\n",
      "Epoch 148/200, Batch 14/17, Loss G: 3.601270914077759, Loss D: 0.01928265206515789\n",
      "Epoch 148/200, Batch 15/17, Loss G: 4.030170917510986, Loss D: 0.007584432139992714\n",
      "Epoch 148/200, Batch 16/17, Loss G: 4.149738311767578, Loss D: 0.012553387321531773\n",
      "Epoch 149/200, Batch 0/17, Loss G: 3.9474120140075684, Loss D: 0.009535383433103561\n",
      "Epoch 149/200, Batch 1/17, Loss G: 3.950575828552246, Loss D: 0.00697513809427619\n",
      "Epoch 149/200, Batch 2/17, Loss G: 3.9592056274414062, Loss D: 0.012413004413247108\n",
      "Epoch 149/200, Batch 3/17, Loss G: 4.011646747589111, Loss D: 0.006482214666903019\n",
      "Epoch 149/200, Batch 4/17, Loss G: 3.856825828552246, Loss D: 0.016274049878120422\n",
      "Epoch 149/200, Batch 5/17, Loss G: 3.9518685340881348, Loss D: 0.013604512438178062\n",
      "Epoch 149/200, Batch 6/17, Loss G: 3.6994810104370117, Loss D: 0.013436960987746716\n",
      "Epoch 149/200, Batch 7/17, Loss G: 3.96469783782959, Loss D: 0.005769216455519199\n",
      "Epoch 149/200, Batch 8/17, Loss G: 3.770869255065918, Loss D: 0.015799295157194138\n",
      "Epoch 149/200, Batch 9/17, Loss G: 3.8496146202087402, Loss D: 0.0174370389431715\n",
      "Epoch 149/200, Batch 10/17, Loss G: 3.8186211585998535, Loss D: 0.010969742201268673\n",
      "Epoch 149/200, Batch 11/17, Loss G: 3.836198329925537, Loss D: 0.02213628962635994\n",
      "Epoch 149/200, Batch 12/17, Loss G: 3.7073135375976562, Loss D: 0.03393412008881569\n",
      "Epoch 149/200, Batch 13/17, Loss G: 4.057789325714111, Loss D: 0.00839901901781559\n",
      "Epoch 149/200, Batch 14/17, Loss G: 3.9366774559020996, Loss D: 0.02144843339920044\n",
      "Epoch 149/200, Batch 15/17, Loss G: 3.7229387760162354, Loss D: 0.0262239258736372\n",
      "Epoch 149/200, Batch 16/17, Loss G: 4.103655815124512, Loss D: 0.026830848306417465\n",
      "Epoch 150/200, Batch 0/17, Loss G: 3.591296672821045, Loss D: 0.04030276834964752\n",
      "Epoch 150/200, Batch 1/17, Loss G: 3.9265034198760986, Loss D: 0.02616281807422638\n",
      "Epoch 150/200, Batch 2/17, Loss G: 3.7539138793945312, Loss D: 0.019941071048378944\n",
      "Epoch 150/200, Batch 3/17, Loss G: 3.8796133995056152, Loss D: 0.013516702689230442\n",
      "Epoch 150/200, Batch 4/17, Loss G: 4.050771713256836, Loss D: 0.014626950025558472\n",
      "Epoch 150/200, Batch 5/17, Loss G: 3.8862669467926025, Loss D: 0.01211164053529501\n",
      "Epoch 150/200, Batch 6/17, Loss G: 3.747864007949829, Loss D: 0.0266460832208395\n",
      "Epoch 150/200, Batch 7/17, Loss G: 3.9286375045776367, Loss D: 0.013802886009216309\n",
      "Epoch 150/200, Batch 8/17, Loss G: 3.8538403511047363, Loss D: 0.014434386044740677\n",
      "Epoch 150/200, Batch 9/17, Loss G: 3.824303150177002, Loss D: 0.012470809742808342\n",
      "Epoch 150/200, Batch 10/17, Loss G: 3.949559211730957, Loss D: 0.011618922464549541\n",
      "Epoch 150/200, Batch 11/17, Loss G: 3.8707361221313477, Loss D: 0.01595216430723667\n",
      "Epoch 150/200, Batch 12/17, Loss G: 3.708289623260498, Loss D: 0.01668410375714302\n",
      "Epoch 150/200, Batch 13/17, Loss G: 3.999130964279175, Loss D: 0.010185275226831436\n",
      "Epoch 150/200, Batch 14/17, Loss G: 3.895139217376709, Loss D: 0.01181214489042759\n",
      "Epoch 150/200, Batch 15/17, Loss G: 3.9400248527526855, Loss D: 0.014880936592817307\n",
      "Epoch 150/200, Batch 16/17, Loss G: 3.7974720001220703, Loss D: 0.013341596350073814\n",
      "Epoch 151/200, Batch 0/17, Loss G: 3.7998929023742676, Loss D: 0.0060799261555075645\n",
      "Epoch 151/200, Batch 1/17, Loss G: 4.024416923522949, Loss D: 0.008815789595246315\n",
      "Epoch 151/200, Batch 2/17, Loss G: 3.8250138759613037, Loss D: 0.014067534357309341\n",
      "Epoch 151/200, Batch 3/17, Loss G: 3.886373996734619, Loss D: 0.007657831534743309\n",
      "Epoch 151/200, Batch 4/17, Loss G: 3.837653160095215, Loss D: 0.022964179515838623\n",
      "Epoch 151/200, Batch 5/17, Loss G: 4.143886089324951, Loss D: 0.005457504186779261\n",
      "Epoch 151/200, Batch 6/17, Loss G: 3.83366060256958, Loss D: 0.011146006174385548\n",
      "Epoch 151/200, Batch 7/17, Loss G: 3.810357093811035, Loss D: 0.012764891609549522\n",
      "Epoch 151/200, Batch 8/17, Loss G: 3.9850454330444336, Loss D: 0.010726213455200195\n",
      "Epoch 151/200, Batch 9/17, Loss G: 3.915278673171997, Loss D: 0.027053995057940483\n",
      "Epoch 151/200, Batch 10/17, Loss G: 3.7665672302246094, Loss D: 0.016884807497262955\n",
      "Epoch 151/200, Batch 11/17, Loss G: 3.819748878479004, Loss D: 0.01834820583462715\n",
      "Epoch 151/200, Batch 12/17, Loss G: 4.12047815322876, Loss D: 0.007546280510723591\n",
      "Epoch 151/200, Batch 13/17, Loss G: 4.043358325958252, Loss D: 0.015522357076406479\n",
      "Epoch 151/200, Batch 14/17, Loss G: 3.8209714889526367, Loss D: 0.008247824385762215\n",
      "Epoch 151/200, Batch 15/17, Loss G: 3.719841480255127, Loss D: 0.01905437558889389\n",
      "Epoch 151/200, Batch 16/17, Loss G: 3.922581672668457, Loss D: 0.012522979639470577\n",
      "Epoch 152/200, Batch 0/17, Loss G: 3.841003894805908, Loss D: 0.01220572181046009\n",
      "Epoch 152/200, Batch 1/17, Loss G: 3.8621315956115723, Loss D: 0.0122897420078516\n",
      "Epoch 152/200, Batch 2/17, Loss G: 3.7901978492736816, Loss D: 0.00823427364230156\n",
      "Epoch 152/200, Batch 3/17, Loss G: 3.8271045684814453, Loss D: 0.01148168183863163\n",
      "Epoch 152/200, Batch 4/17, Loss G: 4.176758766174316, Loss D: 0.018497450277209282\n",
      "Epoch 152/200, Batch 5/17, Loss G: 3.8376407623291016, Loss D: 0.02677188068628311\n",
      "Epoch 152/200, Batch 6/17, Loss G: 3.9889934062957764, Loss D: 0.03152752295136452\n",
      "Epoch 152/200, Batch 7/17, Loss G: 3.954861640930176, Loss D: 0.011686242185533047\n",
      "Epoch 152/200, Batch 8/17, Loss G: 3.8532450199127197, Loss D: 0.02815381996333599\n",
      "Epoch 152/200, Batch 9/17, Loss G: 4.327712535858154, Loss D: 0.024364279583096504\n",
      "Epoch 152/200, Batch 10/17, Loss G: 4.039229393005371, Loss D: 0.0077062323689460754\n",
      "Epoch 152/200, Batch 11/17, Loss G: 3.9822802543640137, Loss D: 0.006693778093904257\n",
      "Epoch 152/200, Batch 12/17, Loss G: 3.638293743133545, Loss D: 0.03162568062543869\n",
      "Epoch 152/200, Batch 13/17, Loss G: 3.8863916397094727, Loss D: 0.041653845459222794\n",
      "Epoch 152/200, Batch 14/17, Loss G: 3.795151472091675, Loss D: 0.020359201356768608\n",
      "Epoch 152/200, Batch 15/17, Loss G: 3.8117482662200928, Loss D: 0.00894582737237215\n",
      "Epoch 152/200, Batch 16/17, Loss G: 3.87845516204834, Loss D: 0.012649842537939548\n",
      "Epoch 153/200, Batch 0/17, Loss G: 4.05651330947876, Loss D: 0.05178837105631828\n",
      "Epoch 153/200, Batch 1/17, Loss G: 3.465303421020508, Loss D: 0.08518170565366745\n",
      "Epoch 153/200, Batch 2/17, Loss G: 4.136280536651611, Loss D: 0.0639033168554306\n",
      "Epoch 153/200, Batch 3/17, Loss G: 3.986445903778076, Loss D: 0.0023006731644272804\n",
      "Epoch 153/200, Batch 4/17, Loss G: 3.6864187717437744, Loss D: 0.017892375588417053\n",
      "Epoch 153/200, Batch 5/17, Loss G: 3.8250346183776855, Loss D: 0.024820251390337944\n",
      "Epoch 153/200, Batch 6/17, Loss G: 3.82656192779541, Loss D: 0.1010621041059494\n",
      "Epoch 153/200, Batch 7/17, Loss G: 3.562530517578125, Loss D: 0.0908999815583229\n",
      "Epoch 153/200, Batch 8/17, Loss G: 3.98966121673584, Loss D: 0.020151548087596893\n",
      "Epoch 153/200, Batch 9/17, Loss G: 3.943681478500366, Loss D: 0.0905771404504776\n",
      "Epoch 153/200, Batch 10/17, Loss G: 3.1956253051757812, Loss D: 0.2575572729110718\n",
      "Epoch 153/200, Batch 11/17, Loss G: 3.7329020500183105, Loss D: 0.05486683174967766\n",
      "Epoch 153/200, Batch 12/17, Loss G: 4.1451311111450195, Loss D: 0.1753939837217331\n",
      "Epoch 153/200, Batch 13/17, Loss G: 3.137899160385132, Loss D: 0.27726858854293823\n",
      "Epoch 153/200, Batch 14/17, Loss G: 3.6535558700561523, Loss D: 0.06240329146385193\n",
      "Epoch 153/200, Batch 15/17, Loss G: 3.9430489540100098, Loss D: 0.12323732674121857\n",
      "Epoch 153/200, Batch 16/17, Loss G: 3.4026906490325928, Loss D: 0.13505159318447113\n",
      "Epoch 154/200, Batch 0/17, Loss G: 3.9516654014587402, Loss D: 0.05980054289102554\n",
      "Epoch 154/200, Batch 1/17, Loss G: 3.9947547912597656, Loss D: 0.04384954273700714\n",
      "Epoch 154/200, Batch 2/17, Loss G: 3.6464641094207764, Loss D: 0.03764398396015167\n",
      "Epoch 154/200, Batch 3/17, Loss G: 3.622676372528076, Loss D: 0.047854289412498474\n",
      "Epoch 154/200, Batch 4/17, Loss G: 3.8262295722961426, Loss D: 0.0865834504365921\n",
      "Epoch 154/200, Batch 5/17, Loss G: 3.466717481613159, Loss D: 0.07628362625837326\n",
      "Epoch 154/200, Batch 6/17, Loss G: 4.033942222595215, Loss D: 0.0248799379914999\n",
      "Epoch 154/200, Batch 7/17, Loss G: 3.7294492721557617, Loss D: 0.03803202882409096\n",
      "Epoch 154/200, Batch 8/17, Loss G: 3.7304751873016357, Loss D: 0.025389259681105614\n",
      "Epoch 154/200, Batch 9/17, Loss G: 3.8530073165893555, Loss D: 0.026576366275548935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200, Batch 10/17, Loss G: 3.84844970703125, Loss D: 0.026433028280735016\n",
      "Epoch 154/200, Batch 11/17, Loss G: 3.981482744216919, Loss D: 0.04319746792316437\n",
      "Epoch 154/200, Batch 12/17, Loss G: 3.9662866592407227, Loss D: 0.022227531298995018\n",
      "Epoch 154/200, Batch 13/17, Loss G: 3.879945993423462, Loss D: 0.026866357773542404\n",
      "Epoch 154/200, Batch 14/17, Loss G: 3.758328914642334, Loss D: 0.0431005135178566\n",
      "Epoch 154/200, Batch 15/17, Loss G: 3.623319625854492, Loss D: 0.045193109661340714\n",
      "Epoch 154/200, Batch 16/17, Loss G: 3.96418833732605, Loss D: 0.020005539059638977\n",
      "Epoch 155/200, Batch 0/17, Loss G: 4.148256301879883, Loss D: 0.04292304813861847\n",
      "Epoch 155/200, Batch 1/17, Loss G: 3.7291836738586426, Loss D: 0.028195399791002274\n",
      "Epoch 155/200, Batch 2/17, Loss G: 3.6868650913238525, Loss D: 0.013843839056789875\n",
      "Epoch 155/200, Batch 3/17, Loss G: 3.7854881286621094, Loss D: 0.01506204903125763\n",
      "Epoch 155/200, Batch 4/17, Loss G: 4.115087985992432, Loss D: 0.018582310527563095\n",
      "Epoch 155/200, Batch 5/17, Loss G: 3.8345422744750977, Loss D: 0.013645399361848831\n",
      "Epoch 155/200, Batch 6/17, Loss G: 3.8306350708007812, Loss D: 0.010869527235627174\n",
      "Epoch 155/200, Batch 7/17, Loss G: 3.8506524562835693, Loss D: 0.021524108946323395\n",
      "Epoch 155/200, Batch 8/17, Loss G: 3.9836010932922363, Loss D: 0.021578190848231316\n",
      "Epoch 155/200, Batch 9/17, Loss G: 3.8153109550476074, Loss D: 0.020698972046375275\n",
      "Epoch 155/200, Batch 10/17, Loss G: 3.9719443321228027, Loss D: 0.018735354766249657\n",
      "Epoch 155/200, Batch 11/17, Loss G: 3.6931943893432617, Loss D: 0.00862380675971508\n",
      "Epoch 155/200, Batch 12/17, Loss G: 3.6641592979431152, Loss D: 0.015252890065312386\n",
      "Epoch 155/200, Batch 13/17, Loss G: 4.0533528327941895, Loss D: 0.00981641560792923\n",
      "Epoch 155/200, Batch 14/17, Loss G: 3.8431124687194824, Loss D: 0.027760563418269157\n",
      "Epoch 155/200, Batch 15/17, Loss G: 3.8112003803253174, Loss D: 0.0400068499147892\n",
      "Epoch 155/200, Batch 16/17, Loss G: 4.181218147277832, Loss D: 0.008892934769392014\n",
      "Epoch 156/200, Batch 0/17, Loss G: 3.9897236824035645, Loss D: 0.011157851666212082\n",
      "Epoch 156/200, Batch 1/17, Loss G: 3.9143710136413574, Loss D: 0.008810057304799557\n",
      "Epoch 156/200, Batch 2/17, Loss G: 3.9055380821228027, Loss D: 0.01309410110116005\n",
      "Epoch 156/200, Batch 3/17, Loss G: 3.93117094039917, Loss D: 0.011571401730179787\n",
      "Epoch 156/200, Batch 4/17, Loss G: 3.9064395427703857, Loss D: 0.010152019560337067\n",
      "Epoch 156/200, Batch 5/17, Loss G: 3.9037177562713623, Loss D: 0.007385440170764923\n",
      "Epoch 156/200, Batch 6/17, Loss G: 3.8620171546936035, Loss D: 0.012231230735778809\n",
      "Epoch 156/200, Batch 7/17, Loss G: 3.8406410217285156, Loss D: 0.010489027015864849\n",
      "Epoch 156/200, Batch 8/17, Loss G: 3.867161273956299, Loss D: 0.009436743333935738\n",
      "Epoch 156/200, Batch 9/17, Loss G: 3.9222402572631836, Loss D: 0.009916839189827442\n",
      "Epoch 156/200, Batch 10/17, Loss G: 3.769847869873047, Loss D: 0.013631739653646946\n",
      "Epoch 156/200, Batch 11/17, Loss G: 3.729715347290039, Loss D: 0.01172479148954153\n",
      "Epoch 156/200, Batch 12/17, Loss G: 3.65958833694458, Loss D: 0.00891435518860817\n",
      "Epoch 156/200, Batch 13/17, Loss G: 3.7245585918426514, Loss D: 0.011136580258607864\n",
      "Epoch 156/200, Batch 14/17, Loss G: 3.916069984436035, Loss D: 0.009148167446255684\n",
      "Epoch 156/200, Batch 15/17, Loss G: 3.872657299041748, Loss D: 0.011220899410545826\n",
      "Epoch 156/200, Batch 16/17, Loss G: 3.9000959396362305, Loss D: 0.018789879977703094\n",
      "Epoch 157/200, Batch 0/17, Loss G: 3.8006415367126465, Loss D: 0.035615455359220505\n",
      "Epoch 157/200, Batch 1/17, Loss G: 3.8750219345092773, Loss D: 0.011257071048021317\n",
      "Epoch 157/200, Batch 2/17, Loss G: 3.9069714546203613, Loss D: 0.015400668606162071\n",
      "Epoch 157/200, Batch 3/17, Loss G: 3.922356128692627, Loss D: 0.01296958327293396\n",
      "Epoch 157/200, Batch 4/17, Loss G: 3.6812100410461426, Loss D: 0.007531307637691498\n",
      "Epoch 157/200, Batch 5/17, Loss G: 4.012355804443359, Loss D: 0.009383203461766243\n",
      "Epoch 157/200, Batch 6/17, Loss G: 3.7813045978546143, Loss D: 0.010463615879416466\n",
      "Epoch 157/200, Batch 7/17, Loss G: 3.867569923400879, Loss D: 0.014223126694560051\n",
      "Epoch 157/200, Batch 8/17, Loss G: 3.9900197982788086, Loss D: 0.020121414214372635\n",
      "Epoch 157/200, Batch 9/17, Loss G: 3.795281171798706, Loss D: 0.010346608236432076\n",
      "Epoch 157/200, Batch 10/17, Loss G: 3.746812343597412, Loss D: 0.014794377610087395\n",
      "Epoch 157/200, Batch 11/17, Loss G: 4.030355453491211, Loss D: 0.01628686860203743\n",
      "Epoch 157/200, Batch 12/17, Loss G: 3.80303692817688, Loss D: 0.013081464916467667\n",
      "Epoch 157/200, Batch 13/17, Loss G: 4.004635334014893, Loss D: 0.009086894802749157\n",
      "Epoch 157/200, Batch 14/17, Loss G: 3.7899017333984375, Loss D: 0.009900456294417381\n",
      "Epoch 157/200, Batch 15/17, Loss G: 3.7208411693573, Loss D: 0.009864578023552895\n",
      "Epoch 157/200, Batch 16/17, Loss G: 3.9507932662963867, Loss D: 0.00929152313619852\n",
      "Epoch 158/200, Batch 0/17, Loss G: 4.001381874084473, Loss D: 0.011269863694906235\n",
      "Epoch 158/200, Batch 1/17, Loss G: 3.936234474182129, Loss D: 0.006975814700126648\n",
      "Epoch 158/200, Batch 2/17, Loss G: 3.9078147411346436, Loss D: 0.007822065614163876\n",
      "Epoch 158/200, Batch 3/17, Loss G: 3.7597570419311523, Loss D: 0.010420003905892372\n",
      "Epoch 158/200, Batch 4/17, Loss G: 3.870569944381714, Loss D: 0.008961849845945835\n",
      "Epoch 158/200, Batch 5/17, Loss G: 3.825652599334717, Loss D: 0.006269540637731552\n",
      "Epoch 158/200, Batch 6/17, Loss G: 3.8927226066589355, Loss D: 0.0038990965113043785\n",
      "Epoch 158/200, Batch 7/17, Loss G: 3.752034902572632, Loss D: 0.006339334882795811\n",
      "Epoch 158/200, Batch 8/17, Loss G: 3.8420934677124023, Loss D: 0.011441396549344063\n",
      "Epoch 158/200, Batch 9/17, Loss G: 3.9199795722961426, Loss D: 0.007000637240707874\n",
      "Epoch 158/200, Batch 10/17, Loss G: 3.817059278488159, Loss D: 0.010629297234117985\n",
      "Epoch 158/200, Batch 11/17, Loss G: 3.7625889778137207, Loss D: 0.01100929081439972\n",
      "Epoch 158/200, Batch 12/17, Loss G: 3.9886860847473145, Loss D: 0.008831322193145752\n",
      "Epoch 158/200, Batch 13/17, Loss G: 3.7015604972839355, Loss D: 0.01242215558886528\n",
      "Epoch 158/200, Batch 14/17, Loss G: 3.879370927810669, Loss D: 0.007819602265954018\n",
      "Epoch 158/200, Batch 15/17, Loss G: 3.897690773010254, Loss D: 0.008404985070228577\n",
      "Epoch 158/200, Batch 16/17, Loss G: 4.2194504737854, Loss D: 0.04728022962808609\n",
      "Epoch 159/200, Batch 0/17, Loss G: 3.380861759185791, Loss D: 0.20365110039710999\n",
      "Epoch 159/200, Batch 1/17, Loss G: 4.107168197631836, Loss D: 0.09942357242107391\n",
      "Epoch 159/200, Batch 2/17, Loss G: 3.792553424835205, Loss D: 0.031491998583078384\n",
      "Epoch 159/200, Batch 3/17, Loss G: 3.9132065773010254, Loss D: 0.02185387723147869\n",
      "Epoch 159/200, Batch 4/17, Loss G: 3.6674890518188477, Loss D: 0.023707015439867973\n",
      "Epoch 159/200, Batch 5/17, Loss G: 3.9885854721069336, Loss D: 0.017369527369737625\n",
      "Epoch 159/200, Batch 6/17, Loss G: 3.847604751586914, Loss D: 0.05676385387778282\n",
      "Epoch 159/200, Batch 7/17, Loss G: 3.508302688598633, Loss D: 0.1272817999124527\n",
      "Epoch 159/200, Batch 8/17, Loss G: 3.9360485076904297, Loss D: 0.0714331865310669\n",
      "Epoch 159/200, Batch 9/17, Loss G: 3.8112783432006836, Loss D: 0.01497019175440073\n",
      "Epoch 159/200, Batch 10/17, Loss G: 3.7179884910583496, Loss D: 0.030328046530485153\n",
      "Epoch 159/200, Batch 11/17, Loss G: 3.8736014366149902, Loss D: 0.07231691479682922\n",
      "Epoch 159/200, Batch 12/17, Loss G: 3.7840964794158936, Loss D: 0.09631552547216415\n",
      "Epoch 159/200, Batch 13/17, Loss G: 3.876561164855957, Loss D: 0.037792306393384933\n",
      "Epoch 159/200, Batch 14/17, Loss G: 3.756838321685791, Loss D: 0.06521868705749512\n",
      "Epoch 159/200, Batch 15/17, Loss G: 3.421873092651367, Loss D: 0.1354769915342331\n",
      "Epoch 159/200, Batch 16/17, Loss G: 3.9473371505737305, Loss D: 0.0757569894194603\n",
      "Epoch 160/200, Batch 0/17, Loss G: 3.7313923835754395, Loss D: 0.06311648339033127\n",
      "Epoch 160/200, Batch 1/17, Loss G: 3.8589253425598145, Loss D: 0.031226657330989838\n",
      "Epoch 160/200, Batch 2/17, Loss G: 3.7848334312438965, Loss D: 0.030390508472919464\n",
      "Epoch 160/200, Batch 3/17, Loss G: 3.918581962585449, Loss D: 0.021527739241719246\n",
      "Epoch 160/200, Batch 4/17, Loss G: 3.9111480712890625, Loss D: 0.01960643008351326\n",
      "Epoch 160/200, Batch 5/17, Loss G: 3.5846571922302246, Loss D: 0.07288630306720734\n",
      "Epoch 160/200, Batch 6/17, Loss G: 3.902859926223755, Loss D: 0.09908176958560944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200, Batch 7/17, Loss G: 3.5404765605926514, Loss D: 0.06915455311536789\n",
      "Epoch 160/200, Batch 8/17, Loss G: 4.000519752502441, Loss D: 0.040059395134449005\n",
      "Epoch 160/200, Batch 9/17, Loss G: 3.681736946105957, Loss D: 0.024532869458198547\n",
      "Epoch 160/200, Batch 10/17, Loss G: 3.708934783935547, Loss D: 0.023873932659626007\n",
      "Epoch 160/200, Batch 11/17, Loss G: 3.6914243698120117, Loss D: 0.02518625557422638\n",
      "Epoch 160/200, Batch 12/17, Loss G: 3.9381890296936035, Loss D: 0.014475925825536251\n",
      "Epoch 160/200, Batch 13/17, Loss G: 3.8586316108703613, Loss D: 0.03828327730298042\n",
      "Epoch 160/200, Batch 14/17, Loss G: 3.6942298412323, Loss D: 0.031474582850933075\n",
      "Epoch 160/200, Batch 15/17, Loss G: 3.9746642112731934, Loss D: 0.01988781988620758\n",
      "Epoch 160/200, Batch 16/17, Loss G: 3.8230643272399902, Loss D: 0.013415037654340267\n",
      "Epoch 161/200, Batch 0/17, Loss G: 3.810962677001953, Loss D: 0.020973339676856995\n",
      "Epoch 161/200, Batch 1/17, Loss G: 3.925194025039673, Loss D: 0.02014875039458275\n",
      "Epoch 161/200, Batch 2/17, Loss G: 4.015103340148926, Loss D: 0.011966997757554054\n",
      "Epoch 161/200, Batch 3/17, Loss G: 3.7061657905578613, Loss D: 0.01257616002112627\n",
      "Epoch 161/200, Batch 4/17, Loss G: 3.921539068222046, Loss D: 0.013649674132466316\n",
      "Epoch 161/200, Batch 5/17, Loss G: 3.927265167236328, Loss D: 0.015112340450286865\n",
      "Epoch 161/200, Batch 6/17, Loss G: 3.8583974838256836, Loss D: 0.016272835433483124\n",
      "Epoch 161/200, Batch 7/17, Loss G: 3.7458696365356445, Loss D: 0.015430389903485775\n",
      "Epoch 161/200, Batch 8/17, Loss G: 3.6206111907958984, Loss D: 0.01904047280550003\n",
      "Epoch 161/200, Batch 9/17, Loss G: 3.8437585830688477, Loss D: 0.010997027158737183\n",
      "Epoch 161/200, Batch 10/17, Loss G: 3.816114902496338, Loss D: 0.011603198945522308\n",
      "Epoch 161/200, Batch 11/17, Loss G: 3.7010769844055176, Loss D: 0.013938652351498604\n",
      "Epoch 161/200, Batch 12/17, Loss G: 3.8542604446411133, Loss D: 0.010041028261184692\n",
      "Epoch 161/200, Batch 13/17, Loss G: 3.742708921432495, Loss D: 0.013095490634441376\n",
      "Epoch 161/200, Batch 14/17, Loss G: 3.8544259071350098, Loss D: 0.011045601218938828\n",
      "Epoch 161/200, Batch 15/17, Loss G: 3.8188672065734863, Loss D: 0.01348075270652771\n",
      "Epoch 161/200, Batch 16/17, Loss G: 3.879289150238037, Loss D: 0.013487868010997772\n",
      "Epoch 162/200, Batch 0/17, Loss G: 3.8951199054718018, Loss D: 0.007988275960087776\n",
      "Epoch 162/200, Batch 1/17, Loss G: 3.720625400543213, Loss D: 0.008073367178440094\n",
      "Epoch 162/200, Batch 2/17, Loss G: 3.8868722915649414, Loss D: 0.008656182326376438\n",
      "Epoch 162/200, Batch 3/17, Loss G: 3.8468847274780273, Loss D: 0.00968097522854805\n",
      "Epoch 162/200, Batch 4/17, Loss G: 3.7324438095092773, Loss D: 0.029408715665340424\n",
      "Epoch 162/200, Batch 5/17, Loss G: 3.5044960975646973, Loss D: 0.045633599162101746\n",
      "Epoch 162/200, Batch 6/17, Loss G: 4.025983810424805, Loss D: 0.023663654923439026\n",
      "Epoch 162/200, Batch 7/17, Loss G: 3.7818551063537598, Loss D: 0.011902529746294022\n",
      "Epoch 162/200, Batch 8/17, Loss G: 3.8262906074523926, Loss D: 0.014826525002717972\n",
      "Epoch 162/200, Batch 9/17, Loss G: 3.796147346496582, Loss D: 0.007306070066988468\n",
      "Epoch 162/200, Batch 10/17, Loss G: 3.8038594722747803, Loss D: 0.011599734425544739\n",
      "Epoch 162/200, Batch 11/17, Loss G: 3.814943313598633, Loss D: 0.011360976845026016\n",
      "Epoch 162/200, Batch 12/17, Loss G: 3.668968677520752, Loss D: 0.013511838391423225\n",
      "Epoch 162/200, Batch 13/17, Loss G: 3.759263515472412, Loss D: 0.013945434242486954\n",
      "Epoch 162/200, Batch 14/17, Loss G: 3.8664979934692383, Loss D: 0.009790392592549324\n",
      "Epoch 162/200, Batch 15/17, Loss G: 3.902911424636841, Loss D: 0.008335340768098831\n",
      "Epoch 162/200, Batch 16/17, Loss G: 3.777235507965088, Loss D: 0.004387750755995512\n",
      "Epoch 163/200, Batch 0/17, Loss G: 3.944432258605957, Loss D: 0.016479454934597015\n",
      "Epoch 163/200, Batch 1/17, Loss G: 3.8888087272644043, Loss D: 0.007692232728004456\n",
      "Epoch 163/200, Batch 2/17, Loss G: 3.8736679553985596, Loss D: 0.005008601117879152\n",
      "Epoch 163/200, Batch 3/17, Loss G: 3.811981678009033, Loss D: 0.010467384941875935\n",
      "Epoch 163/200, Batch 4/17, Loss G: 3.949223279953003, Loss D: 0.01692816987633705\n",
      "Epoch 163/200, Batch 5/17, Loss G: 3.736321449279785, Loss D: 0.009582030586898327\n",
      "Epoch 163/200, Batch 6/17, Loss G: 3.7244505882263184, Loss D: 0.007574421353638172\n",
      "Epoch 163/200, Batch 7/17, Loss G: 3.82289981842041, Loss D: 0.00977401901036501\n",
      "Epoch 163/200, Batch 8/17, Loss G: 3.9429879188537598, Loss D: 0.00846075639128685\n",
      "Epoch 163/200, Batch 9/17, Loss G: 3.8677148818969727, Loss D: 0.009320173412561417\n",
      "Epoch 163/200, Batch 10/17, Loss G: 3.745474100112915, Loss D: 0.01519988477230072\n",
      "Epoch 163/200, Batch 11/17, Loss G: 3.6203651428222656, Loss D: 0.009521677158772945\n",
      "Epoch 163/200, Batch 12/17, Loss G: 3.610734462738037, Loss D: 0.01244254969060421\n",
      "Epoch 163/200, Batch 13/17, Loss G: 3.884434223175049, Loss D: 0.009081539697945118\n",
      "Epoch 163/200, Batch 14/17, Loss G: 3.938492774963379, Loss D: 0.012707588262856007\n",
      "Epoch 163/200, Batch 15/17, Loss G: 3.818230628967285, Loss D: 0.008701087906956673\n",
      "Epoch 163/200, Batch 16/17, Loss G: 3.788896083831787, Loss D: 0.00740030175074935\n",
      "Epoch 164/200, Batch 0/17, Loss G: 3.863436222076416, Loss D: 0.011698991060256958\n",
      "Epoch 164/200, Batch 1/17, Loss G: 3.8953099250793457, Loss D: 0.017825311049818993\n",
      "Epoch 164/200, Batch 2/17, Loss G: 3.6207242012023926, Loss D: 0.011306949891149998\n",
      "Epoch 164/200, Batch 3/17, Loss G: 3.905345916748047, Loss D: 0.009713996201753616\n",
      "Epoch 164/200, Batch 4/17, Loss G: 3.828946590423584, Loss D: 0.010930102318525314\n",
      "Epoch 164/200, Batch 5/17, Loss G: 3.923750400543213, Loss D: 0.008235957473516464\n",
      "Epoch 164/200, Batch 6/17, Loss G: 3.8687143325805664, Loss D: 0.005222594831138849\n",
      "Epoch 164/200, Batch 7/17, Loss G: 3.848098039627075, Loss D: 0.009743694216012955\n",
      "Epoch 164/200, Batch 8/17, Loss G: 3.932884931564331, Loss D: 0.009848551824688911\n",
      "Epoch 164/200, Batch 9/17, Loss G: 3.834296703338623, Loss D: 0.007803607732057571\n",
      "Epoch 164/200, Batch 10/17, Loss G: 3.681790351867676, Loss D: 0.006465353537350893\n",
      "Epoch 164/200, Batch 11/17, Loss G: 3.788900852203369, Loss D: 0.009622201323509216\n",
      "Epoch 164/200, Batch 12/17, Loss G: 3.7528958320617676, Loss D: 0.010629994794726372\n",
      "Epoch 164/200, Batch 13/17, Loss G: 3.93861985206604, Loss D: 0.008572868071496487\n",
      "Epoch 164/200, Batch 14/17, Loss G: 3.726031541824341, Loss D: 0.005995537154376507\n",
      "Epoch 164/200, Batch 15/17, Loss G: 3.7581305503845215, Loss D: 0.006077924743294716\n",
      "Epoch 164/200, Batch 16/17, Loss G: 3.9594309329986572, Loss D: 0.006923791486769915\n",
      "Epoch 165/200, Batch 0/17, Loss G: 3.949317216873169, Loss D: 0.007104160264134407\n",
      "Epoch 165/200, Batch 1/17, Loss G: 3.782888889312744, Loss D: 0.005302636418491602\n",
      "Epoch 165/200, Batch 2/17, Loss G: 3.876192569732666, Loss D: 0.004536708816885948\n",
      "Epoch 165/200, Batch 3/17, Loss G: 3.8636789321899414, Loss D: 0.0070189666002988815\n",
      "Epoch 165/200, Batch 4/17, Loss G: 3.8123950958251953, Loss D: 0.005176384933292866\n",
      "Epoch 165/200, Batch 5/17, Loss G: 3.932082176208496, Loss D: 0.006692903116345406\n",
      "Epoch 165/200, Batch 6/17, Loss G: 3.733480930328369, Loss D: 0.005580270197242498\n",
      "Epoch 165/200, Batch 7/17, Loss G: 3.8429386615753174, Loss D: 0.005908783059567213\n",
      "Epoch 165/200, Batch 8/17, Loss G: 3.850555896759033, Loss D: 0.0077624390833079815\n",
      "Epoch 165/200, Batch 9/17, Loss G: 3.7341699600219727, Loss D: 0.011828191578388214\n",
      "Epoch 165/200, Batch 10/17, Loss G: 3.636338710784912, Loss D: 0.017048202455043793\n",
      "Epoch 165/200, Batch 11/17, Loss G: 3.908565044403076, Loss D: 0.008131656795740128\n",
      "Epoch 165/200, Batch 12/17, Loss G: 3.698624849319458, Loss D: 0.012007229030132294\n",
      "Epoch 165/200, Batch 13/17, Loss G: 3.9068827629089355, Loss D: 0.0061961449682712555\n",
      "Epoch 165/200, Batch 14/17, Loss G: 3.839460611343384, Loss D: 0.005861539393663406\n",
      "Epoch 165/200, Batch 15/17, Loss G: 3.802657127380371, Loss D: 0.010235946625471115\n",
      "Epoch 165/200, Batch 16/17, Loss G: 3.968184232711792, Loss D: 0.008637522347271442\n",
      "Epoch 166/200, Batch 0/17, Loss G: 4.052347183227539, Loss D: 0.009077790193259716\n",
      "Epoch 166/200, Batch 1/17, Loss G: 3.682788610458374, Loss D: 0.008246442303061485\n",
      "Epoch 166/200, Batch 2/17, Loss G: 3.7159910202026367, Loss D: 0.0072433422319591045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/200, Batch 3/17, Loss G: 4.077709197998047, Loss D: 0.0037719563115388155\n",
      "Epoch 166/200, Batch 4/17, Loss G: 3.7993831634521484, Loss D: 0.008398289792239666\n",
      "Epoch 166/200, Batch 5/17, Loss G: 3.8181207180023193, Loss D: 0.006182845216244459\n",
      "Epoch 166/200, Batch 6/17, Loss G: 3.812145709991455, Loss D: 0.007107683457434177\n",
      "Epoch 166/200, Batch 7/17, Loss G: 3.7378883361816406, Loss D: 0.006432783789932728\n",
      "Epoch 166/200, Batch 8/17, Loss G: 3.765416145324707, Loss D: 0.009499100968241692\n",
      "Epoch 166/200, Batch 9/17, Loss G: 4.097133159637451, Loss D: 0.010361535474658012\n",
      "Epoch 166/200, Batch 10/17, Loss G: 3.8353676795959473, Loss D: 0.005928875878453255\n",
      "Epoch 166/200, Batch 11/17, Loss G: 3.8562111854553223, Loss D: 0.003842955455183983\n",
      "Epoch 166/200, Batch 12/17, Loss G: 3.9120090007781982, Loss D: 0.0037253329064697027\n",
      "Epoch 166/200, Batch 13/17, Loss G: 4.038657188415527, Loss D: 0.004172170534729958\n",
      "Epoch 166/200, Batch 14/17, Loss G: 3.8617360591888428, Loss D: 0.01426391489803791\n",
      "Epoch 166/200, Batch 15/17, Loss G: 3.5042004585266113, Loss D: 0.01785394921898842\n",
      "Epoch 166/200, Batch 16/17, Loss G: 3.9506406784057617, Loss D: 0.007186456583440304\n",
      "Epoch 167/200, Batch 0/17, Loss G: 3.9851043224334717, Loss D: 0.010508842766284943\n",
      "Epoch 167/200, Batch 1/17, Loss G: 3.950888156890869, Loss D: 0.01495311688631773\n",
      "Epoch 167/200, Batch 2/17, Loss G: 3.5592875480651855, Loss D: 0.0397346206009388\n",
      "Epoch 167/200, Batch 3/17, Loss G: 3.90779709815979, Loss D: 0.021544016897678375\n",
      "Epoch 167/200, Batch 4/17, Loss G: 3.9183835983276367, Loss D: 0.01350102573633194\n",
      "Epoch 167/200, Batch 5/17, Loss G: 3.749939203262329, Loss D: 0.012495966628193855\n",
      "Epoch 167/200, Batch 6/17, Loss G: 3.790032386779785, Loss D: 0.009507954120635986\n",
      "Epoch 167/200, Batch 7/17, Loss G: 3.9258575439453125, Loss D: 0.010804496705532074\n",
      "Epoch 167/200, Batch 8/17, Loss G: 3.9294021129608154, Loss D: 0.0040788124315440655\n",
      "Epoch 167/200, Batch 9/17, Loss G: 3.7115225791931152, Loss D: 0.00991229247301817\n",
      "Epoch 167/200, Batch 10/17, Loss G: 3.6424856185913086, Loss D: 0.018273990601301193\n",
      "Epoch 167/200, Batch 11/17, Loss G: 3.8922524452209473, Loss D: 0.008801352232694626\n",
      "Epoch 167/200, Batch 12/17, Loss G: 3.713494300842285, Loss D: 0.011052584275603294\n",
      "Epoch 167/200, Batch 13/17, Loss G: 3.678983449935913, Loss D: 0.005782925058156252\n",
      "Epoch 167/200, Batch 14/17, Loss G: 3.88497257232666, Loss D: 0.009133848361670971\n",
      "Epoch 167/200, Batch 15/17, Loss G: 3.770543098449707, Loss D: 0.0046895407140254974\n",
      "Epoch 167/200, Batch 16/17, Loss G: 4.009878158569336, Loss D: 0.011464051902294159\n",
      "Epoch 168/200, Batch 0/17, Loss G: 3.9433538913726807, Loss D: 0.005399050656706095\n",
      "Epoch 168/200, Batch 1/17, Loss G: 3.830404043197632, Loss D: 0.005023994483053684\n",
      "Epoch 168/200, Batch 2/17, Loss G: 3.8382201194763184, Loss D: 0.004265175200998783\n",
      "Epoch 168/200, Batch 3/17, Loss G: 3.776008367538452, Loss D: 0.011585300788283348\n",
      "Epoch 168/200, Batch 4/17, Loss G: 3.6108407974243164, Loss D: 0.012329951860010624\n",
      "Epoch 168/200, Batch 5/17, Loss G: 3.760770320892334, Loss D: 0.005730160977691412\n",
      "Epoch 168/200, Batch 6/17, Loss G: 3.782310962677002, Loss D: 0.007694388274103403\n",
      "Epoch 168/200, Batch 7/17, Loss G: 3.7479679584503174, Loss D: 0.010675761848688126\n",
      "Epoch 168/200, Batch 8/17, Loss G: 3.7012412548065186, Loss D: 0.011803611181676388\n",
      "Epoch 168/200, Batch 9/17, Loss G: 3.8458669185638428, Loss D: 0.008019590750336647\n",
      "Epoch 168/200, Batch 10/17, Loss G: 3.883375883102417, Loss D: 0.004081002902239561\n",
      "Epoch 168/200, Batch 11/17, Loss G: 3.905256748199463, Loss D: 0.011399591341614723\n",
      "Epoch 168/200, Batch 12/17, Loss G: 3.6107687950134277, Loss D: 0.011112920939922333\n",
      "Epoch 168/200, Batch 13/17, Loss G: 3.840860605239868, Loss D: 0.0067846523597836494\n",
      "Epoch 168/200, Batch 14/17, Loss G: 3.868691921234131, Loss D: 0.007445881143212318\n",
      "Epoch 168/200, Batch 15/17, Loss G: 3.96871280670166, Loss D: 0.004618032369762659\n",
      "Epoch 168/200, Batch 16/17, Loss G: 3.97959566116333, Loss D: 0.004186434671282768\n",
      "Epoch 169/200, Batch 0/17, Loss G: 3.9068844318389893, Loss D: 0.00516142975538969\n",
      "Epoch 169/200, Batch 1/17, Loss G: 3.7463388442993164, Loss D: 0.005310222040861845\n",
      "Epoch 169/200, Batch 2/17, Loss G: 3.819465160369873, Loss D: 0.003802543506026268\n",
      "Epoch 169/200, Batch 3/17, Loss G: 3.831068754196167, Loss D: 0.006606524344533682\n",
      "Epoch 169/200, Batch 4/17, Loss G: 3.7282660007476807, Loss D: 0.007764920592308044\n",
      "Epoch 169/200, Batch 5/17, Loss G: 3.711996555328369, Loss D: 0.01037833746522665\n",
      "Epoch 169/200, Batch 6/17, Loss G: 3.8446311950683594, Loss D: 0.00711890310049057\n",
      "Epoch 169/200, Batch 7/17, Loss G: 3.715181350708008, Loss D: 0.0077625904232263565\n",
      "Epoch 169/200, Batch 8/17, Loss G: 3.822258710861206, Loss D: 0.007737975101917982\n",
      "Epoch 169/200, Batch 9/17, Loss G: 3.8973608016967773, Loss D: 0.009064795449376106\n",
      "Epoch 169/200, Batch 10/17, Loss G: 3.87286376953125, Loss D: 0.007642604876309633\n",
      "Epoch 169/200, Batch 11/17, Loss G: 3.8780930042266846, Loss D: 0.006962258368730545\n",
      "Epoch 169/200, Batch 12/17, Loss G: 3.7473959922790527, Loss D: 0.012192869558930397\n",
      "Epoch 169/200, Batch 13/17, Loss G: 3.7208914756774902, Loss D: 0.019014881923794746\n",
      "Epoch 169/200, Batch 14/17, Loss G: 3.8592562675476074, Loss D: 0.013407694175839424\n",
      "Epoch 169/200, Batch 15/17, Loss G: 3.7898054122924805, Loss D: 0.011932643130421638\n",
      "Epoch 169/200, Batch 16/17, Loss G: 4.2489166259765625, Loss D: 0.019201481714844704\n",
      "Epoch 170/200, Batch 0/17, Loss G: 3.571512222290039, Loss D: 0.02524789609014988\n",
      "Epoch 170/200, Batch 1/17, Loss G: 4.01911735534668, Loss D: 0.006099143996834755\n",
      "Epoch 170/200, Batch 2/17, Loss G: 3.8093080520629883, Loss D: 0.017496436834335327\n",
      "Epoch 170/200, Batch 3/17, Loss G: 4.016788005828857, Loss D: 0.012566866353154182\n",
      "Epoch 170/200, Batch 4/17, Loss G: 3.750798225402832, Loss D: 0.012249869294464588\n",
      "Epoch 170/200, Batch 5/17, Loss G: 3.8598310947418213, Loss D: 0.005083219613879919\n",
      "Epoch 170/200, Batch 6/17, Loss G: 3.666532039642334, Loss D: 0.012404694221913815\n",
      "Epoch 170/200, Batch 7/17, Loss G: 3.635873556137085, Loss D: 0.02410447783768177\n",
      "Epoch 170/200, Batch 8/17, Loss G: 3.804320812225342, Loss D: 0.020407693460583687\n",
      "Epoch 170/200, Batch 9/17, Loss G: 3.7094674110412598, Loss D: 0.004961181432008743\n",
      "Epoch 170/200, Batch 10/17, Loss G: 3.661391258239746, Loss D: 0.018198128789663315\n",
      "Epoch 170/200, Batch 11/17, Loss G: 3.911642074584961, Loss D: 0.009061860851943493\n",
      "Epoch 170/200, Batch 12/17, Loss G: 3.832313060760498, Loss D: 0.008334101177752018\n",
      "Epoch 170/200, Batch 13/17, Loss G: 3.822467803955078, Loss D: 0.008155577816069126\n",
      "Epoch 170/200, Batch 14/17, Loss G: 3.763953685760498, Loss D: 0.005500094965100288\n",
      "Epoch 170/200, Batch 15/17, Loss G: 3.805790424346924, Loss D: 0.006029801443219185\n",
      "Epoch 170/200, Batch 16/17, Loss G: 3.7494194507598877, Loss D: 0.008199045434594154\n",
      "Epoch 171/200, Batch 0/17, Loss G: 3.691927433013916, Loss D: 0.009489472955465317\n",
      "Epoch 171/200, Batch 1/17, Loss G: 4.057469367980957, Loss D: 0.004746393300592899\n",
      "Epoch 171/200, Batch 2/17, Loss G: 3.8146042823791504, Loss D: 0.006994018331170082\n",
      "Epoch 171/200, Batch 3/17, Loss G: 3.8799686431884766, Loss D: 0.007829287089407444\n",
      "Epoch 171/200, Batch 4/17, Loss G: 3.9233627319335938, Loss D: 0.004286657553166151\n",
      "Epoch 171/200, Batch 5/17, Loss G: 3.8238210678100586, Loss D: 0.007336277049034834\n",
      "Epoch 171/200, Batch 6/17, Loss G: 3.616091728210449, Loss D: 0.012901430018246174\n",
      "Epoch 171/200, Batch 7/17, Loss G: 3.845116138458252, Loss D: 0.006102598737925291\n",
      "Epoch 171/200, Batch 8/17, Loss G: 3.777439594268799, Loss D: 0.009962273761630058\n",
      "Epoch 171/200, Batch 9/17, Loss G: 3.7451305389404297, Loss D: 0.007396402768790722\n",
      "Epoch 171/200, Batch 10/17, Loss G: 3.882941246032715, Loss D: 0.013234345242381096\n",
      "Epoch 171/200, Batch 11/17, Loss G: 3.874760866165161, Loss D: 0.005119542591273785\n",
      "Epoch 171/200, Batch 12/17, Loss G: 3.7848966121673584, Loss D: 0.004463939927518368\n",
      "Epoch 171/200, Batch 13/17, Loss G: 3.854680061340332, Loss D: 0.005141089670360088\n",
      "Epoch 171/200, Batch 14/17, Loss G: 3.659438133239746, Loss D: 0.007619253359735012\n",
      "Epoch 171/200, Batch 15/17, Loss G: 3.673426866531372, Loss D: 0.00452630128711462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200, Batch 16/17, Loss G: 3.7148523330688477, Loss D: 0.008394728414714336\n",
      "Epoch 172/200, Batch 0/17, Loss G: 3.987593650817871, Loss D: 0.005411637015640736\n",
      "Epoch 172/200, Batch 1/17, Loss G: 3.830115795135498, Loss D: 0.008479391224682331\n",
      "Epoch 172/200, Batch 2/17, Loss G: 4.043510437011719, Loss D: 0.007238177116960287\n",
      "Epoch 172/200, Batch 3/17, Loss G: 3.5513923168182373, Loss D: 0.0172259621322155\n",
      "Epoch 172/200, Batch 4/17, Loss G: 3.970203399658203, Loss D: 0.015188634395599365\n",
      "Epoch 172/200, Batch 5/17, Loss G: 3.660743236541748, Loss D: 0.014236188493669033\n",
      "Epoch 172/200, Batch 6/17, Loss G: 3.9606270790100098, Loss D: 0.004171409644186497\n",
      "Epoch 172/200, Batch 7/17, Loss G: 3.7997183799743652, Loss D: 0.011441553011536598\n",
      "Epoch 172/200, Batch 8/17, Loss G: 3.7163000106811523, Loss D: 0.011196945793926716\n",
      "Epoch 172/200, Batch 9/17, Loss G: 3.6303815841674805, Loss D: 0.010881032794713974\n",
      "Epoch 172/200, Batch 10/17, Loss G: 3.657390594482422, Loss D: 0.009417598135769367\n",
      "Epoch 172/200, Batch 11/17, Loss G: 3.714505195617676, Loss D: 0.0069862231612205505\n",
      "Epoch 172/200, Batch 12/17, Loss G: 3.753404140472412, Loss D: 0.006507025100290775\n",
      "Epoch 172/200, Batch 13/17, Loss G: 3.7512502670288086, Loss D: 0.013242550194263458\n",
      "Epoch 172/200, Batch 14/17, Loss G: 3.716503620147705, Loss D: 0.007071448490023613\n",
      "Epoch 172/200, Batch 15/17, Loss G: 4.070255279541016, Loss D: 0.006730155553668737\n",
      "Epoch 172/200, Batch 16/17, Loss G: 4.02394962310791, Loss D: 0.010498452931642532\n",
      "Epoch 173/200, Batch 0/17, Loss G: 3.9686503410339355, Loss D: 0.004139827564358711\n",
      "Epoch 173/200, Batch 1/17, Loss G: 3.839725971221924, Loss D: 0.026077859103679657\n",
      "Epoch 173/200, Batch 2/17, Loss G: 3.422642946243286, Loss D: 0.06451628357172012\n",
      "Epoch 173/200, Batch 3/17, Loss G: 4.042695045471191, Loss D: 0.07971546798944473\n",
      "Epoch 173/200, Batch 4/17, Loss G: 3.671478748321533, Loss D: 0.021728085353970528\n",
      "Epoch 173/200, Batch 5/17, Loss G: 3.5448570251464844, Loss D: 0.018177257850766182\n",
      "Epoch 173/200, Batch 6/17, Loss G: 3.75066876411438, Loss D: 0.013904227875173092\n",
      "Epoch 173/200, Batch 7/17, Loss G: 3.7377495765686035, Loss D: 0.034233227372169495\n",
      "Epoch 173/200, Batch 8/17, Loss G: 3.431201457977295, Loss D: 0.14900808036327362\n",
      "Epoch 173/200, Batch 9/17, Loss G: 4.056278705596924, Loss D: 0.2524709701538086\n",
      "Epoch 173/200, Batch 10/17, Loss G: 3.332416534423828, Loss D: 0.13803403079509735\n",
      "Epoch 173/200, Batch 11/17, Loss G: 3.8687846660614014, Loss D: 0.13893376290798187\n",
      "Epoch 173/200, Batch 12/17, Loss G: 3.494253158569336, Loss D: 0.12398336827754974\n",
      "Epoch 173/200, Batch 13/17, Loss G: 3.7052204608917236, Loss D: 0.11737697571516037\n",
      "Epoch 173/200, Batch 14/17, Loss G: 3.5721592903137207, Loss D: 0.10203924775123596\n",
      "Epoch 173/200, Batch 15/17, Loss G: 3.9885406494140625, Loss D: 0.12171593308448792\n",
      "Epoch 173/200, Batch 16/17, Loss G: 3.5443520545959473, Loss D: 0.181361585855484\n",
      "Epoch 174/200, Batch 0/17, Loss G: 3.9565823078155518, Loss D: 0.054243408143520355\n",
      "Epoch 174/200, Batch 1/17, Loss G: 3.7096505165100098, Loss D: 0.06296186149120331\n",
      "Epoch 174/200, Batch 2/17, Loss G: 3.765690565109253, Loss D: 0.0769077017903328\n",
      "Epoch 174/200, Batch 3/17, Loss G: 3.811399459838867, Loss D: 0.10431671142578125\n",
      "Epoch 174/200, Batch 4/17, Loss G: 3.5598859786987305, Loss D: 0.07299972325563431\n",
      "Epoch 174/200, Batch 5/17, Loss G: 4.137149333953857, Loss D: 0.05412540212273598\n",
      "Epoch 174/200, Batch 6/17, Loss G: 3.565657138824463, Loss D: 0.04084382951259613\n",
      "Epoch 174/200, Batch 7/17, Loss G: 3.7023801803588867, Loss D: 0.030511580407619476\n",
      "Epoch 174/200, Batch 8/17, Loss G: 3.6864657402038574, Loss D: 0.03663687780499458\n",
      "Epoch 174/200, Batch 9/17, Loss G: 3.9638938903808594, Loss D: 0.06873679906129837\n",
      "Epoch 174/200, Batch 10/17, Loss G: 3.585273027420044, Loss D: 0.07505203783512115\n",
      "Epoch 174/200, Batch 11/17, Loss G: 3.972219467163086, Loss D: 0.049108486622571945\n",
      "Epoch 174/200, Batch 12/17, Loss G: 3.6842877864837646, Loss D: 0.03771570697426796\n",
      "Epoch 174/200, Batch 13/17, Loss G: 3.6368393898010254, Loss D: 0.02751384675502777\n",
      "Epoch 174/200, Batch 14/17, Loss G: 3.7660703659057617, Loss D: 0.07001343369483948\n",
      "Epoch 174/200, Batch 15/17, Loss G: 3.5297141075134277, Loss D: 0.0518188551068306\n",
      "Epoch 174/200, Batch 16/17, Loss G: 3.942093849182129, Loss D: 0.024578925222158432\n",
      "Epoch 175/200, Batch 0/17, Loss G: 3.963505983352661, Loss D: 0.05211475118994713\n",
      "Epoch 175/200, Batch 1/17, Loss G: 3.734323501586914, Loss D: 0.0603252612054348\n",
      "Epoch 175/200, Batch 2/17, Loss G: 4.0812225341796875, Loss D: 0.026952657848596573\n",
      "Epoch 175/200, Batch 3/17, Loss G: 3.9660487174987793, Loss D: 0.06219770386815071\n",
      "Epoch 175/200, Batch 4/17, Loss G: 3.601547956466675, Loss D: 0.0638190507888794\n",
      "Epoch 175/200, Batch 5/17, Loss G: 3.7822353839874268, Loss D: 0.019452661275863647\n",
      "Epoch 175/200, Batch 6/17, Loss G: 3.764035224914551, Loss D: 0.07617798447608948\n",
      "Epoch 175/200, Batch 7/17, Loss G: 3.5970094203948975, Loss D: 0.055688563734292984\n",
      "Epoch 175/200, Batch 8/17, Loss G: 3.756503105163574, Loss D: 0.02984432876110077\n",
      "Epoch 175/200, Batch 9/17, Loss G: 3.933058261871338, Loss D: 0.04656502604484558\n",
      "Epoch 175/200, Batch 10/17, Loss G: 3.4093170166015625, Loss D: 0.03809057176113129\n",
      "Epoch 175/200, Batch 11/17, Loss G: 3.536884069442749, Loss D: 0.017407014966011047\n",
      "Epoch 175/200, Batch 12/17, Loss G: 3.8531293869018555, Loss D: 0.02051786333322525\n",
      "Epoch 175/200, Batch 13/17, Loss G: 3.722959280014038, Loss D: 0.012373154982924461\n",
      "Epoch 175/200, Batch 14/17, Loss G: 3.751319408416748, Loss D: 0.018149225041270256\n",
      "Epoch 175/200, Batch 15/17, Loss G: 3.782249927520752, Loss D: 0.019991997629404068\n",
      "Epoch 175/200, Batch 16/17, Loss G: 3.778958797454834, Loss D: 0.010305756703019142\n",
      "Epoch 176/200, Batch 0/17, Loss G: 3.652379035949707, Loss D: 0.018603291362524033\n",
      "Epoch 176/200, Batch 1/17, Loss G: 3.649507522583008, Loss D: 0.010915112681686878\n",
      "Epoch 176/200, Batch 2/17, Loss G: 4.005748271942139, Loss D: 0.013323241844773293\n",
      "Epoch 176/200, Batch 3/17, Loss G: 3.7406907081604004, Loss D: 0.015686122700572014\n",
      "Epoch 176/200, Batch 4/17, Loss G: 3.9123857021331787, Loss D: 0.011011684313416481\n",
      "Epoch 176/200, Batch 5/17, Loss G: 3.810168743133545, Loss D: 0.014833341352641582\n",
      "Epoch 176/200, Batch 6/17, Loss G: 3.673518419265747, Loss D: 0.01034113485366106\n",
      "Epoch 176/200, Batch 7/17, Loss G: 3.6379737854003906, Loss D: 0.01230028085410595\n",
      "Epoch 176/200, Batch 8/17, Loss G: 3.9169352054595947, Loss D: 0.02523130178451538\n",
      "Epoch 176/200, Batch 9/17, Loss G: 3.5729966163635254, Loss D: 0.02090894989669323\n",
      "Epoch 176/200, Batch 10/17, Loss G: 3.751448392868042, Loss D: 0.01177571713924408\n",
      "Epoch 176/200, Batch 11/17, Loss G: 3.7580809593200684, Loss D: 0.00868069939315319\n",
      "Epoch 176/200, Batch 12/17, Loss G: 3.799468517303467, Loss D: 0.019918128848075867\n",
      "Epoch 176/200, Batch 13/17, Loss G: 3.726919651031494, Loss D: 0.014457708224654198\n",
      "Epoch 176/200, Batch 14/17, Loss G: 3.7327661514282227, Loss D: 0.012656953185796738\n",
      "Epoch 176/200, Batch 15/17, Loss G: 3.732832908630371, Loss D: 0.01155671663582325\n",
      "Epoch 176/200, Batch 16/17, Loss G: 3.691588878631592, Loss D: 0.014114880003035069\n",
      "Epoch 177/200, Batch 0/17, Loss G: 3.724897861480713, Loss D: 0.020753920078277588\n",
      "Epoch 177/200, Batch 1/17, Loss G: 3.8162994384765625, Loss D: 0.02006463333964348\n",
      "Epoch 177/200, Batch 2/17, Loss G: 3.5697412490844727, Loss D: 0.010461089201271534\n",
      "Epoch 177/200, Batch 3/17, Loss G: 3.923868179321289, Loss D: 0.00980251096189022\n",
      "Epoch 177/200, Batch 4/17, Loss G: 3.6132383346557617, Loss D: 0.010001290589571\n",
      "Epoch 177/200, Batch 5/17, Loss G: 3.7501134872436523, Loss D: 0.013347726315259933\n",
      "Epoch 177/200, Batch 6/17, Loss G: 3.7484607696533203, Loss D: 0.015346227213740349\n",
      "Epoch 177/200, Batch 7/17, Loss G: 3.702014446258545, Loss D: 0.004173089750111103\n",
      "Epoch 177/200, Batch 8/17, Loss G: 3.850135326385498, Loss D: 0.006183863617479801\n",
      "Epoch 177/200, Batch 9/17, Loss G: 3.731459856033325, Loss D: 0.01063802931457758\n",
      "Epoch 177/200, Batch 10/17, Loss G: 3.789212226867676, Loss D: 0.009880360215902328\n",
      "Epoch 177/200, Batch 11/17, Loss G: 3.839797019958496, Loss D: 0.007782568223774433\n",
      "Epoch 177/200, Batch 12/17, Loss G: 3.826542377471924, Loss D: 0.0064906105399131775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200, Batch 13/17, Loss G: 3.63297176361084, Loss D: 0.006844365503638983\n",
      "Epoch 177/200, Batch 14/17, Loss G: 3.638988733291626, Loss D: 0.009095540270209312\n",
      "Epoch 177/200, Batch 15/17, Loss G: 3.8226613998413086, Loss D: 0.012933354824781418\n",
      "Epoch 177/200, Batch 16/17, Loss G: 3.9920172691345215, Loss D: 0.004048631060868502\n",
      "Epoch 178/200, Batch 0/17, Loss G: 3.67984676361084, Loss D: 0.013127915561199188\n",
      "Epoch 178/200, Batch 1/17, Loss G: 3.9353837966918945, Loss D: 0.005528673063963652\n",
      "Epoch 178/200, Batch 2/17, Loss G: 4.004917144775391, Loss D: 0.003610357642173767\n",
      "Epoch 178/200, Batch 3/17, Loss G: 3.802338123321533, Loss D: 0.004233046900480986\n",
      "Epoch 178/200, Batch 4/17, Loss G: 3.676161766052246, Loss D: 0.004252527374774218\n",
      "Epoch 178/200, Batch 5/17, Loss G: 3.7315502166748047, Loss D: 0.006973654497414827\n",
      "Epoch 178/200, Batch 6/17, Loss G: 3.955984115600586, Loss D: 0.005387216340750456\n",
      "Epoch 178/200, Batch 7/17, Loss G: 3.7193875312805176, Loss D: 0.007905285805463791\n",
      "Epoch 178/200, Batch 8/17, Loss G: 3.8970589637756348, Loss D: 0.0035535111092031\n",
      "Epoch 178/200, Batch 9/17, Loss G: 3.7466044425964355, Loss D: 0.005125313065946102\n",
      "Epoch 178/200, Batch 10/17, Loss G: 3.596864700317383, Loss D: 0.0031201187521219254\n",
      "Epoch 178/200, Batch 11/17, Loss G: 3.621375322341919, Loss D: 0.005621370393782854\n",
      "Epoch 178/200, Batch 12/17, Loss G: 3.9457461833953857, Loss D: 0.008043975569307804\n",
      "Epoch 178/200, Batch 13/17, Loss G: 3.8500185012817383, Loss D: 0.003246563719585538\n",
      "Epoch 178/200, Batch 14/17, Loss G: 3.772104263305664, Loss D: 0.010030528530478477\n",
      "Epoch 178/200, Batch 15/17, Loss G: 3.721414566040039, Loss D: 0.006461497396230698\n",
      "Epoch 178/200, Batch 16/17, Loss G: 3.6268386840820312, Loss D: 0.00895734317600727\n",
      "Epoch 179/200, Batch 0/17, Loss G: 3.6876955032348633, Loss D: 0.0049111731350421906\n",
      "Epoch 179/200, Batch 1/17, Loss G: 3.7437195777893066, Loss D: 0.0064298720099031925\n",
      "Epoch 179/200, Batch 2/17, Loss G: 3.6606783866882324, Loss D: 0.004600119777023792\n",
      "Epoch 179/200, Batch 3/17, Loss G: 3.800673723220825, Loss D: 0.006048665381968021\n",
      "Epoch 179/200, Batch 4/17, Loss G: 3.838052749633789, Loss D: 0.003988093696534634\n",
      "Epoch 179/200, Batch 5/17, Loss G: 3.7512993812561035, Loss D: 0.0033191093243658543\n",
      "Epoch 179/200, Batch 6/17, Loss G: 3.875617742538452, Loss D: 0.0037676412612199783\n",
      "Epoch 179/200, Batch 7/17, Loss G: 3.930447578430176, Loss D: 0.007594540249556303\n",
      "Epoch 179/200, Batch 8/17, Loss G: 3.7691657543182373, Loss D: 0.003722886787727475\n",
      "Epoch 179/200, Batch 9/17, Loss G: 3.695570707321167, Loss D: 0.0055242134258151054\n",
      "Epoch 179/200, Batch 10/17, Loss G: 3.5875210762023926, Loss D: 0.003234259784221649\n",
      "Epoch 179/200, Batch 11/17, Loss G: 3.60611891746521, Loss D: 0.010526956990361214\n",
      "Epoch 179/200, Batch 12/17, Loss G: 3.89439058303833, Loss D: 0.0052074892446398735\n",
      "Epoch 179/200, Batch 13/17, Loss G: 3.710693836212158, Loss D: 0.029030650854110718\n",
      "Epoch 179/200, Batch 14/17, Loss G: 3.372664451599121, Loss D: 0.04729345440864563\n",
      "Epoch 179/200, Batch 15/17, Loss G: 3.934704065322876, Loss D: 0.011021256446838379\n",
      "Epoch 179/200, Batch 16/17, Loss G: 4.124191761016846, Loss D: 0.031510911881923676\n",
      "Epoch 180/200, Batch 0/17, Loss G: 3.6617279052734375, Loss D: 0.027592195197939873\n",
      "Epoch 180/200, Batch 1/17, Loss G: 3.7612500190734863, Loss D: 0.011244279332458973\n",
      "Epoch 180/200, Batch 2/17, Loss G: 3.794490337371826, Loss D: 0.01348841655999422\n",
      "Epoch 180/200, Batch 3/17, Loss G: 3.940251111984253, Loss D: 0.015936164185404778\n",
      "Epoch 180/200, Batch 4/17, Loss G: 3.67423677444458, Loss D: 0.01231826189905405\n",
      "Epoch 180/200, Batch 5/17, Loss G: 3.7440829277038574, Loss D: 0.014127279631793499\n",
      "Epoch 180/200, Batch 6/17, Loss G: 3.9877734184265137, Loss D: 0.019666042178869247\n",
      "Epoch 180/200, Batch 7/17, Loss G: 3.6658849716186523, Loss D: 0.01419998612254858\n",
      "Epoch 180/200, Batch 8/17, Loss G: 3.578819751739502, Loss D: 0.006180049851536751\n",
      "Epoch 180/200, Batch 9/17, Loss G: 3.6050028800964355, Loss D: 0.008864853531122208\n",
      "Epoch 180/200, Batch 10/17, Loss G: 3.8839101791381836, Loss D: 0.007613580673933029\n",
      "Epoch 180/200, Batch 11/17, Loss G: 3.6242785453796387, Loss D: 0.007813167758286\n",
      "Epoch 180/200, Batch 12/17, Loss G: 3.6581640243530273, Loss D: 0.010106584057211876\n",
      "Epoch 180/200, Batch 13/17, Loss G: 3.75693941116333, Loss D: 0.010628635995090008\n",
      "Epoch 180/200, Batch 14/17, Loss G: 3.717796802520752, Loss D: 0.009862758219242096\n",
      "Epoch 180/200, Batch 15/17, Loss G: 3.665097951889038, Loss D: 0.004369582049548626\n",
      "Epoch 180/200, Batch 16/17, Loss G: 4.110423564910889, Loss D: 0.002407320775091648\n",
      "Epoch 181/200, Batch 0/17, Loss G: 3.7645950317382812, Loss D: 0.008911277167499065\n",
      "Epoch 181/200, Batch 1/17, Loss G: 3.708146572113037, Loss D: 0.006045472342520952\n",
      "Epoch 181/200, Batch 2/17, Loss G: 3.619736671447754, Loss D: 0.008007294498383999\n",
      "Epoch 181/200, Batch 3/17, Loss G: 3.7383060455322266, Loss D: 0.007277952041476965\n",
      "Epoch 181/200, Batch 4/17, Loss G: 3.736142635345459, Loss D: 0.006180053576827049\n",
      "Epoch 181/200, Batch 5/17, Loss G: 3.726839542388916, Loss D: 0.005997892469167709\n",
      "Epoch 181/200, Batch 6/17, Loss G: 3.799389123916626, Loss D: 0.005399714224040508\n",
      "Epoch 181/200, Batch 7/17, Loss G: 3.6682493686676025, Loss D: 0.006241874769330025\n",
      "Epoch 181/200, Batch 8/17, Loss G: 3.8565926551818848, Loss D: 0.005009674467146397\n",
      "Epoch 181/200, Batch 9/17, Loss G: 3.664888381958008, Loss D: 0.0068054161965847015\n",
      "Epoch 181/200, Batch 10/17, Loss G: 3.6920952796936035, Loss D: 0.007602250203490257\n",
      "Epoch 181/200, Batch 11/17, Loss G: 3.792271614074707, Loss D: 0.005117797292768955\n",
      "Epoch 181/200, Batch 12/17, Loss G: 3.703789710998535, Loss D: 0.005292641464620829\n",
      "Epoch 181/200, Batch 13/17, Loss G: 3.7454962730407715, Loss D: 0.007801472209393978\n",
      "Epoch 181/200, Batch 14/17, Loss G: 3.7426180839538574, Loss D: 0.004440668039023876\n",
      "Epoch 181/200, Batch 15/17, Loss G: 3.766174793243408, Loss D: 0.005186036694794893\n",
      "Epoch 181/200, Batch 16/17, Loss G: 3.859182834625244, Loss D: 0.0037421383894979954\n",
      "Epoch 182/200, Batch 0/17, Loss G: 3.699875831604004, Loss D: 0.004166667349636555\n",
      "Epoch 182/200, Batch 1/17, Loss G: 3.8434953689575195, Loss D: 0.007751743774861097\n",
      "Epoch 182/200, Batch 2/17, Loss G: 3.925710678100586, Loss D: 0.0039321077056229115\n",
      "Epoch 182/200, Batch 3/17, Loss G: 3.7258620262145996, Loss D: 0.0043444279581308365\n",
      "Epoch 182/200, Batch 4/17, Loss G: 3.844182014465332, Loss D: 0.005320737138390541\n",
      "Epoch 182/200, Batch 5/17, Loss G: 3.7282094955444336, Loss D: 0.004914129618555307\n",
      "Epoch 182/200, Batch 6/17, Loss G: 3.709451198577881, Loss D: 0.004529261030256748\n",
      "Epoch 182/200, Batch 7/17, Loss G: 3.7751636505126953, Loss D: 0.004294656217098236\n",
      "Epoch 182/200, Batch 8/17, Loss G: 3.7316179275512695, Loss D: 0.0065087988041341305\n",
      "Epoch 182/200, Batch 9/17, Loss G: 3.7685399055480957, Loss D: 0.004236418288201094\n",
      "Epoch 182/200, Batch 10/17, Loss G: 3.7003908157348633, Loss D: 0.007286247797310352\n",
      "Epoch 182/200, Batch 11/17, Loss G: 3.644624710083008, Loss D: 0.006079583428800106\n",
      "Epoch 182/200, Batch 12/17, Loss G: 3.6751468181610107, Loss D: 0.006187354680150747\n",
      "Epoch 182/200, Batch 13/17, Loss G: 3.69320011138916, Loss D: 0.010030083358287811\n",
      "Epoch 182/200, Batch 14/17, Loss G: 3.8015575408935547, Loss D: 0.0043073915876448154\n",
      "Epoch 182/200, Batch 15/17, Loss G: 3.868137836456299, Loss D: 0.012922318652272224\n",
      "Epoch 182/200, Batch 16/17, Loss G: 3.7098684310913086, Loss D: 0.009631030261516571\n",
      "Epoch 183/200, Batch 0/17, Loss G: 3.8559675216674805, Loss D: 0.00603293813765049\n",
      "Epoch 183/200, Batch 1/17, Loss G: 3.682258129119873, Loss D: 0.00454758433625102\n",
      "Epoch 183/200, Batch 2/17, Loss G: 3.521428346633911, Loss D: 0.004845825955271721\n",
      "Epoch 183/200, Batch 3/17, Loss G: 3.6521668434143066, Loss D: 0.004705853760242462\n",
      "Epoch 183/200, Batch 4/17, Loss G: 3.7834219932556152, Loss D: 0.006778598763048649\n",
      "Epoch 183/200, Batch 5/17, Loss G: 3.7304909229278564, Loss D: 0.010292562656104565\n",
      "Epoch 183/200, Batch 6/17, Loss G: 3.5805370807647705, Loss D: 0.010801154188811779\n",
      "Epoch 183/200, Batch 7/17, Loss G: 3.9225311279296875, Loss D: 0.00475934986025095\n",
      "Epoch 183/200, Batch 8/17, Loss G: 3.6931865215301514, Loss D: 0.004369053989648819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200, Batch 9/17, Loss G: 3.8344030380249023, Loss D: 0.007082946598529816\n",
      "Epoch 183/200, Batch 10/17, Loss G: 3.7452945709228516, Loss D: 0.0035603628493845463\n",
      "Epoch 183/200, Batch 11/17, Loss G: 3.643712282180786, Loss D: 0.006512570194900036\n",
      "Epoch 183/200, Batch 12/17, Loss G: 3.7464354038238525, Loss D: 0.00449999887496233\n",
      "Epoch 183/200, Batch 13/17, Loss G: 3.737161636352539, Loss D: 0.008667769841849804\n",
      "Epoch 183/200, Batch 14/17, Loss G: 3.649843692779541, Loss D: 0.012268942780792713\n",
      "Epoch 183/200, Batch 15/17, Loss G: 3.835559844970703, Loss D: 0.002682237885892391\n",
      "Epoch 183/200, Batch 16/17, Loss G: 4.024994850158691, Loss D: 0.01100226491689682\n",
      "Epoch 184/200, Batch 0/17, Loss G: 3.6396121978759766, Loss D: 0.008321421220898628\n",
      "Epoch 184/200, Batch 1/17, Loss G: 3.7274513244628906, Loss D: 0.004308268427848816\n",
      "Epoch 184/200, Batch 2/17, Loss G: 3.9355626106262207, Loss D: 0.004213868174701929\n",
      "Epoch 184/200, Batch 3/17, Loss G: 3.6975371837615967, Loss D: 0.007222412154078484\n",
      "Epoch 184/200, Batch 4/17, Loss G: 3.772165298461914, Loss D: 0.002924330299720168\n",
      "Epoch 184/200, Batch 5/17, Loss G: 3.5514235496520996, Loss D: 0.009707298129796982\n",
      "Epoch 184/200, Batch 6/17, Loss G: 4.066213607788086, Loss D: 0.006316224113106728\n",
      "Epoch 184/200, Batch 7/17, Loss G: 3.8217082023620605, Loss D: 0.00847700797021389\n",
      "Epoch 184/200, Batch 8/17, Loss G: 3.7109389305114746, Loss D: 0.004880453459918499\n",
      "Epoch 184/200, Batch 9/17, Loss G: 3.7200636863708496, Loss D: 0.008738996461033821\n",
      "Epoch 184/200, Batch 10/17, Loss G: 3.685666561126709, Loss D: 0.00938020646572113\n",
      "Epoch 184/200, Batch 11/17, Loss G: 3.711088180541992, Loss D: 0.003061047289520502\n",
      "Epoch 184/200, Batch 12/17, Loss G: 3.80437970161438, Loss D: 0.00698844576254487\n",
      "Epoch 184/200, Batch 13/17, Loss G: 3.727715253829956, Loss D: 0.02111523598432541\n",
      "Epoch 184/200, Batch 14/17, Loss G: 3.332766056060791, Loss D: 0.057112421840429306\n",
      "Epoch 184/200, Batch 15/17, Loss G: 3.7594456672668457, Loss D: 0.03625822439789772\n",
      "Epoch 184/200, Batch 16/17, Loss G: 3.7694098949432373, Loss D: 0.02068372257053852\n",
      "Epoch 185/200, Batch 0/17, Loss G: 3.6854183673858643, Loss D: 0.010855194181203842\n",
      "Epoch 185/200, Batch 1/17, Loss G: 3.7860796451568604, Loss D: 0.007530694827437401\n",
      "Epoch 185/200, Batch 2/17, Loss G: 3.6713571548461914, Loss D: 0.008999071083962917\n",
      "Epoch 185/200, Batch 3/17, Loss G: 3.7771477699279785, Loss D: 0.005031867418438196\n",
      "Epoch 185/200, Batch 4/17, Loss G: 3.797654390335083, Loss D: 0.0038962680846452713\n",
      "Epoch 185/200, Batch 5/17, Loss G: 3.6677722930908203, Loss D: 0.007449580822139978\n",
      "Epoch 185/200, Batch 6/17, Loss G: 3.8539023399353027, Loss D: 0.0038023078814148903\n",
      "Epoch 185/200, Batch 7/17, Loss G: 3.7105932235717773, Loss D: 0.007215720601379871\n",
      "Epoch 185/200, Batch 8/17, Loss G: 3.8197646141052246, Loss D: 0.006558319553732872\n",
      "Epoch 185/200, Batch 9/17, Loss G: 3.9756147861480713, Loss D: 0.00536040123552084\n",
      "Epoch 185/200, Batch 10/17, Loss G: 3.6928539276123047, Loss D: 0.0060416096821427345\n",
      "Epoch 185/200, Batch 11/17, Loss G: 3.644656181335449, Loss D: 0.005276547744870186\n",
      "Epoch 185/200, Batch 12/17, Loss G: 3.715996742248535, Loss D: 0.007975544780492783\n",
      "Epoch 185/200, Batch 13/17, Loss G: 3.774217128753662, Loss D: 0.006618122570216656\n",
      "Epoch 185/200, Batch 14/17, Loss G: 3.70241641998291, Loss D: 0.0035324092023074627\n",
      "Epoch 185/200, Batch 15/17, Loss G: 3.8047220706939697, Loss D: 0.005584791302680969\n",
      "Epoch 185/200, Batch 16/17, Loss G: 3.747417449951172, Loss D: 0.004892674274742603\n",
      "Epoch 186/200, Batch 0/17, Loss G: 3.749559164047241, Loss D: 0.004362685605883598\n",
      "Epoch 186/200, Batch 1/17, Loss G: 3.8069238662719727, Loss D: 0.00250493036583066\n",
      "Epoch 186/200, Batch 2/17, Loss G: 3.601317882537842, Loss D: 0.006825706921517849\n",
      "Epoch 186/200, Batch 3/17, Loss G: 3.8432745933532715, Loss D: 0.004090392962098122\n",
      "Epoch 186/200, Batch 4/17, Loss G: 3.578433036804199, Loss D: 0.004986688960343599\n",
      "Epoch 186/200, Batch 5/17, Loss G: 3.5499119758605957, Loss D: 0.004527290351688862\n",
      "Epoch 186/200, Batch 6/17, Loss G: 3.7718796730041504, Loss D: 0.004678898490965366\n",
      "Epoch 186/200, Batch 7/17, Loss G: 3.7148709297180176, Loss D: 0.006914522498846054\n",
      "Epoch 186/200, Batch 8/17, Loss G: 3.834115505218506, Loss D: 0.005373306572437286\n",
      "Epoch 186/200, Batch 9/17, Loss G: 3.801525115966797, Loss D: 0.0024043056182563305\n",
      "Epoch 186/200, Batch 10/17, Loss G: 3.7297325134277344, Loss D: 0.003547252621501684\n",
      "Epoch 186/200, Batch 11/17, Loss G: 3.671741485595703, Loss D: 0.004964039660990238\n",
      "Epoch 186/200, Batch 12/17, Loss G: 3.6952919960021973, Loss D: 0.008761744946241379\n",
      "Epoch 186/200, Batch 13/17, Loss G: 3.9504499435424805, Loss D: 0.0056700324639678\n",
      "Epoch 186/200, Batch 14/17, Loss G: 3.726104259490967, Loss D: 0.004984845407307148\n",
      "Epoch 186/200, Batch 15/17, Loss G: 3.6399178504943848, Loss D: 0.005238687619566917\n",
      "Epoch 186/200, Batch 16/17, Loss G: 3.8131041526794434, Loss D: 0.0039710551500320435\n",
      "Epoch 187/200, Batch 0/17, Loss G: 3.7288341522216797, Loss D: 0.005033481400460005\n",
      "Epoch 187/200, Batch 1/17, Loss G: 3.7891488075256348, Loss D: 0.009455463849008083\n",
      "Epoch 187/200, Batch 2/17, Loss G: 3.884775161743164, Loss D: 0.012417290359735489\n",
      "Epoch 187/200, Batch 3/17, Loss G: 3.7802109718322754, Loss D: 0.006073836702853441\n",
      "Epoch 187/200, Batch 4/17, Loss G: 3.852386951446533, Loss D: 0.003215703647583723\n",
      "Epoch 187/200, Batch 5/17, Loss G: 3.6728036403656006, Loss D: 0.005521389655768871\n",
      "Epoch 187/200, Batch 6/17, Loss G: 3.6628332138061523, Loss D: 0.009834780357778072\n",
      "Epoch 187/200, Batch 7/17, Loss G: 3.7483201026916504, Loss D: 0.002497067442163825\n",
      "Epoch 187/200, Batch 8/17, Loss G: 3.6188254356384277, Loss D: 0.00546686165034771\n",
      "Epoch 187/200, Batch 9/17, Loss G: 3.577098846435547, Loss D: 0.006275946740061045\n",
      "Epoch 187/200, Batch 10/17, Loss G: 3.7753987312316895, Loss D: 0.004440673626959324\n",
      "Epoch 187/200, Batch 11/17, Loss G: 3.668677806854248, Loss D: 0.008046369068324566\n",
      "Epoch 187/200, Batch 12/17, Loss G: 3.5043272972106934, Loss D: 0.005015388131141663\n",
      "Epoch 187/200, Batch 13/17, Loss G: 3.62587833404541, Loss D: 0.006883649155497551\n",
      "Epoch 187/200, Batch 14/17, Loss G: 3.716425895690918, Loss D: 0.0032954891212284565\n",
      "Epoch 187/200, Batch 15/17, Loss G: 3.727849006652832, Loss D: 0.004969776142388582\n",
      "Epoch 187/200, Batch 16/17, Loss G: 3.7051119804382324, Loss D: 0.0056762718595564365\n",
      "Epoch 188/200, Batch 0/17, Loss G: 3.6184451580047607, Loss D: 0.009381131269037724\n",
      "Epoch 188/200, Batch 1/17, Loss G: 3.779442310333252, Loss D: 0.004813049919903278\n",
      "Epoch 188/200, Batch 2/17, Loss G: 3.8320295810699463, Loss D: 0.011643201112747192\n",
      "Epoch 188/200, Batch 3/17, Loss G: 3.581392288208008, Loss D: 0.0039844755083322525\n",
      "Epoch 188/200, Batch 4/17, Loss G: 3.593778133392334, Loss D: 0.004935878328979015\n",
      "Epoch 188/200, Batch 5/17, Loss G: 3.6057491302490234, Loss D: 0.0038422178477048874\n",
      "Epoch 188/200, Batch 6/17, Loss G: 3.648526191711426, Loss D: 0.0038201273418962955\n",
      "Epoch 188/200, Batch 7/17, Loss G: 3.8484745025634766, Loss D: 0.0036663287319242954\n",
      "Epoch 188/200, Batch 8/17, Loss G: 3.7637579441070557, Loss D: 0.0035586394369602203\n",
      "Epoch 188/200, Batch 9/17, Loss G: 3.7652368545532227, Loss D: 0.0049604382365942\n",
      "Epoch 188/200, Batch 10/17, Loss G: 3.7552945613861084, Loss D: 0.0033246749080717564\n",
      "Epoch 188/200, Batch 11/17, Loss G: 3.753577947616577, Loss D: 0.00586997764185071\n",
      "Epoch 188/200, Batch 12/17, Loss G: 3.6771035194396973, Loss D: 0.005408634431660175\n",
      "Epoch 188/200, Batch 13/17, Loss G: 3.7562732696533203, Loss D: 0.003887950675562024\n",
      "Epoch 188/200, Batch 14/17, Loss G: 3.649519920349121, Loss D: 0.003938286565244198\n",
      "Epoch 188/200, Batch 15/17, Loss G: 3.8729419708251953, Loss D: 0.004339223727583885\n",
      "Epoch 188/200, Batch 16/17, Loss G: 3.8473973274230957, Loss D: 0.003560299053788185\n",
      "Epoch 189/200, Batch 0/17, Loss G: 3.801332950592041, Loss D: 0.009583648294210434\n",
      "Epoch 189/200, Batch 1/17, Loss G: 3.635817289352417, Loss D: 0.006056775338947773\n",
      "Epoch 189/200, Batch 2/17, Loss G: 3.544222354888916, Loss D: 0.006812753155827522\n",
      "Epoch 189/200, Batch 3/17, Loss G: 3.821547031402588, Loss D: 0.0026512439362704754\n",
      "Epoch 189/200, Batch 4/17, Loss G: 3.8267600536346436, Loss D: 0.009556041099131107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200, Batch 5/17, Loss G: 3.691549777984619, Loss D: 0.004260813817381859\n",
      "Epoch 189/200, Batch 6/17, Loss G: 3.703552722930908, Loss D: 0.0032613256480544806\n",
      "Epoch 189/200, Batch 7/17, Loss G: 3.785841941833496, Loss D: 0.005295287352055311\n",
      "Epoch 189/200, Batch 8/17, Loss G: 3.7671422958374023, Loss D: 0.005132950842380524\n",
      "Epoch 189/200, Batch 9/17, Loss G: 3.5622129440307617, Loss D: 0.005456502083688974\n",
      "Epoch 189/200, Batch 10/17, Loss G: 3.8470940589904785, Loss D: 0.003614844288676977\n",
      "Epoch 189/200, Batch 11/17, Loss G: 3.72946834564209, Loss D: 0.005668548867106438\n",
      "Epoch 189/200, Batch 12/17, Loss G: 3.797616958618164, Loss D: 0.0036944400053471327\n",
      "Epoch 189/200, Batch 13/17, Loss G: 3.8892579078674316, Loss D: 0.002014534082263708\n",
      "Epoch 189/200, Batch 14/17, Loss G: 3.7329931259155273, Loss D: 0.010457883588969707\n",
      "Epoch 189/200, Batch 15/17, Loss G: 3.5214200019836426, Loss D: 0.010583865456283092\n",
      "Epoch 189/200, Batch 16/17, Loss G: 3.6580686569213867, Loss D: 0.008640101179480553\n",
      "Epoch 190/200, Batch 0/17, Loss G: 3.5760602951049805, Loss D: 0.028596067801117897\n",
      "Epoch 190/200, Batch 1/17, Loss G: 3.9078245162963867, Loss D: 0.01977681554853916\n",
      "Epoch 190/200, Batch 2/17, Loss G: 3.6442744731903076, Loss D: 0.005683070048689842\n",
      "Epoch 190/200, Batch 3/17, Loss G: 3.7869532108306885, Loss D: 0.003544755280017853\n",
      "Epoch 190/200, Batch 4/17, Loss G: 3.493265151977539, Loss D: 0.010861232876777649\n",
      "Epoch 190/200, Batch 5/17, Loss G: 3.7112913131713867, Loss D: 0.006699655670672655\n",
      "Epoch 190/200, Batch 6/17, Loss G: 3.720569610595703, Loss D: 0.013797140680253506\n",
      "Epoch 190/200, Batch 7/17, Loss G: 3.6176600456237793, Loss D: 0.013341858983039856\n",
      "Epoch 190/200, Batch 8/17, Loss G: 3.784761905670166, Loss D: 0.006686396896839142\n",
      "Epoch 190/200, Batch 9/17, Loss G: 3.74851131439209, Loss D: 0.014126425608992577\n",
      "Epoch 190/200, Batch 10/17, Loss G: 3.636874198913574, Loss D: 0.03512631356716156\n",
      "Epoch 190/200, Batch 11/17, Loss G: 3.9421987533569336, Loss D: 0.04130776599049568\n",
      "Epoch 190/200, Batch 12/17, Loss G: 3.683861017227173, Loss D: 0.016898179426789284\n",
      "Epoch 190/200, Batch 13/17, Loss G: 3.6916186809539795, Loss D: 0.011974379420280457\n",
      "Epoch 190/200, Batch 14/17, Loss G: 3.7079343795776367, Loss D: 0.013805408030748367\n",
      "Epoch 190/200, Batch 15/17, Loss G: 3.7414979934692383, Loss D: 0.01876670867204666\n",
      "Epoch 190/200, Batch 16/17, Loss G: 3.576841354370117, Loss D: 0.020582785829901695\n",
      "Epoch 191/200, Batch 0/17, Loss G: 3.8276352882385254, Loss D: 0.009242497384548187\n",
      "Epoch 191/200, Batch 1/17, Loss G: 3.713085174560547, Loss D: 0.007969843223690987\n",
      "Epoch 191/200, Batch 2/17, Loss G: 3.724514961242676, Loss D: 0.009877784177660942\n",
      "Epoch 191/200, Batch 3/17, Loss G: 3.7935280799865723, Loss D: 0.005357273854315281\n",
      "Epoch 191/200, Batch 4/17, Loss G: 3.842050313949585, Loss D: 0.004902061074972153\n",
      "Epoch 191/200, Batch 5/17, Loss G: 3.8129067420959473, Loss D: 0.006930485367774963\n",
      "Epoch 191/200, Batch 6/17, Loss G: 3.6356635093688965, Loss D: 0.008484680205583572\n",
      "Epoch 191/200, Batch 7/17, Loss G: 3.7738468647003174, Loss D: 0.006526533048599958\n",
      "Epoch 191/200, Batch 8/17, Loss G: 3.9492878913879395, Loss D: 0.0036487686447799206\n",
      "Epoch 191/200, Batch 9/17, Loss G: 3.781121253967285, Loss D: 0.0068505131639540195\n",
      "Epoch 191/200, Batch 10/17, Loss G: 3.715229034423828, Loss D: 0.004588960204273462\n",
      "Epoch 191/200, Batch 11/17, Loss G: 3.675579071044922, Loss D: 0.007655265275388956\n",
      "Epoch 191/200, Batch 12/17, Loss G: 3.5747807025909424, Loss D: 0.012053974904119968\n",
      "Epoch 191/200, Batch 13/17, Loss G: 3.7802066802978516, Loss D: 0.007729946170002222\n",
      "Epoch 191/200, Batch 14/17, Loss G: 3.6525015830993652, Loss D: 0.0047276318073272705\n",
      "Epoch 191/200, Batch 15/17, Loss G: 3.6265645027160645, Loss D: 0.006907330360263586\n",
      "Epoch 191/200, Batch 16/17, Loss G: 3.9109950065612793, Loss D: 0.003973579965531826\n",
      "Epoch 192/200, Batch 0/17, Loss G: 3.68302059173584, Loss D: 0.004792831838130951\n",
      "Epoch 192/200, Batch 1/17, Loss G: 3.699939250946045, Loss D: 0.004321806598454714\n",
      "Epoch 192/200, Batch 2/17, Loss G: 3.632723808288574, Loss D: 0.01260670181363821\n",
      "Epoch 192/200, Batch 3/17, Loss G: 3.5882821083068848, Loss D: 0.021764803677797318\n",
      "Epoch 192/200, Batch 4/17, Loss G: 3.7966322898864746, Loss D: 0.015357133001089096\n",
      "Epoch 192/200, Batch 5/17, Loss G: 3.5895631313323975, Loss D: 0.00605509290471673\n",
      "Epoch 192/200, Batch 6/17, Loss G: 3.5571932792663574, Loss D: 0.01294702384620905\n",
      "Epoch 192/200, Batch 7/17, Loss G: 3.8836464881896973, Loss D: 0.038082022219896317\n",
      "Epoch 192/200, Batch 8/17, Loss G: 3.2664685249328613, Loss D: 0.08058173954486847\n",
      "Epoch 192/200, Batch 9/17, Loss G: 4.064914226531982, Loss D: 0.06197482720017433\n",
      "Epoch 192/200, Batch 10/17, Loss G: 3.758676290512085, Loss D: 0.004379840102046728\n",
      "Epoch 192/200, Batch 11/17, Loss G: 3.685070037841797, Loss D: 0.02937440760433674\n",
      "Epoch 192/200, Batch 12/17, Loss G: 3.6960806846618652, Loss D: 0.02822098135948181\n",
      "Epoch 192/200, Batch 13/17, Loss G: 3.897620677947998, Loss D: 0.11553607881069183\n",
      "Epoch 192/200, Batch 14/17, Loss G: 3.2801167964935303, Loss D: 0.12099797278642654\n",
      "Epoch 192/200, Batch 15/17, Loss G: 3.750804901123047, Loss D: 0.08202821016311646\n",
      "Epoch 192/200, Batch 16/17, Loss G: 3.3314547538757324, Loss D: 0.107277512550354\n",
      "Epoch 193/200, Batch 0/17, Loss G: 3.5667123794555664, Loss D: 0.06761777400970459\n",
      "Epoch 193/200, Batch 1/17, Loss G: 3.815049409866333, Loss D: 0.24726302921772003\n",
      "Epoch 193/200, Batch 2/17, Loss G: 3.1162962913513184, Loss D: 0.20468705892562866\n",
      "Epoch 193/200, Batch 3/17, Loss G: 3.5966315269470215, Loss D: 0.09306131303310394\n",
      "Epoch 193/200, Batch 4/17, Loss G: 3.3363070487976074, Loss D: 0.07983645796775818\n",
      "Epoch 193/200, Batch 5/17, Loss G: 3.606668472290039, Loss D: 0.06568530946969986\n",
      "Epoch 193/200, Batch 6/17, Loss G: 3.5949559211730957, Loss D: 0.0688171237707138\n",
      "Epoch 193/200, Batch 7/17, Loss G: 3.7992610931396484, Loss D: 0.13113343715667725\n",
      "Epoch 193/200, Batch 8/17, Loss G: 3.2322497367858887, Loss D: 0.1471939980983734\n",
      "Epoch 193/200, Batch 9/17, Loss G: 3.6722800731658936, Loss D: 0.07950998842716217\n",
      "Epoch 193/200, Batch 10/17, Loss G: 3.4860305786132812, Loss D: 0.06018534302711487\n",
      "Epoch 193/200, Batch 11/17, Loss G: 3.8975772857666016, Loss D: 0.041989974677562714\n",
      "Epoch 193/200, Batch 12/17, Loss G: 3.901935577392578, Loss D: 0.04581509530544281\n",
      "Epoch 193/200, Batch 13/17, Loss G: 3.5595779418945312, Loss D: 0.053758926689624786\n",
      "Epoch 193/200, Batch 14/17, Loss G: 3.723644733428955, Loss D: 0.06726144254207611\n",
      "Epoch 193/200, Batch 15/17, Loss G: 3.518355369567871, Loss D: 0.09452658891677856\n",
      "Epoch 193/200, Batch 16/17, Loss G: 3.9880616664886475, Loss D: 0.07371258735656738\n",
      "Epoch 194/200, Batch 0/17, Loss G: 3.4342308044433594, Loss D: 0.1852247565984726\n",
      "Epoch 194/200, Batch 1/17, Loss G: 3.562354564666748, Loss D: 0.16518960893154144\n",
      "Epoch 194/200, Batch 2/17, Loss G: 3.7679994106292725, Loss D: 0.17783355712890625\n",
      "Epoch 194/200, Batch 3/17, Loss G: 3.596682071685791, Loss D: 0.10206396877765656\n",
      "Epoch 194/200, Batch 4/17, Loss G: 3.6642227172851562, Loss D: 0.0794040709733963\n",
      "Epoch 194/200, Batch 5/17, Loss G: 3.367304563522339, Loss D: 0.07866423577070236\n",
      "Epoch 194/200, Batch 6/17, Loss G: 3.709679126739502, Loss D: 0.05774397403001785\n",
      "Epoch 194/200, Batch 7/17, Loss G: 3.5350852012634277, Loss D: 0.05809587240219116\n",
      "Epoch 194/200, Batch 8/17, Loss G: 3.372525215148926, Loss D: 0.10116229206323624\n",
      "Epoch 194/200, Batch 9/17, Loss G: 3.8808586597442627, Loss D: 0.04930786043405533\n",
      "Epoch 194/200, Batch 10/17, Loss G: 3.5517358779907227, Loss D: 0.05891234427690506\n",
      "Epoch 194/200, Batch 11/17, Loss G: 3.621184825897217, Loss D: 0.0560179241001606\n",
      "Epoch 194/200, Batch 12/17, Loss G: 3.7784128189086914, Loss D: 0.02727513387799263\n",
      "Epoch 194/200, Batch 13/17, Loss G: 3.670131206512451, Loss D: 0.034755729138851166\n",
      "Epoch 194/200, Batch 14/17, Loss G: 3.6219820976257324, Loss D: 0.029437581077218056\n",
      "Epoch 194/200, Batch 15/17, Loss G: 3.584566831588745, Loss D: 0.020966697484254837\n",
      "Epoch 194/200, Batch 16/17, Loss G: 3.5369343757629395, Loss D: 0.04331788420677185\n",
      "Epoch 195/200, Batch 0/17, Loss G: 3.167531728744507, Loss D: 0.11672987788915634\n",
      "Epoch 195/200, Batch 1/17, Loss G: 3.635202169418335, Loss D: 0.102238729596138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/200, Batch 2/17, Loss G: 3.6004881858825684, Loss D: 0.0330461747944355\n",
      "Epoch 195/200, Batch 3/17, Loss G: 3.4497833251953125, Loss D: 0.04374327138066292\n",
      "Epoch 195/200, Batch 4/17, Loss G: 3.646284818649292, Loss D: 0.06993553042411804\n",
      "Epoch 195/200, Batch 5/17, Loss G: 3.5866384506225586, Loss D: 0.050352275371551514\n",
      "Epoch 195/200, Batch 6/17, Loss G: 3.767062187194824, Loss D: 0.021037638187408447\n",
      "Epoch 195/200, Batch 7/17, Loss G: 3.8171892166137695, Loss D: 0.04860841855406761\n",
      "Epoch 195/200, Batch 8/17, Loss G: 3.5148725509643555, Loss D: 0.04315735772252083\n",
      "Epoch 195/200, Batch 9/17, Loss G: 3.8933544158935547, Loss D: 0.016064129769802094\n",
      "Epoch 195/200, Batch 10/17, Loss G: 3.7552223205566406, Loss D: 0.03129364177584648\n",
      "Epoch 195/200, Batch 11/17, Loss G: 3.652827262878418, Loss D: 0.031634438782930374\n",
      "Epoch 195/200, Batch 12/17, Loss G: 3.7642710208892822, Loss D: 0.019642436876893044\n",
      "Epoch 195/200, Batch 13/17, Loss G: 3.7589125633239746, Loss D: 0.024543818086385727\n",
      "Epoch 195/200, Batch 14/17, Loss G: 3.456908702850342, Loss D: 0.0164061039686203\n",
      "Epoch 195/200, Batch 15/17, Loss G: 3.706801414489746, Loss D: 0.014910581521689892\n",
      "Epoch 195/200, Batch 16/17, Loss G: 3.8438758850097656, Loss D: 0.014767415821552277\n",
      "Epoch 196/200, Batch 0/17, Loss G: 3.653268814086914, Loss D: 0.011628624051809311\n",
      "Epoch 196/200, Batch 1/17, Loss G: 3.697880268096924, Loss D: 0.01604986935853958\n",
      "Epoch 196/200, Batch 2/17, Loss G: 3.667968511581421, Loss D: 0.005975145380944014\n",
      "Epoch 196/200, Batch 3/17, Loss G: 3.64062762260437, Loss D: 0.00796500500291586\n",
      "Epoch 196/200, Batch 4/17, Loss G: 3.5371289253234863, Loss D: 0.007175533100962639\n",
      "Epoch 196/200, Batch 5/17, Loss G: 3.625852108001709, Loss D: 0.009836566634476185\n",
      "Epoch 196/200, Batch 6/17, Loss G: 3.6387155055999756, Loss D: 0.0081822844222188\n",
      "Epoch 196/200, Batch 7/17, Loss G: 3.5118045806884766, Loss D: 0.01408378779888153\n",
      "Epoch 196/200, Batch 8/17, Loss G: 3.7605981826782227, Loss D: 0.009623091667890549\n",
      "Epoch 196/200, Batch 9/17, Loss G: 3.773005485534668, Loss D: 0.010023594833910465\n",
      "Epoch 196/200, Batch 10/17, Loss G: 3.6576929092407227, Loss D: 0.020297473296523094\n",
      "Epoch 196/200, Batch 11/17, Loss G: 3.790003538131714, Loss D: 0.01097662653774023\n",
      "Epoch 196/200, Batch 12/17, Loss G: 3.655827283859253, Loss D: 0.019587699323892593\n",
      "Epoch 196/200, Batch 13/17, Loss G: 3.694030284881592, Loss D: 0.020059939473867416\n",
      "Epoch 196/200, Batch 14/17, Loss G: 3.763209819793701, Loss D: 0.007053151726722717\n",
      "Epoch 196/200, Batch 15/17, Loss G: 3.63789701461792, Loss D: 0.010876288637518883\n",
      "Epoch 196/200, Batch 16/17, Loss G: 3.863646984100342, Loss D: 0.018891919404268265\n",
      "Epoch 197/200, Batch 0/17, Loss G: 3.3851208686828613, Loss D: 0.05049634352326393\n",
      "Epoch 197/200, Batch 1/17, Loss G: 3.8103270530700684, Loss D: 0.05062468349933624\n",
      "Epoch 197/200, Batch 2/17, Loss G: 3.5915615558624268, Loss D: 0.014085059054195881\n",
      "Epoch 197/200, Batch 3/17, Loss G: 3.603184223175049, Loss D: 0.0207569170743227\n",
      "Epoch 197/200, Batch 4/17, Loss G: 3.8648109436035156, Loss D: 0.025657564401626587\n",
      "Epoch 197/200, Batch 5/17, Loss G: 3.556612014770508, Loss D: 0.011709638871252537\n",
      "Epoch 197/200, Batch 6/17, Loss G: 3.706869602203369, Loss D: 0.012555922381579876\n",
      "Epoch 197/200, Batch 7/17, Loss G: 3.698791742324829, Loss D: 0.01511387899518013\n",
      "Epoch 197/200, Batch 8/17, Loss G: 3.5845441818237305, Loss D: 0.010290698148310184\n",
      "Epoch 197/200, Batch 9/17, Loss G: 3.6049716472625732, Loss D: 0.01244354248046875\n",
      "Epoch 197/200, Batch 10/17, Loss G: 3.7364132404327393, Loss D: 0.008839281275868416\n",
      "Epoch 197/200, Batch 11/17, Loss G: 3.5914456844329834, Loss D: 0.010481276549398899\n",
      "Epoch 197/200, Batch 12/17, Loss G: 3.6078898906707764, Loss D: 0.009805632755160332\n",
      "Epoch 197/200, Batch 13/17, Loss G: 3.672299385070801, Loss D: 0.00749328825622797\n",
      "Epoch 197/200, Batch 14/17, Loss G: 3.572133779525757, Loss D: 0.009057686664164066\n",
      "Epoch 197/200, Batch 15/17, Loss G: 3.752138614654541, Loss D: 0.009469588287174702\n",
      "Epoch 197/200, Batch 16/17, Loss G: 3.660895347595215, Loss D: 0.009254846721887589\n",
      "Epoch 198/200, Batch 0/17, Loss G: 3.460313320159912, Loss D: 0.017164528369903564\n",
      "Epoch 198/200, Batch 1/17, Loss G: 3.682906150817871, Loss D: 0.010853037238121033\n",
      "Epoch 198/200, Batch 2/17, Loss G: 3.782632827758789, Loss D: 0.009810101240873337\n",
      "Epoch 198/200, Batch 3/17, Loss G: 3.6280109882354736, Loss D: 0.009128281846642494\n",
      "Epoch 198/200, Batch 4/17, Loss G: 3.5028672218322754, Loss D: 0.012101976200938225\n",
      "Epoch 198/200, Batch 5/17, Loss G: 3.7932658195495605, Loss D: 0.006964379921555519\n",
      "Epoch 198/200, Batch 6/17, Loss G: 3.726226568222046, Loss D: 0.008492357097566128\n",
      "Epoch 198/200, Batch 7/17, Loss G: 3.838391065597534, Loss D: 0.008500833995640278\n",
      "Epoch 198/200, Batch 8/17, Loss G: 3.576380491256714, Loss D: 0.008871855214238167\n",
      "Epoch 198/200, Batch 9/17, Loss G: 3.7218222618103027, Loss D: 0.0043980819173157215\n",
      "Epoch 198/200, Batch 10/17, Loss G: 3.818533420562744, Loss D: 0.005366576835513115\n",
      "Epoch 198/200, Batch 11/17, Loss G: 3.589233875274658, Loss D: 0.005478798411786556\n",
      "Epoch 198/200, Batch 12/17, Loss G: 3.62005615234375, Loss D: 0.010704844258725643\n",
      "Epoch 198/200, Batch 13/17, Loss G: 3.632988929748535, Loss D: 0.0039962055161595345\n",
      "Epoch 198/200, Batch 14/17, Loss G: 3.599062919616699, Loss D: 0.005658783949911594\n",
      "Epoch 198/200, Batch 15/17, Loss G: 3.6224236488342285, Loss D: 0.007293052971363068\n",
      "Epoch 198/200, Batch 16/17, Loss G: 3.670896530151367, Loss D: 0.002755324821919203\n",
      "Epoch 199/200, Batch 0/17, Loss G: 3.6540231704711914, Loss D: 0.005376769229769707\n",
      "Epoch 199/200, Batch 1/17, Loss G: 3.7434732913970947, Loss D: 0.007279079873114824\n",
      "Epoch 199/200, Batch 2/17, Loss G: 3.6331753730773926, Loss D: 0.005472993943840265\n",
      "Epoch 199/200, Batch 3/17, Loss G: 3.6593339443206787, Loss D: 0.005483895540237427\n",
      "Epoch 199/200, Batch 4/17, Loss G: 3.756317615509033, Loss D: 0.006769938860088587\n",
      "Epoch 199/200, Batch 5/17, Loss G: 3.6423041820526123, Loss D: 0.003962377086281776\n",
      "Epoch 199/200, Batch 6/17, Loss G: 3.5508065223693848, Loss D: 0.004523350857198238\n",
      "Epoch 199/200, Batch 7/17, Loss G: 3.827226161956787, Loss D: 0.0045136334374547005\n",
      "Epoch 199/200, Batch 8/17, Loss G: 3.6135706901550293, Loss D: 0.003906655590981245\n",
      "Epoch 199/200, Batch 9/17, Loss G: 3.7196104526519775, Loss D: 0.0034276044461876154\n",
      "Epoch 199/200, Batch 10/17, Loss G: 3.5994768142700195, Loss D: 0.0024148006923496723\n",
      "Epoch 199/200, Batch 11/17, Loss G: 3.81354022026062, Loss D: 0.0030755531042814255\n",
      "Epoch 199/200, Batch 12/17, Loss G: 3.5778238773345947, Loss D: 0.003638505470007658\n",
      "Epoch 199/200, Batch 13/17, Loss G: 3.6601626873016357, Loss D: 0.00470343604683876\n",
      "Epoch 199/200, Batch 14/17, Loss G: 3.7740697860717773, Loss D: 0.006081723142415285\n",
      "Epoch 199/200, Batch 15/17, Loss G: 3.6560492515563965, Loss D: 0.0015141490148380399\n",
      "Epoch 199/200, Batch 16/17, Loss G: 3.640310525894165, Loss D: 0.00394169706851244\n",
      "Epoch 200/200, Batch 0/17, Loss G: 3.7640256881713867, Loss D: 0.003941351547837257\n",
      "Epoch 200/200, Batch 1/17, Loss G: 3.7425689697265625, Loss D: 0.008722572587430477\n",
      "Epoch 200/200, Batch 2/17, Loss G: 3.646129608154297, Loss D: 0.003507667686790228\n",
      "Epoch 200/200, Batch 3/17, Loss G: 3.553284168243408, Loss D: 0.015344641171395779\n",
      "Epoch 200/200, Batch 4/17, Loss G: 3.875714063644409, Loss D: 0.00765544269233942\n",
      "Epoch 200/200, Batch 5/17, Loss G: 3.6509361267089844, Loss D: 0.01241332571953535\n",
      "Epoch 200/200, Batch 6/17, Loss G: 3.65585994720459, Loss D: 0.0029691129457205534\n",
      "Epoch 200/200, Batch 7/17, Loss G: 3.5076279640197754, Loss D: 0.016125096008181572\n",
      "Epoch 200/200, Batch 8/17, Loss G: 3.782691240310669, Loss D: 0.005611793603748083\n",
      "Epoch 200/200, Batch 9/17, Loss G: 3.5300660133361816, Loss D: 0.026361575350165367\n",
      "Epoch 200/200, Batch 10/17, Loss G: 3.6426353454589844, Loss D: 0.01878943480551243\n",
      "Epoch 200/200, Batch 11/17, Loss G: 3.6464552879333496, Loss D: 0.00800070445984602\n",
      "Epoch 200/200, Batch 12/17, Loss G: 3.7186968326568604, Loss D: 0.004132611211389303\n",
      "Epoch 200/200, Batch 13/17, Loss G: 3.630988836288452, Loss D: 0.008296099491417408\n",
      "Epoch 200/200, Batch 14/17, Loss G: 3.6557183265686035, Loss D: 0.004529810976237059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Batch 15/17, Loss G: 3.4882116317749023, Loss D: 0.006417832802981138\n",
      "Epoch 200/200, Batch 16/17, Loss G: 3.775808334350586, Loss D: 0.0021065950859338045\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = (256, 256)\n",
    "batch_size = 8\n",
    "num_epochs = 200\n",
    "\n",
    "bw_folder = \"black_and_white\"\n",
    "color_folder = \"colored\"\n",
    "\n",
    "train_loader = load_dataset(bw_folder, color_folder, image_size, batch_size)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "train(generator, discriminator, train_loader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d65423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the path to the user's desktop\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "\n",
    "# Save the weights to the desktop\n",
    "weights_path = os.path.join(desktop_path, \"saved_weights5.pth\")\n",
    "torch.save(generator.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98e16e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the trained model\n",
    "generator = Generator()\n",
    "weights_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"saved_weights5.pth\")\n",
    "generator.load_state_dict(torch.load(weights_path))\n",
    "generator.eval()\n",
    "\n",
    "# Preprocess the test image\n",
    "test_image_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\" ,\"onePieceColoring\",\"black_and_white\", \"image_132.jpg\")\n",
    "image_size = (256, 256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "])\n",
    "test_image = Image.open(test_image_path)\n",
    "input_image = transform(test_image).unsqueeze(0)\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    colorized_image = generator(input_image)\n",
    "\n",
    "# Postprocess the colorized image\n",
    "colorized_image = colorized_image.squeeze(0).permute(1, 2, 0)\n",
    "colorized_image = (colorized_image + 1) / 2  # Denormalize the image\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "colorized_image_np = colorized_image.cpu().numpy()\n",
    "\n",
    "# Convert the numpy array to a PIL Image\n",
    "colorized_image_pil = Image.fromarray((255 * colorized_image_np).astype(np.uint8))\n",
    "\n",
    "# Save the colorized image\n",
    "save_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"colored_by_model5.jpg\")\n",
    "colorized_image_pil.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
