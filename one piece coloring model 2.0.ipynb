{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874eda56",
   "metadata": {},
   "source": [
    "# ChatGPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037172a7",
   "metadata": {},
   "source": [
    "# PERPLEXITY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db210671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Batch 0/17, Loss G: 13.144043922424316, Loss D: 0.2546835243701935\n",
      "Epoch 1/200, Batch 1/17, Loss G: 12.996423721313477, Loss D: 0.23017817735671997\n",
      "Epoch 1/200, Batch 2/17, Loss G: 12.391427993774414, Loss D: 0.2005060762166977\n",
      "Epoch 1/200, Batch 3/17, Loss G: 11.514909744262695, Loss D: 0.17453822493553162\n",
      "Epoch 1/200, Batch 4/17, Loss G: 10.995420455932617, Loss D: 0.17776291072368622\n",
      "Epoch 1/200, Batch 5/17, Loss G: 10.745889663696289, Loss D: 0.21635034680366516\n",
      "Epoch 1/200, Batch 6/17, Loss G: 10.020939826965332, Loss D: 0.17553022503852844\n",
      "Epoch 1/200, Batch 7/17, Loss G: 9.9434814453125, Loss D: 0.17866140604019165\n",
      "Epoch 1/200, Batch 8/17, Loss G: 9.279485702514648, Loss D: 0.16081073880195618\n",
      "Epoch 1/200, Batch 9/17, Loss G: 9.317586898803711, Loss D: 0.13852867484092712\n",
      "Epoch 1/200, Batch 10/17, Loss G: 9.077943801879883, Loss D: 0.1669844090938568\n",
      "Epoch 1/200, Batch 11/17, Loss G: 8.874163627624512, Loss D: 0.15416814386844635\n",
      "Epoch 1/200, Batch 12/17, Loss G: 8.692224502563477, Loss D: 0.12594887614250183\n",
      "Epoch 1/200, Batch 13/17, Loss G: 8.770970344543457, Loss D: 0.12851351499557495\n",
      "Epoch 1/200, Batch 14/17, Loss G: 8.428114891052246, Loss D: 0.11430522799491882\n",
      "Epoch 1/200, Batch 15/17, Loss G: 8.564159393310547, Loss D: 0.10765489190816879\n",
      "Epoch 1/200, Batch 16/17, Loss G: 8.365440368652344, Loss D: 0.13897249102592468\n",
      "Epoch 2/200, Batch 0/17, Loss G: 8.156161308288574, Loss D: 0.11499050259590149\n",
      "Epoch 2/200, Batch 1/17, Loss G: 8.008038520812988, Loss D: 0.15790420770645142\n",
      "Epoch 2/200, Batch 2/17, Loss G: 8.164392471313477, Loss D: 0.13538630306720734\n",
      "Epoch 2/200, Batch 3/17, Loss G: 8.11505126953125, Loss D: 0.15246254205703735\n",
      "Epoch 2/200, Batch 4/17, Loss G: 8.402233123779297, Loss D: 0.11554306745529175\n",
      "Epoch 2/200, Batch 5/17, Loss G: 7.8433732986450195, Loss D: 0.12096245586872101\n",
      "Epoch 2/200, Batch 6/17, Loss G: 7.957101345062256, Loss D: 0.11503659188747406\n",
      "Epoch 2/200, Batch 7/17, Loss G: 7.880724906921387, Loss D: 0.11226123571395874\n",
      "Epoch 2/200, Batch 8/17, Loss G: 8.04124641418457, Loss D: 0.150173157453537\n",
      "Epoch 2/200, Batch 9/17, Loss G: 7.786961078643799, Loss D: 0.12248343229293823\n",
      "Epoch 2/200, Batch 10/17, Loss G: 7.952263355255127, Loss D: 0.15895108878612518\n",
      "Epoch 2/200, Batch 11/17, Loss G: 7.548679828643799, Loss D: 0.1570095419883728\n",
      "Epoch 2/200, Batch 12/17, Loss G: 7.522865295410156, Loss D: 0.10605195164680481\n",
      "Epoch 2/200, Batch 13/17, Loss G: 7.586904048919678, Loss D: 0.11709362268447876\n",
      "Epoch 2/200, Batch 14/17, Loss G: 7.288858413696289, Loss D: 0.1327105164527893\n",
      "Epoch 2/200, Batch 15/17, Loss G: 7.522672653198242, Loss D: 0.13959167897701263\n",
      "Epoch 2/200, Batch 16/17, Loss G: 7.288926124572754, Loss D: 0.17735755443572998\n",
      "Epoch 3/200, Batch 0/17, Loss G: 7.9132771492004395, Loss D: 0.2330773025751114\n",
      "Epoch 3/200, Batch 1/17, Loss G: 7.688388824462891, Loss D: 0.1106702908873558\n",
      "Epoch 3/200, Batch 2/17, Loss G: 7.320505142211914, Loss D: 0.18824373185634613\n",
      "Epoch 3/200, Batch 3/17, Loss G: 7.489132881164551, Loss D: 0.13300122320652008\n",
      "Epoch 3/200, Batch 4/17, Loss G: 7.381607532501221, Loss D: 0.13131862878799438\n",
      "Epoch 3/200, Batch 5/17, Loss G: 7.168724060058594, Loss D: 0.1326281577348709\n",
      "Epoch 3/200, Batch 6/17, Loss G: 7.357743740081787, Loss D: 0.11720601469278336\n",
      "Epoch 3/200, Batch 7/17, Loss G: 7.3598127365112305, Loss D: 0.16866318881511688\n",
      "Epoch 3/200, Batch 8/17, Loss G: 7.299777984619141, Loss D: 0.12185913324356079\n",
      "Epoch 3/200, Batch 9/17, Loss G: 7.4251389503479, Loss D: 0.1076190397143364\n",
      "Epoch 3/200, Batch 10/17, Loss G: 7.57389497756958, Loss D: 0.12812215089797974\n",
      "Epoch 3/200, Batch 11/17, Loss G: 7.155854225158691, Loss D: 0.11693869531154633\n",
      "Epoch 3/200, Batch 12/17, Loss G: 7.144493103027344, Loss D: 0.14675623178482056\n",
      "Epoch 3/200, Batch 13/17, Loss G: 7.116606712341309, Loss D: 0.12816599011421204\n",
      "Epoch 3/200, Batch 14/17, Loss G: 6.924720287322998, Loss D: 0.12140531092882156\n",
      "Epoch 3/200, Batch 15/17, Loss G: 6.987846374511719, Loss D: 0.18139725923538208\n",
      "Epoch 3/200, Batch 16/17, Loss G: 7.211759567260742, Loss D: 0.183214470744133\n",
      "Epoch 4/200, Batch 0/17, Loss G: 6.966578006744385, Loss D: 0.13293889164924622\n",
      "Epoch 4/200, Batch 1/17, Loss G: 6.993887901306152, Loss D: 0.11034508794546127\n",
      "Epoch 4/200, Batch 2/17, Loss G: 7.329283714294434, Loss D: 0.12635768949985504\n",
      "Epoch 4/200, Batch 3/17, Loss G: 7.158193588256836, Loss D: 0.0827912837266922\n",
      "Epoch 4/200, Batch 4/17, Loss G: 6.951511383056641, Loss D: 0.09613803774118423\n",
      "Epoch 4/200, Batch 5/17, Loss G: 6.911415100097656, Loss D: 0.12915530800819397\n",
      "Epoch 4/200, Batch 6/17, Loss G: 6.989877700805664, Loss D: 0.1772959977388382\n",
      "Epoch 4/200, Batch 7/17, Loss G: 7.471465587615967, Loss D: 0.13966894149780273\n",
      "Epoch 4/200, Batch 8/17, Loss G: 7.343214988708496, Loss D: 0.09628413617610931\n",
      "Epoch 4/200, Batch 9/17, Loss G: 6.908407211303711, Loss D: 0.1345674693584442\n",
      "Epoch 4/200, Batch 10/17, Loss G: 6.987893104553223, Loss D: 0.10613273084163666\n",
      "Epoch 4/200, Batch 11/17, Loss G: 6.957778453826904, Loss D: 0.18157169222831726\n",
      "Epoch 4/200, Batch 12/17, Loss G: 6.880027770996094, Loss D: 0.1677437126636505\n",
      "Epoch 4/200, Batch 13/17, Loss G: 7.155470848083496, Loss D: 0.1460312455892563\n",
      "Epoch 4/200, Batch 14/17, Loss G: 7.313981533050537, Loss D: 0.08526815474033356\n",
      "Epoch 4/200, Batch 15/17, Loss G: 7.046093940734863, Loss D: 0.1405632495880127\n",
      "Epoch 4/200, Batch 16/17, Loss G: 6.924478054046631, Loss D: 0.2466953694820404\n",
      "Epoch 5/200, Batch 0/17, Loss G: 7.178618431091309, Loss D: 0.1187981590628624\n",
      "Epoch 5/200, Batch 1/17, Loss G: 6.710837364196777, Loss D: 0.11847319453954697\n",
      "Epoch 5/200, Batch 2/17, Loss G: 6.916365623474121, Loss D: 0.13514693081378937\n",
      "Epoch 5/200, Batch 3/17, Loss G: 6.792637825012207, Loss D: 0.14124424755573273\n",
      "Epoch 5/200, Batch 4/17, Loss G: 6.801946640014648, Loss D: 0.12250252068042755\n",
      "Epoch 5/200, Batch 5/17, Loss G: 6.848645210266113, Loss D: 0.1471986472606659\n",
      "Epoch 5/200, Batch 6/17, Loss G: 6.5746870040893555, Loss D: 0.17937423288822174\n",
      "Epoch 5/200, Batch 7/17, Loss G: 6.939476490020752, Loss D: 0.13068081438541412\n",
      "Epoch 5/200, Batch 8/17, Loss G: 6.740255355834961, Loss D: 0.13461928069591522\n",
      "Epoch 5/200, Batch 9/17, Loss G: 6.8596343994140625, Loss D: 0.11489531397819519\n",
      "Epoch 5/200, Batch 10/17, Loss G: 6.5967302322387695, Loss D: 0.08193894475698471\n",
      "Epoch 5/200, Batch 11/17, Loss G: 6.913970470428467, Loss D: 0.09851539880037308\n",
      "Epoch 5/200, Batch 12/17, Loss G: 7.216395378112793, Loss D: 0.16470597684383392\n",
      "Epoch 5/200, Batch 13/17, Loss G: 6.636014461517334, Loss D: 0.23217833042144775\n",
      "Epoch 5/200, Batch 14/17, Loss G: 7.198024272918701, Loss D: 0.0749443769454956\n",
      "Epoch 5/200, Batch 15/17, Loss G: 7.156803131103516, Loss D: 0.11336815357208252\n",
      "Epoch 5/200, Batch 16/17, Loss G: 6.606987953186035, Loss D: 0.16167931258678436\n",
      "Epoch 6/200, Batch 0/17, Loss G: 6.481569290161133, Loss D: 0.12396350502967834\n",
      "Epoch 6/200, Batch 1/17, Loss G: 7.035454750061035, Loss D: 0.12825079262256622\n",
      "Epoch 6/200, Batch 2/17, Loss G: 7.171672821044922, Loss D: 0.14605304598808289\n",
      "Epoch 6/200, Batch 3/17, Loss G: 7.1970062255859375, Loss D: 0.14292550086975098\n",
      "Epoch 6/200, Batch 4/17, Loss G: 7.022307395935059, Loss D: 0.17705166339874268\n",
      "Epoch 6/200, Batch 5/17, Loss G: 6.369715213775635, Loss D: 0.17562980949878693\n",
      "Epoch 6/200, Batch 6/17, Loss G: 6.712983131408691, Loss D: 0.1150096207857132\n",
      "Epoch 6/200, Batch 7/17, Loss G: 6.679436683654785, Loss D: 0.10725375264883041\n",
      "Epoch 6/200, Batch 8/17, Loss G: 6.7346510887146, Loss D: 0.14034990966320038\n",
      "Epoch 6/200, Batch 9/17, Loss G: 7.246085166931152, Loss D: 0.23220036923885345\n",
      "Epoch 6/200, Batch 10/17, Loss G: 7.0157575607299805, Loss D: 0.08793812990188599\n",
      "Epoch 6/200, Batch 11/17, Loss G: 6.604242324829102, Loss D: 0.15748153626918793\n",
      "Epoch 6/200, Batch 12/17, Loss G: 6.993514060974121, Loss D: 0.08677895367145538\n",
      "Epoch 6/200, Batch 13/17, Loss G: 6.938813209533691, Loss D: 0.13332098722457886\n",
      "Epoch 6/200, Batch 14/17, Loss G: 6.31126070022583, Loss D: 0.15187908709049225\n",
      "Epoch 6/200, Batch 15/17, Loss G: 6.957535743713379, Loss D: 0.13037461042404175\n",
      "Epoch 6/200, Batch 16/17, Loss G: 6.431892395019531, Loss D: 0.08635807037353516\n",
      "Epoch 7/200, Batch 0/17, Loss G: 6.692946434020996, Loss D: 0.13315001130104065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Batch 1/17, Loss G: 6.514565467834473, Loss D: 0.16086213290691376\n",
      "Epoch 7/200, Batch 2/17, Loss G: 6.63516902923584, Loss D: 0.18262633681297302\n",
      "Epoch 7/200, Batch 3/17, Loss G: 6.302554130554199, Loss D: 0.20115520060062408\n",
      "Epoch 7/200, Batch 4/17, Loss G: 6.819849014282227, Loss D: 0.1710727959871292\n",
      "Epoch 7/200, Batch 5/17, Loss G: 6.556005477905273, Loss D: 0.11634686589241028\n",
      "Epoch 7/200, Batch 6/17, Loss G: 6.711249351501465, Loss D: 0.08369147777557373\n",
      "Epoch 7/200, Batch 7/17, Loss G: 6.562498569488525, Loss D: 0.11935809999704361\n",
      "Epoch 7/200, Batch 8/17, Loss G: 6.74778938293457, Loss D: 0.1516401767730713\n",
      "Epoch 7/200, Batch 9/17, Loss G: 6.316725254058838, Loss D: 0.10963739454746246\n",
      "Epoch 7/200, Batch 10/17, Loss G: 6.536334037780762, Loss D: 0.13659654557704926\n",
      "Epoch 7/200, Batch 11/17, Loss G: 6.634660720825195, Loss D: 0.269967645406723\n",
      "Epoch 7/200, Batch 12/17, Loss G: 6.488336563110352, Loss D: 0.132921040058136\n",
      "Epoch 7/200, Batch 13/17, Loss G: 6.959465503692627, Loss D: 0.13720673322677612\n",
      "Epoch 7/200, Batch 14/17, Loss G: 6.535786151885986, Loss D: 0.14049531519412994\n",
      "Epoch 7/200, Batch 15/17, Loss G: 6.888113021850586, Loss D: 0.11626332998275757\n",
      "Epoch 7/200, Batch 16/17, Loss G: 6.464327812194824, Loss D: 0.22390174865722656\n",
      "Epoch 8/200, Batch 0/17, Loss G: 6.952329635620117, Loss D: 0.1889582872390747\n",
      "Epoch 8/200, Batch 1/17, Loss G: 6.414992332458496, Loss D: 0.06657756865024567\n",
      "Epoch 8/200, Batch 2/17, Loss G: 6.245856285095215, Loss D: 0.1263055056333542\n",
      "Epoch 8/200, Batch 3/17, Loss G: 6.421796798706055, Loss D: 0.14509180188179016\n",
      "Epoch 8/200, Batch 4/17, Loss G: 6.952341079711914, Loss D: 0.15568578243255615\n",
      "Epoch 8/200, Batch 5/17, Loss G: 6.590022087097168, Loss D: 0.08543329685926437\n",
      "Epoch 8/200, Batch 6/17, Loss G: 6.345190048217773, Loss D: 0.1366610825061798\n",
      "Epoch 8/200, Batch 7/17, Loss G: 6.657667636871338, Loss D: 0.11550324410200119\n",
      "Epoch 8/200, Batch 8/17, Loss G: 6.316164016723633, Loss D: 0.1816880702972412\n",
      "Epoch 8/200, Batch 9/17, Loss G: 6.605966091156006, Loss D: 0.14119170606136322\n",
      "Epoch 8/200, Batch 10/17, Loss G: 6.779684066772461, Loss D: 0.12697064876556396\n",
      "Epoch 8/200, Batch 11/17, Loss G: 6.843853950500488, Loss D: 0.17875194549560547\n",
      "Epoch 8/200, Batch 12/17, Loss G: 6.196321487426758, Loss D: 0.2708345055580139\n",
      "Epoch 8/200, Batch 13/17, Loss G: 6.394438743591309, Loss D: 0.12907671928405762\n",
      "Epoch 8/200, Batch 14/17, Loss G: 6.5660624504089355, Loss D: 0.10582897067070007\n",
      "Epoch 8/200, Batch 15/17, Loss G: 6.387454032897949, Loss D: 0.06960684061050415\n",
      "Epoch 8/200, Batch 16/17, Loss G: 6.594261169433594, Loss D: 0.1132189929485321\n",
      "Epoch 9/200, Batch 0/17, Loss G: 6.536380290985107, Loss D: 0.13647323846817017\n",
      "Epoch 9/200, Batch 1/17, Loss G: 6.105327606201172, Loss D: 0.20034141838550568\n",
      "Epoch 9/200, Batch 2/17, Loss G: 6.503119468688965, Loss D: 0.1430303454399109\n",
      "Epoch 9/200, Batch 3/17, Loss G: 6.1124725341796875, Loss D: 0.12318781763315201\n",
      "Epoch 9/200, Batch 4/17, Loss G: 6.3750505447387695, Loss D: 0.08247755467891693\n",
      "Epoch 9/200, Batch 5/17, Loss G: 6.780641555786133, Loss D: 0.11742736399173737\n",
      "Epoch 9/200, Batch 6/17, Loss G: 6.160085201263428, Loss D: 0.12268055975437164\n",
      "Epoch 9/200, Batch 7/17, Loss G: 6.514953136444092, Loss D: 0.10933099687099457\n",
      "Epoch 9/200, Batch 8/17, Loss G: 6.260501861572266, Loss D: 0.16273845732212067\n",
      "Epoch 9/200, Batch 9/17, Loss G: 6.1573591232299805, Loss D: 0.16627493500709534\n",
      "Epoch 9/200, Batch 10/17, Loss G: 6.724127292633057, Loss D: 0.10817575454711914\n",
      "Epoch 9/200, Batch 11/17, Loss G: 6.420989036560059, Loss D: 0.10097531974315643\n",
      "Epoch 9/200, Batch 12/17, Loss G: 6.349484443664551, Loss D: 0.11237359046936035\n",
      "Epoch 9/200, Batch 13/17, Loss G: 6.106465816497803, Loss D: 0.15936772525310516\n",
      "Epoch 9/200, Batch 14/17, Loss G: 6.657271385192871, Loss D: 0.19807149469852448\n",
      "Epoch 9/200, Batch 15/17, Loss G: 5.904021263122559, Loss D: 0.14983156323432922\n",
      "Epoch 9/200, Batch 16/17, Loss G: 6.2991743087768555, Loss D: 0.0839688703417778\n",
      "Epoch 10/200, Batch 0/17, Loss G: 6.399059772491455, Loss D: 0.07708520442247391\n",
      "Epoch 10/200, Batch 1/17, Loss G: 6.421448230743408, Loss D: 0.10476420819759369\n",
      "Epoch 10/200, Batch 2/17, Loss G: 6.140522003173828, Loss D: 0.13946330547332764\n",
      "Epoch 10/200, Batch 3/17, Loss G: 6.150577545166016, Loss D: 0.1987411081790924\n",
      "Epoch 10/200, Batch 4/17, Loss G: 6.218125343322754, Loss D: 0.10163062065839767\n",
      "Epoch 10/200, Batch 5/17, Loss G: 6.543838977813721, Loss D: 0.06936755776405334\n",
      "Epoch 10/200, Batch 6/17, Loss G: 6.3225789070129395, Loss D: 0.09776882082223892\n",
      "Epoch 10/200, Batch 7/17, Loss G: 6.126493453979492, Loss D: 0.12338285893201828\n",
      "Epoch 10/200, Batch 8/17, Loss G: 6.71080207824707, Loss D: 0.27678248286247253\n",
      "Epoch 10/200, Batch 9/17, Loss G: 6.056776523590088, Loss D: 0.1450691968202591\n",
      "Epoch 10/200, Batch 10/17, Loss G: 6.084554672241211, Loss D: 0.1570928543806076\n",
      "Epoch 10/200, Batch 11/17, Loss G: 6.4774322509765625, Loss D: 0.13840705156326294\n",
      "Epoch 10/200, Batch 12/17, Loss G: 5.778987407684326, Loss D: 0.0856928676366806\n",
      "Epoch 10/200, Batch 13/17, Loss G: 5.900328636169434, Loss D: 0.07887693494558334\n",
      "Epoch 10/200, Batch 14/17, Loss G: 6.122555732727051, Loss D: 0.14349040389060974\n",
      "Epoch 10/200, Batch 15/17, Loss G: 5.932960510253906, Loss D: 0.13245698809623718\n",
      "Epoch 10/200, Batch 16/17, Loss G: 6.419270992279053, Loss D: 0.16507230699062347\n",
      "Epoch 11/200, Batch 0/17, Loss G: 5.9309797286987305, Loss D: 0.16022548079490662\n",
      "Epoch 11/200, Batch 1/17, Loss G: 5.888508319854736, Loss D: 0.0898880660533905\n",
      "Epoch 11/200, Batch 2/17, Loss G: 6.057432174682617, Loss D: 0.08902505040168762\n",
      "Epoch 11/200, Batch 3/17, Loss G: 5.871790885925293, Loss D: 0.12972880899906158\n",
      "Epoch 11/200, Batch 4/17, Loss G: 6.525124549865723, Loss D: 0.10516390204429626\n",
      "Epoch 11/200, Batch 5/17, Loss G: 5.776706695556641, Loss D: 0.1565207839012146\n",
      "Epoch 11/200, Batch 6/17, Loss G: 6.4421186447143555, Loss D: 0.16798019409179688\n",
      "Epoch 11/200, Batch 7/17, Loss G: 5.936311721801758, Loss D: 0.14547915756702423\n",
      "Epoch 11/200, Batch 8/17, Loss G: 6.326109409332275, Loss D: 0.07487233728170395\n",
      "Epoch 11/200, Batch 9/17, Loss G: 6.064745903015137, Loss D: 0.06583786010742188\n",
      "Epoch 11/200, Batch 10/17, Loss G: 5.930730819702148, Loss D: 0.08820847421884537\n",
      "Epoch 11/200, Batch 11/17, Loss G: 5.935831069946289, Loss D: 0.11744236946105957\n",
      "Epoch 11/200, Batch 12/17, Loss G: 5.907561302185059, Loss D: 0.13832131028175354\n",
      "Epoch 11/200, Batch 13/17, Loss G: 6.261395454406738, Loss D: 0.19564604759216309\n",
      "Epoch 11/200, Batch 14/17, Loss G: 6.237422943115234, Loss D: 0.1733156442642212\n",
      "Epoch 11/200, Batch 15/17, Loss G: 7.176301956176758, Loss D: 0.13343623280525208\n",
      "Epoch 11/200, Batch 16/17, Loss G: 6.54451847076416, Loss D: 0.061352938413619995\n",
      "Epoch 12/200, Batch 0/17, Loss G: 6.276030540466309, Loss D: 0.08398141711950302\n",
      "Epoch 12/200, Batch 1/17, Loss G: 6.75920295715332, Loss D: 0.1441931128501892\n",
      "Epoch 12/200, Batch 2/17, Loss G: 6.336491584777832, Loss D: 0.09473306685686111\n",
      "Epoch 12/200, Batch 3/17, Loss G: 6.909319877624512, Loss D: 0.06235907971858978\n",
      "Epoch 12/200, Batch 4/17, Loss G: 6.427855491638184, Loss D: 0.1164749264717102\n",
      "Epoch 12/200, Batch 5/17, Loss G: 6.300954818725586, Loss D: 0.12091810256242752\n",
      "Epoch 12/200, Batch 6/17, Loss G: 6.533129692077637, Loss D: 0.11725495755672455\n",
      "Epoch 12/200, Batch 7/17, Loss G: 5.996257781982422, Loss D: 0.09824753552675247\n",
      "Epoch 12/200, Batch 8/17, Loss G: 5.9911651611328125, Loss D: 0.07016488164663315\n",
      "Epoch 12/200, Batch 9/17, Loss G: 6.2886505126953125, Loss D: 0.06363703310489655\n",
      "Epoch 12/200, Batch 10/17, Loss G: 5.875216007232666, Loss D: 0.13962966203689575\n",
      "Epoch 12/200, Batch 11/17, Loss G: 6.315113544464111, Loss D: 0.1186545193195343\n",
      "Epoch 12/200, Batch 12/17, Loss G: 6.304454803466797, Loss D: 0.08408406376838684\n",
      "Epoch 12/200, Batch 13/17, Loss G: 5.866177558898926, Loss D: 0.07428743690252304\n",
      "Epoch 12/200, Batch 14/17, Loss G: 6.294122695922852, Loss D: 0.09199807792901993\n",
      "Epoch 12/200, Batch 15/17, Loss G: 6.042004585266113, Loss D: 0.12072225660085678\n",
      "Epoch 12/200, Batch 16/17, Loss G: 5.936976432800293, Loss D: 0.10754777491092682\n",
      "Epoch 13/200, Batch 0/17, Loss G: 5.96034574508667, Loss D: 0.08305659890174866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Batch 1/17, Loss G: 6.120887756347656, Loss D: 0.088837169110775\n",
      "Epoch 13/200, Batch 2/17, Loss G: 5.816176414489746, Loss D: 0.08620739728212357\n",
      "Epoch 13/200, Batch 3/17, Loss G: 6.410170555114746, Loss D: 0.06559615582227707\n",
      "Epoch 13/200, Batch 4/17, Loss G: 5.722382545471191, Loss D: 0.08213654160499573\n",
      "Epoch 13/200, Batch 5/17, Loss G: 6.10139799118042, Loss D: 0.10776391625404358\n",
      "Epoch 13/200, Batch 6/17, Loss G: 5.891660690307617, Loss D: 0.08158250898122787\n",
      "Epoch 13/200, Batch 7/17, Loss G: 6.164983749389648, Loss D: 0.14620986580848694\n",
      "Epoch 13/200, Batch 8/17, Loss G: 5.693112373352051, Loss D: 0.11549228429794312\n",
      "Epoch 13/200, Batch 9/17, Loss G: 5.906113624572754, Loss D: 0.11566787213087082\n",
      "Epoch 13/200, Batch 10/17, Loss G: 5.747712135314941, Loss D: 0.09769114851951599\n",
      "Epoch 13/200, Batch 11/17, Loss G: 5.990267276763916, Loss D: 0.08414775133132935\n",
      "Epoch 13/200, Batch 12/17, Loss G: 5.832071304321289, Loss D: 0.07994012534618378\n",
      "Epoch 13/200, Batch 13/17, Loss G: 5.868648529052734, Loss D: 0.07951853424310684\n",
      "Epoch 13/200, Batch 14/17, Loss G: 5.716137409210205, Loss D: 0.12256217002868652\n",
      "Epoch 13/200, Batch 15/17, Loss G: 6.425467014312744, Loss D: 0.12565459311008453\n",
      "Epoch 13/200, Batch 16/17, Loss G: 5.733724594116211, Loss D: 0.0930255725979805\n",
      "Epoch 14/200, Batch 0/17, Loss G: 5.854170799255371, Loss D: 0.06269041448831558\n",
      "Epoch 14/200, Batch 1/17, Loss G: 5.992856025695801, Loss D: 0.06947240233421326\n",
      "Epoch 14/200, Batch 2/17, Loss G: 5.700818061828613, Loss D: 0.08400243520736694\n",
      "Epoch 14/200, Batch 3/17, Loss G: 5.976844787597656, Loss D: 0.059937529265880585\n",
      "Epoch 14/200, Batch 4/17, Loss G: 5.669979572296143, Loss D: 0.06597862392663956\n",
      "Epoch 14/200, Batch 5/17, Loss G: 5.839605331420898, Loss D: 0.07985963672399521\n",
      "Epoch 14/200, Batch 6/17, Loss G: 6.264123439788818, Loss D: 0.079097680747509\n",
      "Epoch 14/200, Batch 7/17, Loss G: 6.057082176208496, Loss D: 0.13110655546188354\n",
      "Epoch 14/200, Batch 8/17, Loss G: 5.871278285980225, Loss D: 0.06542064249515533\n",
      "Epoch 14/200, Batch 9/17, Loss G: 6.2589111328125, Loss D: 0.1496795266866684\n",
      "Epoch 14/200, Batch 10/17, Loss G: 5.642354965209961, Loss D: 0.1815248727798462\n",
      "Epoch 14/200, Batch 11/17, Loss G: 6.09854793548584, Loss D: 0.1683645099401474\n",
      "Epoch 14/200, Batch 12/17, Loss G: 5.79194974899292, Loss D: 0.055308133363723755\n",
      "Epoch 14/200, Batch 13/17, Loss G: 5.909622669219971, Loss D: 0.06787020713090897\n",
      "Epoch 14/200, Batch 14/17, Loss G: 5.853743553161621, Loss D: 0.07641929388046265\n",
      "Epoch 14/200, Batch 15/17, Loss G: 5.689943313598633, Loss D: 0.07860162854194641\n",
      "Epoch 14/200, Batch 16/17, Loss G: 6.056305885314941, Loss D: 0.07149776816368103\n",
      "Epoch 15/200, Batch 0/17, Loss G: 5.694420337677002, Loss D: 0.1258353739976883\n",
      "Epoch 15/200, Batch 1/17, Loss G: 6.06751823425293, Loss D: 0.13278770446777344\n",
      "Epoch 15/200, Batch 2/17, Loss G: 5.547025680541992, Loss D: 0.17731361091136932\n",
      "Epoch 15/200, Batch 3/17, Loss G: 5.950954437255859, Loss D: 0.052343569695949554\n",
      "Epoch 15/200, Batch 4/17, Loss G: 5.810844421386719, Loss D: 0.07740295678377151\n",
      "Epoch 15/200, Batch 5/17, Loss G: 5.369746208190918, Loss D: 0.08923377841711044\n",
      "Epoch 15/200, Batch 6/17, Loss G: 5.930521011352539, Loss D: 0.11628352105617523\n",
      "Epoch 15/200, Batch 7/17, Loss G: 5.646721363067627, Loss D: 0.05990738421678543\n",
      "Epoch 15/200, Batch 8/17, Loss G: 5.579310417175293, Loss D: 0.07451718300580978\n",
      "Epoch 15/200, Batch 9/17, Loss G: 5.798181533813477, Loss D: 0.11227180063724518\n",
      "Epoch 15/200, Batch 10/17, Loss G: 5.443076133728027, Loss D: 0.1265476942062378\n",
      "Epoch 15/200, Batch 11/17, Loss G: 6.103821754455566, Loss D: 0.06469228118658066\n",
      "Epoch 15/200, Batch 12/17, Loss G: 6.156148433685303, Loss D: 0.08308513462543488\n",
      "Epoch 15/200, Batch 13/17, Loss G: 5.631682395935059, Loss D: 0.08946588635444641\n",
      "Epoch 15/200, Batch 14/17, Loss G: 6.199445724487305, Loss D: 0.11103930324316025\n",
      "Epoch 15/200, Batch 15/17, Loss G: 5.572278022766113, Loss D: 0.11643719673156738\n",
      "Epoch 15/200, Batch 16/17, Loss G: 6.093493461608887, Loss D: 0.07302521914243698\n",
      "Epoch 16/200, Batch 0/17, Loss G: 5.593960762023926, Loss D: 0.06604181975126266\n",
      "Epoch 16/200, Batch 1/17, Loss G: 5.964240074157715, Loss D: 0.07638991624116898\n",
      "Epoch 16/200, Batch 2/17, Loss G: 5.485566139221191, Loss D: 0.047654889523983\n",
      "Epoch 16/200, Batch 3/17, Loss G: 5.669051170349121, Loss D: 0.09369079023599625\n",
      "Epoch 16/200, Batch 4/17, Loss G: 5.466815948486328, Loss D: 0.11449733376502991\n",
      "Epoch 16/200, Batch 5/17, Loss G: 5.799464702606201, Loss D: 0.2100224494934082\n",
      "Epoch 16/200, Batch 6/17, Loss G: 5.688154697418213, Loss D: 0.07486224919557571\n",
      "Epoch 16/200, Batch 7/17, Loss G: 5.575816631317139, Loss D: 0.08538022637367249\n",
      "Epoch 16/200, Batch 8/17, Loss G: 5.8578996658325195, Loss D: 0.08877899497747421\n",
      "Epoch 16/200, Batch 9/17, Loss G: 6.115328788757324, Loss D: 0.08046138286590576\n",
      "Epoch 16/200, Batch 10/17, Loss G: 5.899588584899902, Loss D: 0.07284872233867645\n",
      "Epoch 16/200, Batch 11/17, Loss G: 5.843350887298584, Loss D: 0.052761368453502655\n",
      "Epoch 16/200, Batch 12/17, Loss G: 5.700415134429932, Loss D: 0.06017857789993286\n",
      "Epoch 16/200, Batch 13/17, Loss G: 5.882305145263672, Loss D: 0.061426158994436264\n",
      "Epoch 16/200, Batch 14/17, Loss G: 5.755611419677734, Loss D: 0.10545466840267181\n",
      "Epoch 16/200, Batch 15/17, Loss G: 6.265087127685547, Loss D: 0.07220792770385742\n",
      "Epoch 16/200, Batch 16/17, Loss G: 5.731572151184082, Loss D: 0.06649542599916458\n",
      "Epoch 17/200, Batch 0/17, Loss G: 5.723177909851074, Loss D: 0.08274634182453156\n",
      "Epoch 17/200, Batch 1/17, Loss G: 5.718905448913574, Loss D: 0.06578276306390762\n",
      "Epoch 17/200, Batch 2/17, Loss G: 5.726678371429443, Loss D: 0.05123847723007202\n",
      "Epoch 17/200, Batch 3/17, Loss G: 5.615954399108887, Loss D: 0.04652009904384613\n",
      "Epoch 17/200, Batch 4/17, Loss G: 5.522322654724121, Loss D: 0.07959731668233871\n",
      "Epoch 17/200, Batch 5/17, Loss G: 5.908852577209473, Loss D: 0.1557275950908661\n",
      "Epoch 17/200, Batch 6/17, Loss G: 5.499421119689941, Loss D: 0.09663641452789307\n",
      "Epoch 17/200, Batch 7/17, Loss G: 5.580996513366699, Loss D: 0.10608850419521332\n",
      "Epoch 17/200, Batch 8/17, Loss G: 5.816908359527588, Loss D: 0.041273605078458786\n",
      "Epoch 17/200, Batch 9/17, Loss G: 5.559251308441162, Loss D: 0.03833150491118431\n",
      "Epoch 17/200, Batch 10/17, Loss G: 5.476656913757324, Loss D: 0.0475350022315979\n",
      "Epoch 17/200, Batch 11/17, Loss G: 5.8113861083984375, Loss D: 0.0693252682685852\n",
      "Epoch 17/200, Batch 12/17, Loss G: 5.733885288238525, Loss D: 0.06087104231119156\n",
      "Epoch 17/200, Batch 13/17, Loss G: 5.9639739990234375, Loss D: 0.10512184351682663\n",
      "Epoch 17/200, Batch 14/17, Loss G: 5.256770133972168, Loss D: 0.15019096434116364\n",
      "Epoch 17/200, Batch 15/17, Loss G: 6.034088611602783, Loss D: 0.16944171488285065\n",
      "Epoch 17/200, Batch 16/17, Loss G: 5.741924285888672, Loss D: 0.0492866076529026\n",
      "Epoch 18/200, Batch 0/17, Loss G: 5.477725028991699, Loss D: 0.10326807200908661\n",
      "Epoch 18/200, Batch 1/17, Loss G: 5.6677117347717285, Loss D: 0.09204094111919403\n",
      "Epoch 18/200, Batch 2/17, Loss G: 5.574766159057617, Loss D: 0.06647296249866486\n",
      "Epoch 18/200, Batch 3/17, Loss G: 5.663010597229004, Loss D: 0.05557744950056076\n",
      "Epoch 18/200, Batch 4/17, Loss G: 5.343762397766113, Loss D: 0.05556195229291916\n",
      "Epoch 18/200, Batch 5/17, Loss G: 5.723412990570068, Loss D: 0.05253372713923454\n",
      "Epoch 18/200, Batch 6/17, Loss G: 5.653881072998047, Loss D: 0.05384344235062599\n",
      "Epoch 18/200, Batch 7/17, Loss G: 5.945094108581543, Loss D: 0.05573223531246185\n",
      "Epoch 18/200, Batch 8/17, Loss G: 5.902249336242676, Loss D: 0.089440256357193\n",
      "Epoch 18/200, Batch 9/17, Loss G: 5.900608062744141, Loss D: 0.07308390736579895\n",
      "Epoch 18/200, Batch 10/17, Loss G: 5.547102928161621, Loss D: 0.07785715162754059\n",
      "Epoch 18/200, Batch 11/17, Loss G: 5.960051536560059, Loss D: 0.1379825472831726\n",
      "Epoch 18/200, Batch 12/17, Loss G: 5.561899185180664, Loss D: 0.10715921968221664\n",
      "Epoch 18/200, Batch 13/17, Loss G: 5.702411651611328, Loss D: 0.05280078947544098\n",
      "Epoch 18/200, Batch 14/17, Loss G: 5.529500961303711, Loss D: 0.041888363659381866\n",
      "Epoch 18/200, Batch 15/17, Loss G: 5.513405799865723, Loss D: 0.055603016167879105\n",
      "Epoch 18/200, Batch 16/17, Loss G: 5.714138507843018, Loss D: 0.0724872350692749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Batch 0/17, Loss G: 5.579339981079102, Loss D: 0.052514709532260895\n",
      "Epoch 19/200, Batch 1/17, Loss G: 5.7586894035339355, Loss D: 0.07752960920333862\n",
      "Epoch 19/200, Batch 2/17, Loss G: 5.605216026306152, Loss D: 0.07374760508537292\n",
      "Epoch 19/200, Batch 3/17, Loss G: 5.511015892028809, Loss D: 0.06525661796331406\n",
      "Epoch 19/200, Batch 4/17, Loss G: 5.744221210479736, Loss D: 0.09079723060131073\n",
      "Epoch 19/200, Batch 5/17, Loss G: 5.1043901443481445, Loss D: 0.13373063504695892\n",
      "Epoch 19/200, Batch 6/17, Loss G: 5.786962509155273, Loss D: 0.09288973361253738\n",
      "Epoch 19/200, Batch 7/17, Loss G: 5.644351482391357, Loss D: 0.028270989656448364\n",
      "Epoch 19/200, Batch 8/17, Loss G: 5.459386825561523, Loss D: 0.05657610297203064\n",
      "Epoch 19/200, Batch 9/17, Loss G: 5.625794887542725, Loss D: 0.04330229014158249\n",
      "Epoch 19/200, Batch 10/17, Loss G: 5.46687650680542, Loss D: 0.06232664734125137\n",
      "Epoch 19/200, Batch 11/17, Loss G: 5.266922950744629, Loss D: 0.10014835000038147\n",
      "Epoch 19/200, Batch 12/17, Loss G: 5.739413738250732, Loss D: 0.30853739380836487\n",
      "Epoch 19/200, Batch 13/17, Loss G: 5.465823650360107, Loss D: 0.09601430594921112\n",
      "Epoch 19/200, Batch 14/17, Loss G: 5.278414726257324, Loss D: 0.0875188484787941\n",
      "Epoch 19/200, Batch 15/17, Loss G: 5.613659381866455, Loss D: 0.061354056000709534\n",
      "Epoch 19/200, Batch 16/17, Loss G: 5.588080883026123, Loss D: 0.12091086804866791\n",
      "Epoch 20/200, Batch 0/17, Loss G: 5.89985990524292, Loss D: 0.13741524517536163\n",
      "Epoch 20/200, Batch 1/17, Loss G: 5.761619567871094, Loss D: 0.06131719425320625\n",
      "Epoch 20/200, Batch 2/17, Loss G: 5.621438503265381, Loss D: 0.032044798135757446\n",
      "Epoch 20/200, Batch 3/17, Loss G: 5.535274028778076, Loss D: 0.05707415193319321\n",
      "Epoch 20/200, Batch 4/17, Loss G: 5.50112771987915, Loss D: 0.08814673125743866\n",
      "Epoch 20/200, Batch 5/17, Loss G: 5.117705821990967, Loss D: 0.15338623523712158\n",
      "Epoch 20/200, Batch 6/17, Loss G: 5.795262336730957, Loss D: 0.23117493093013763\n",
      "Epoch 20/200, Batch 7/17, Loss G: 5.4225754737854, Loss D: 0.056578248739242554\n",
      "Epoch 20/200, Batch 8/17, Loss G: 5.364041328430176, Loss D: 0.050940126180648804\n",
      "Epoch 20/200, Batch 9/17, Loss G: 5.618050575256348, Loss D: 0.04162541404366493\n",
      "Epoch 20/200, Batch 10/17, Loss G: 5.314284324645996, Loss D: 0.04011478275060654\n",
      "Epoch 20/200, Batch 11/17, Loss G: 5.421907424926758, Loss D: 0.06227913498878479\n",
      "Epoch 20/200, Batch 12/17, Loss G: 5.618893623352051, Loss D: 0.05841221660375595\n",
      "Epoch 20/200, Batch 13/17, Loss G: 5.618011474609375, Loss D: 0.08165009319782257\n",
      "Epoch 20/200, Batch 14/17, Loss G: 5.470254421234131, Loss D: 0.0642472580075264\n",
      "Epoch 20/200, Batch 15/17, Loss G: 5.690770149230957, Loss D: 0.09682142734527588\n",
      "Epoch 20/200, Batch 16/17, Loss G: 5.238399505615234, Loss D: 0.09700746089220047\n",
      "Epoch 21/200, Batch 0/17, Loss G: 5.850314140319824, Loss D: 0.10511595010757446\n",
      "Epoch 21/200, Batch 1/17, Loss G: 5.447271347045898, Loss D: 0.046303968876600266\n",
      "Epoch 21/200, Batch 2/17, Loss G: 5.586538791656494, Loss D: 0.062222227454185486\n",
      "Epoch 21/200, Batch 3/17, Loss G: 5.440674781799316, Loss D: 0.03807215392589569\n",
      "Epoch 21/200, Batch 4/17, Loss G: 5.316012382507324, Loss D: 0.06531699746847153\n",
      "Epoch 21/200, Batch 5/17, Loss G: 5.416015148162842, Loss D: 0.061869245022535324\n",
      "Epoch 21/200, Batch 6/17, Loss G: 5.56976318359375, Loss D: 0.05625443905591965\n",
      "Epoch 21/200, Batch 7/17, Loss G: 5.5157151222229, Loss D: 0.06854331493377686\n",
      "Epoch 21/200, Batch 8/17, Loss G: 5.672920227050781, Loss D: 0.16284358501434326\n",
      "Epoch 21/200, Batch 9/17, Loss G: 5.100528717041016, Loss D: 0.1728825718164444\n",
      "Epoch 21/200, Batch 10/17, Loss G: 5.551202297210693, Loss D: 0.10826665908098221\n",
      "Epoch 21/200, Batch 11/17, Loss G: 5.580880165100098, Loss D: 0.048413943499326706\n",
      "Epoch 21/200, Batch 12/17, Loss G: 5.410139083862305, Loss D: 0.0969487726688385\n",
      "Epoch 21/200, Batch 13/17, Loss G: 5.560884475708008, Loss D: 0.09452670067548752\n",
      "Epoch 21/200, Batch 14/17, Loss G: 5.42197322845459, Loss D: 0.05329015851020813\n",
      "Epoch 21/200, Batch 15/17, Loss G: 5.347287178039551, Loss D: 0.09041602164506912\n",
      "Epoch 21/200, Batch 16/17, Loss G: 5.632495880126953, Loss D: 0.17103491723537445\n",
      "Epoch 22/200, Batch 0/17, Loss G: 5.351909637451172, Loss D: 0.09002150595188141\n",
      "Epoch 22/200, Batch 1/17, Loss G: 5.723759174346924, Loss D: 0.06363725662231445\n",
      "Epoch 22/200, Batch 2/17, Loss G: 5.494122505187988, Loss D: 0.07820931822061539\n",
      "Epoch 22/200, Batch 3/17, Loss G: 5.505422592163086, Loss D: 0.06551006436347961\n",
      "Epoch 22/200, Batch 4/17, Loss G: 5.301307678222656, Loss D: 0.09409482777118683\n",
      "Epoch 22/200, Batch 5/17, Loss G: 5.395766735076904, Loss D: 0.058098893612623215\n",
      "Epoch 22/200, Batch 6/17, Loss G: 5.338437080383301, Loss D: 0.06689832359552383\n",
      "Epoch 22/200, Batch 7/17, Loss G: 4.9272871017456055, Loss D: 0.12207936495542526\n",
      "Epoch 22/200, Batch 8/17, Loss G: 5.750251770019531, Loss D: 0.1684568077325821\n",
      "Epoch 22/200, Batch 9/17, Loss G: 5.671673774719238, Loss D: 0.051929716020822525\n",
      "Epoch 22/200, Batch 10/17, Loss G: 5.577141761779785, Loss D: 0.03698909282684326\n",
      "Epoch 22/200, Batch 11/17, Loss G: 5.348995685577393, Loss D: 0.041865698993206024\n",
      "Epoch 22/200, Batch 12/17, Loss G: 5.247006416320801, Loss D: 0.0647113099694252\n",
      "Epoch 22/200, Batch 13/17, Loss G: 5.889618873596191, Loss D: 0.12372832745313644\n",
      "Epoch 22/200, Batch 14/17, Loss G: 5.088172912597656, Loss D: 0.13977186381816864\n",
      "Epoch 22/200, Batch 15/17, Loss G: 5.717708587646484, Loss D: 0.15163885056972504\n",
      "Epoch 22/200, Batch 16/17, Loss G: 5.634658336639404, Loss D: 0.04581722244620323\n",
      "Epoch 23/200, Batch 0/17, Loss G: 5.301055431365967, Loss D: 0.06410028040409088\n",
      "Epoch 23/200, Batch 1/17, Loss G: 5.718222141265869, Loss D: 0.056347720324993134\n",
      "Epoch 23/200, Batch 2/17, Loss G: 5.17063570022583, Loss D: 0.04145305976271629\n",
      "Epoch 23/200, Batch 3/17, Loss G: 5.266490936279297, Loss D: 0.06122975796461105\n",
      "Epoch 23/200, Batch 4/17, Loss G: 5.508707046508789, Loss D: 0.0484902523458004\n",
      "Epoch 23/200, Batch 5/17, Loss G: 5.215917587280273, Loss D: 0.06313593685626984\n",
      "Epoch 23/200, Batch 6/17, Loss G: 5.45852518081665, Loss D: 0.06054367870092392\n",
      "Epoch 23/200, Batch 7/17, Loss G: 5.197108268737793, Loss D: 0.06394091993570328\n",
      "Epoch 23/200, Batch 8/17, Loss G: 5.242711067199707, Loss D: 0.061598896980285645\n",
      "Epoch 23/200, Batch 9/17, Loss G: 5.6216630935668945, Loss D: 0.0487484410405159\n",
      "Epoch 23/200, Batch 10/17, Loss G: 5.469767093658447, Loss D: 0.06269213557243347\n",
      "Epoch 23/200, Batch 11/17, Loss G: 5.864091873168945, Loss D: 0.1371995061635971\n",
      "Epoch 23/200, Batch 12/17, Loss G: 4.9970197677612305, Loss D: 0.08878215402364731\n",
      "Epoch 23/200, Batch 13/17, Loss G: 5.59957218170166, Loss D: 0.04832381010055542\n",
      "Epoch 23/200, Batch 14/17, Loss G: 5.758673667907715, Loss D: 0.05728661268949509\n",
      "Epoch 23/200, Batch 15/17, Loss G: 5.708806037902832, Loss D: 0.03727366030216217\n",
      "Epoch 23/200, Batch 16/17, Loss G: 5.4706525802612305, Loss D: 0.047403693199157715\n",
      "Epoch 24/200, Batch 0/17, Loss G: 5.294930458068848, Loss D: 0.05179198831319809\n",
      "Epoch 24/200, Batch 1/17, Loss G: 5.543041229248047, Loss D: 0.08130280673503876\n",
      "Epoch 24/200, Batch 2/17, Loss G: 5.1915717124938965, Loss D: 0.07901504635810852\n",
      "Epoch 24/200, Batch 3/17, Loss G: 5.533483982086182, Loss D: 0.10267575830221176\n",
      "Epoch 24/200, Batch 4/17, Loss G: 5.249038219451904, Loss D: 0.0960983857512474\n",
      "Epoch 24/200, Batch 5/17, Loss G: 5.811918258666992, Loss D: 0.08080477267503738\n",
      "Epoch 24/200, Batch 6/17, Loss G: 5.38206672668457, Loss D: 0.029591139405965805\n",
      "Epoch 24/200, Batch 7/17, Loss G: 5.549671173095703, Loss D: 0.0398523285984993\n",
      "Epoch 24/200, Batch 8/17, Loss G: 5.44036340713501, Loss D: 0.04762644320726395\n",
      "Epoch 24/200, Batch 9/17, Loss G: 5.476188659667969, Loss D: 0.04132585972547531\n",
      "Epoch 24/200, Batch 10/17, Loss G: 5.149863243103027, Loss D: 0.07209159433841705\n",
      "Epoch 24/200, Batch 11/17, Loss G: 5.402461051940918, Loss D: 0.12067869305610657\n",
      "Epoch 24/200, Batch 12/17, Loss G: 5.217198371887207, Loss D: 0.09247460216283798\n",
      "Epoch 24/200, Batch 13/17, Loss G: 5.641256809234619, Loss D: 0.08405540883541107\n",
      "Epoch 24/200, Batch 14/17, Loss G: 5.345450401306152, Loss D: 0.04128323495388031\n",
      "Epoch 24/200, Batch 15/17, Loss G: 5.649048805236816, Loss D: 0.02510569617152214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Batch 16/17, Loss G: 5.611673831939697, Loss D: 0.025806788355112076\n",
      "Epoch 25/200, Batch 0/17, Loss G: 5.155540466308594, Loss D: 0.050159480422735214\n",
      "Epoch 25/200, Batch 1/17, Loss G: 5.319460391998291, Loss D: 0.02642800286412239\n",
      "Epoch 25/200, Batch 2/17, Loss G: 5.462569236755371, Loss D: 0.03076520934700966\n",
      "Epoch 25/200, Batch 3/17, Loss G: 5.245382308959961, Loss D: 0.02987590618431568\n",
      "Epoch 25/200, Batch 4/17, Loss G: 5.407851696014404, Loss D: 0.029772482812404633\n",
      "Epoch 25/200, Batch 5/17, Loss G: 5.168210029602051, Loss D: 0.04519045352935791\n",
      "Epoch 25/200, Batch 6/17, Loss G: 5.392905235290527, Loss D: 0.07675552368164062\n",
      "Epoch 25/200, Batch 7/17, Loss G: 5.1284894943237305, Loss D: 0.1075509637594223\n",
      "Epoch 25/200, Batch 8/17, Loss G: 5.5686774253845215, Loss D: 0.14393827319145203\n",
      "Epoch 25/200, Batch 9/17, Loss G: 5.319249629974365, Loss D: 0.03771303594112396\n",
      "Epoch 25/200, Batch 10/17, Loss G: 5.315615653991699, Loss D: 0.030994873493909836\n",
      "Epoch 25/200, Batch 11/17, Loss G: 5.123390197753906, Loss D: 0.05150783061981201\n",
      "Epoch 25/200, Batch 12/17, Loss G: 5.408572673797607, Loss D: 0.03496617078781128\n",
      "Epoch 25/200, Batch 13/17, Loss G: 5.390883445739746, Loss D: 0.09029868990182877\n",
      "Epoch 25/200, Batch 14/17, Loss G: 5.1573638916015625, Loss D: 0.12690351903438568\n",
      "Epoch 25/200, Batch 15/17, Loss G: 5.7315239906311035, Loss D: 0.1609450876712799\n",
      "Epoch 25/200, Batch 16/17, Loss G: 5.024639129638672, Loss D: 0.0840991884469986\n",
      "Epoch 26/200, Batch 0/17, Loss G: 5.392082214355469, Loss D: 0.03899127244949341\n",
      "Epoch 26/200, Batch 1/17, Loss G: 5.421409606933594, Loss D: 0.042294345796108246\n",
      "Epoch 26/200, Batch 2/17, Loss G: 5.251577377319336, Loss D: 0.06777626276016235\n",
      "Epoch 26/200, Batch 3/17, Loss G: 5.737288951873779, Loss D: 0.1075822040438652\n",
      "Epoch 26/200, Batch 4/17, Loss G: 5.0642242431640625, Loss D: 0.1792507767677307\n",
      "Epoch 26/200, Batch 5/17, Loss G: 5.710259437561035, Loss D: 0.1408883035182953\n",
      "Epoch 26/200, Batch 6/17, Loss G: 5.0697855949401855, Loss D: 0.04847980663180351\n",
      "Epoch 26/200, Batch 7/17, Loss G: 5.316151142120361, Loss D: 0.04885838180780411\n",
      "Epoch 26/200, Batch 8/17, Loss G: 5.181052207946777, Loss D: 0.04728696867823601\n",
      "Epoch 26/200, Batch 9/17, Loss G: 5.068680763244629, Loss D: 0.09667700529098511\n",
      "Epoch 26/200, Batch 10/17, Loss G: 5.277111053466797, Loss D: 0.05765479803085327\n",
      "Epoch 26/200, Batch 11/17, Loss G: 5.199566841125488, Loss D: 0.07265745848417282\n",
      "Epoch 26/200, Batch 12/17, Loss G: 5.3405537605285645, Loss D: 0.1302376091480255\n",
      "Epoch 26/200, Batch 13/17, Loss G: 4.946863174438477, Loss D: 0.11532608419656754\n",
      "Epoch 26/200, Batch 14/17, Loss G: 5.4999237060546875, Loss D: 0.10802053660154343\n",
      "Epoch 26/200, Batch 15/17, Loss G: 5.238929748535156, Loss D: 0.04725128784775734\n",
      "Epoch 26/200, Batch 16/17, Loss G: 5.443330764770508, Loss D: 0.03157820552587509\n",
      "Epoch 27/200, Batch 0/17, Loss G: 5.811859607696533, Loss D: 0.0318838506937027\n",
      "Epoch 27/200, Batch 1/17, Loss G: 5.680428504943848, Loss D: 0.03737834841012955\n",
      "Epoch 27/200, Batch 2/17, Loss G: 5.275769233703613, Loss D: 0.054286859929561615\n",
      "Epoch 27/200, Batch 3/17, Loss G: 5.381376266479492, Loss D: 0.0623379647731781\n",
      "Epoch 27/200, Batch 4/17, Loss G: 5.179795265197754, Loss D: 0.06040339171886444\n",
      "Epoch 27/200, Batch 5/17, Loss G: 5.300710678100586, Loss D: 0.05885256454348564\n",
      "Epoch 27/200, Batch 6/17, Loss G: 5.064323902130127, Loss D: 0.06309070438146591\n",
      "Epoch 27/200, Batch 7/17, Loss G: 5.493470191955566, Loss D: 0.10254377871751785\n",
      "Epoch 27/200, Batch 8/17, Loss G: 5.248862266540527, Loss D: 0.09413430839776993\n",
      "Epoch 27/200, Batch 9/17, Loss G: 5.759071350097656, Loss D: 0.02523205056786537\n",
      "Epoch 27/200, Batch 10/17, Loss G: 5.477352142333984, Loss D: 0.04348691925406456\n",
      "Epoch 27/200, Batch 11/17, Loss G: 5.200746536254883, Loss D: 0.02915932424366474\n",
      "Epoch 27/200, Batch 12/17, Loss G: 5.141227722167969, Loss D: 0.04197809472680092\n",
      "Epoch 27/200, Batch 13/17, Loss G: 5.704189300537109, Loss D: 0.03927942365407944\n",
      "Epoch 27/200, Batch 14/17, Loss G: 5.330700397491455, Loss D: 0.02980739437043667\n",
      "Epoch 27/200, Batch 15/17, Loss G: 4.875945091247559, Loss D: 0.0648188665509224\n",
      "Epoch 27/200, Batch 16/17, Loss G: 5.7153778076171875, Loss D: 0.09243681281805038\n",
      "Epoch 28/200, Batch 0/17, Loss G: 4.661177635192871, Loss D: 0.1839389204978943\n",
      "Epoch 28/200, Batch 1/17, Loss G: 5.557498931884766, Loss D: 0.2618786692619324\n",
      "Epoch 28/200, Batch 2/17, Loss G: 5.323159217834473, Loss D: 0.041840627789497375\n",
      "Epoch 28/200, Batch 3/17, Loss G: 4.724847793579102, Loss D: 0.1640210747718811\n",
      "Epoch 28/200, Batch 4/17, Loss G: 5.271152496337891, Loss D: 0.17963075637817383\n",
      "Epoch 28/200, Batch 5/17, Loss G: 5.002569675445557, Loss D: 0.029244612902402878\n",
      "Epoch 28/200, Batch 6/17, Loss G: 5.151468276977539, Loss D: 0.07314584404230118\n",
      "Epoch 28/200, Batch 7/17, Loss G: 5.062363624572754, Loss D: 0.05636760964989662\n",
      "Epoch 28/200, Batch 8/17, Loss G: 5.035150527954102, Loss D: 0.05851531773805618\n",
      "Epoch 28/200, Batch 9/17, Loss G: 5.361634254455566, Loss D: 0.07136616855859756\n",
      "Epoch 28/200, Batch 10/17, Loss G: 5.211613178253174, Loss D: 0.08763639628887177\n",
      "Epoch 28/200, Batch 11/17, Loss G: 5.497881889343262, Loss D: 0.10620860755443573\n",
      "Epoch 28/200, Batch 12/17, Loss G: 4.960285663604736, Loss D: 0.14380769431591034\n",
      "Epoch 28/200, Batch 13/17, Loss G: 5.479405879974365, Loss D: 0.05829039588570595\n",
      "Epoch 28/200, Batch 14/17, Loss G: 5.272770881652832, Loss D: 0.022365637123584747\n",
      "Epoch 28/200, Batch 15/17, Loss G: 5.319791793823242, Loss D: 0.02292976900935173\n",
      "Epoch 28/200, Batch 16/17, Loss G: 5.488059043884277, Loss D: 0.023625534027814865\n",
      "Epoch 29/200, Batch 0/17, Loss G: 5.302002906799316, Loss D: 0.04375087469816208\n",
      "Epoch 29/200, Batch 1/17, Loss G: 5.430698394775391, Loss D: 0.06396155804395676\n",
      "Epoch 29/200, Batch 2/17, Loss G: 5.197107791900635, Loss D: 0.05944795906543732\n",
      "Epoch 29/200, Batch 3/17, Loss G: 5.5189313888549805, Loss D: 0.02933722920715809\n",
      "Epoch 29/200, Batch 4/17, Loss G: 5.157496452331543, Loss D: 0.030782602727413177\n",
      "Epoch 29/200, Batch 5/17, Loss G: 5.257513046264648, Loss D: 0.016905533149838448\n",
      "Epoch 29/200, Batch 6/17, Loss G: 5.214494228363037, Loss D: 0.017481330782175064\n",
      "Epoch 29/200, Batch 7/17, Loss G: 5.214194297790527, Loss D: 0.02980434149503708\n",
      "Epoch 29/200, Batch 8/17, Loss G: 5.043657302856445, Loss D: 0.04607319459319115\n",
      "Epoch 29/200, Batch 9/17, Loss G: 5.561326026916504, Loss D: 0.021659471094608307\n",
      "Epoch 29/200, Batch 10/17, Loss G: 5.428174018859863, Loss D: 0.044169679284095764\n",
      "Epoch 29/200, Batch 11/17, Loss G: 5.105836868286133, Loss D: 0.04053512588143349\n",
      "Epoch 29/200, Batch 12/17, Loss G: 5.221982955932617, Loss D: 0.02892056107521057\n",
      "Epoch 29/200, Batch 13/17, Loss G: 5.306438446044922, Loss D: 0.04687918350100517\n",
      "Epoch 29/200, Batch 14/17, Loss G: 4.994132041931152, Loss D: 0.06859131157398224\n",
      "Epoch 29/200, Batch 15/17, Loss G: 5.482094764709473, Loss D: 0.12333790212869644\n",
      "Epoch 29/200, Batch 16/17, Loss G: 5.1796674728393555, Loss D: 0.05804000794887543\n",
      "Epoch 30/200, Batch 0/17, Loss G: 5.297151565551758, Loss D: 0.05899994075298309\n",
      "Epoch 30/200, Batch 1/17, Loss G: 5.475891590118408, Loss D: 0.11390725523233414\n",
      "Epoch 30/200, Batch 2/17, Loss G: 4.885629653930664, Loss D: 0.09421025216579437\n",
      "Epoch 30/200, Batch 3/17, Loss G: 5.145164966583252, Loss D: 0.03724835813045502\n",
      "Epoch 30/200, Batch 4/17, Loss G: 5.1606268882751465, Loss D: 0.024682823568582535\n",
      "Epoch 30/200, Batch 5/17, Loss G: 5.400012016296387, Loss D: 0.023412175476551056\n",
      "Epoch 30/200, Batch 6/17, Loss G: 5.178021430969238, Loss D: 0.021131407469511032\n",
      "Epoch 30/200, Batch 7/17, Loss G: 5.2466936111450195, Loss D: 0.03499229624867439\n",
      "Epoch 30/200, Batch 8/17, Loss G: 5.04858922958374, Loss D: 0.0653664618730545\n",
      "Epoch 30/200, Batch 9/17, Loss G: 5.445657253265381, Loss D: 0.07353601604700089\n",
      "Epoch 30/200, Batch 10/17, Loss G: 4.809974670410156, Loss D: 0.08278980106115341\n",
      "Epoch 30/200, Batch 11/17, Loss G: 5.531303405761719, Loss D: 0.05004674568772316\n",
      "Epoch 30/200, Batch 12/17, Loss G: 5.237575531005859, Loss D: 0.018078450113534927\n",
      "Epoch 30/200, Batch 13/17, Loss G: 5.082170009613037, Loss D: 0.036200687289237976\n",
      "Epoch 30/200, Batch 14/17, Loss G: 5.221512794494629, Loss D: 0.037147656083106995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Batch 15/17, Loss G: 5.100699424743652, Loss D: 0.038160596042871475\n",
      "Epoch 30/200, Batch 16/17, Loss G: 5.378626823425293, Loss D: 0.04048239439725876\n",
      "Epoch 31/200, Batch 0/17, Loss G: 5.23514461517334, Loss D: 0.06046425551176071\n",
      "Epoch 31/200, Batch 1/17, Loss G: 5.427838325500488, Loss D: 0.08064476400613785\n",
      "Epoch 31/200, Batch 2/17, Loss G: 5.03574275970459, Loss D: 0.10891535133123398\n",
      "Epoch 31/200, Batch 3/17, Loss G: 5.334218978881836, Loss D: 0.08036243915557861\n",
      "Epoch 31/200, Batch 4/17, Loss G: 5.0950469970703125, Loss D: 0.025049112737178802\n",
      "Epoch 31/200, Batch 5/17, Loss G: 5.194372177124023, Loss D: 0.017652783542871475\n",
      "Epoch 31/200, Batch 6/17, Loss G: 5.397896766662598, Loss D: 0.05394373834133148\n",
      "Epoch 31/200, Batch 7/17, Loss G: 5.224741458892822, Loss D: 0.06437905132770538\n",
      "Epoch 31/200, Batch 8/17, Loss G: 5.312139987945557, Loss D: 0.03641751781105995\n",
      "Epoch 31/200, Batch 9/17, Loss G: 5.288179397583008, Loss D: 0.04866500198841095\n",
      "Epoch 31/200, Batch 10/17, Loss G: 4.9485273361206055, Loss D: 0.10354984551668167\n",
      "Epoch 31/200, Batch 11/17, Loss G: 5.061529159545898, Loss D: 0.11349570751190186\n",
      "Epoch 31/200, Batch 12/17, Loss G: 5.228804111480713, Loss D: 0.032199420034885406\n",
      "Epoch 31/200, Batch 13/17, Loss G: 4.751633644104004, Loss D: 0.09639661759138107\n",
      "Epoch 31/200, Batch 14/17, Loss G: 5.413324356079102, Loss D: 0.2618986666202545\n",
      "Epoch 31/200, Batch 15/17, Loss G: 5.049970626831055, Loss D: 0.06720983982086182\n",
      "Epoch 31/200, Batch 16/17, Loss G: 5.174253940582275, Loss D: 0.04261557757854462\n",
      "Epoch 32/200, Batch 0/17, Loss G: 5.237626075744629, Loss D: 0.04374903440475464\n",
      "Epoch 32/200, Batch 1/17, Loss G: 5.085914611816406, Loss D: 0.0694936215877533\n",
      "Epoch 32/200, Batch 2/17, Loss G: 5.232256889343262, Loss D: 0.09098777174949646\n",
      "Epoch 32/200, Batch 3/17, Loss G: 4.906797885894775, Loss D: 0.07609005272388458\n",
      "Epoch 32/200, Batch 4/17, Loss G: 5.003890037536621, Loss D: 0.07364451885223389\n",
      "Epoch 32/200, Batch 5/17, Loss G: 5.4350409507751465, Loss D: 0.12752068042755127\n",
      "Epoch 32/200, Batch 6/17, Loss G: 4.780038833618164, Loss D: 0.16910897195339203\n",
      "Epoch 32/200, Batch 7/17, Loss G: 5.466772079467773, Loss D: 0.04915352910757065\n",
      "Epoch 32/200, Batch 8/17, Loss G: 5.484255790710449, Loss D: 0.030271902680397034\n",
      "Epoch 32/200, Batch 9/17, Loss G: 5.186675548553467, Loss D: 0.03125908598303795\n",
      "Epoch 32/200, Batch 10/17, Loss G: 4.939123153686523, Loss D: 0.021286403760313988\n",
      "Epoch 32/200, Batch 11/17, Loss G: 4.932677268981934, Loss D: 0.02851444110274315\n",
      "Epoch 32/200, Batch 12/17, Loss G: 5.012007236480713, Loss D: 0.04487064108252525\n",
      "Epoch 32/200, Batch 13/17, Loss G: 4.965367317199707, Loss D: 0.07919394969940186\n",
      "Epoch 32/200, Batch 14/17, Loss G: 5.331406593322754, Loss D: 0.05117666721343994\n",
      "Epoch 32/200, Batch 15/17, Loss G: 5.0654144287109375, Loss D: 0.0510091632604599\n",
      "Epoch 32/200, Batch 16/17, Loss G: 5.463472843170166, Loss D: 0.029426440596580505\n",
      "Epoch 33/200, Batch 0/17, Loss G: 5.13315486907959, Loss D: 0.03674698621034622\n",
      "Epoch 33/200, Batch 1/17, Loss G: 5.087244987487793, Loss D: 0.04008966684341431\n",
      "Epoch 33/200, Batch 2/17, Loss G: 4.93482780456543, Loss D: 0.035257868468761444\n",
      "Epoch 33/200, Batch 3/17, Loss G: 5.162956714630127, Loss D: 0.05727230757474899\n",
      "Epoch 33/200, Batch 4/17, Loss G: 5.131289482116699, Loss D: 0.03939785808324814\n",
      "Epoch 33/200, Batch 5/17, Loss G: 5.0647125244140625, Loss D: 0.045168839395046234\n",
      "Epoch 33/200, Batch 6/17, Loss G: 5.4536261558532715, Loss D: 0.07143602520227432\n",
      "Epoch 33/200, Batch 7/17, Loss G: 4.822277069091797, Loss D: 0.09723887592554092\n",
      "Epoch 33/200, Batch 8/17, Loss G: 5.440024375915527, Loss D: 0.0776078850030899\n",
      "Epoch 33/200, Batch 9/17, Loss G: 5.211725234985352, Loss D: 0.028149131685495377\n",
      "Epoch 33/200, Batch 10/17, Loss G: 5.085587501525879, Loss D: 0.02291412465274334\n",
      "Epoch 33/200, Batch 11/17, Loss G: 5.22209358215332, Loss D: 0.020105790346860886\n",
      "Epoch 33/200, Batch 12/17, Loss G: 5.1625776290893555, Loss D: 0.032811667770147324\n",
      "Epoch 33/200, Batch 13/17, Loss G: 5.117730617523193, Loss D: 0.0528641976416111\n",
      "Epoch 33/200, Batch 14/17, Loss G: 5.400785446166992, Loss D: 0.031490251421928406\n",
      "Epoch 33/200, Batch 15/17, Loss G: 5.226278305053711, Loss D: 0.05552349612116814\n",
      "Epoch 33/200, Batch 16/17, Loss G: 5.016821384429932, Loss D: 0.08121424913406372\n",
      "Epoch 34/200, Batch 0/17, Loss G: 5.3747711181640625, Loss D: 0.12805305421352386\n",
      "Epoch 34/200, Batch 1/17, Loss G: 4.8503899574279785, Loss D: 0.08156965672969818\n",
      "Epoch 34/200, Batch 2/17, Loss G: 5.1826372146606445, Loss D: 0.05612547695636749\n",
      "Epoch 34/200, Batch 3/17, Loss G: 5.243806838989258, Loss D: 0.04223550483584404\n",
      "Epoch 34/200, Batch 4/17, Loss G: 5.332093238830566, Loss D: 0.03955265134572983\n",
      "Epoch 34/200, Batch 5/17, Loss G: 5.136246681213379, Loss D: 0.02198188006877899\n",
      "Epoch 34/200, Batch 6/17, Loss G: 5.07657527923584, Loss D: 0.0271089356392622\n",
      "Epoch 34/200, Batch 7/17, Loss G: 4.951215744018555, Loss D: 0.04222794622182846\n",
      "Epoch 34/200, Batch 8/17, Loss G: 5.16238260269165, Loss D: 0.02932993695139885\n",
      "Epoch 34/200, Batch 9/17, Loss G: 5.085540771484375, Loss D: 0.043341219425201416\n",
      "Epoch 34/200, Batch 10/17, Loss G: 5.126554489135742, Loss D: 0.03386247903108597\n",
      "Epoch 34/200, Batch 11/17, Loss G: 5.151358604431152, Loss D: 0.05837073177099228\n",
      "Epoch 34/200, Batch 12/17, Loss G: 5.006690502166748, Loss D: 0.050188153982162476\n",
      "Epoch 34/200, Batch 13/17, Loss G: 5.055559158325195, Loss D: 0.07170556485652924\n",
      "Epoch 34/200, Batch 14/17, Loss G: 5.024344444274902, Loss D: 0.05140667408704758\n",
      "Epoch 34/200, Batch 15/17, Loss G: 5.544804573059082, Loss D: 0.05488763749599457\n",
      "Epoch 34/200, Batch 16/17, Loss G: 5.1839399337768555, Loss D: 0.020982202142477036\n",
      "Epoch 35/200, Batch 0/17, Loss G: 5.0550079345703125, Loss D: 0.04561644792556763\n",
      "Epoch 35/200, Batch 1/17, Loss G: 5.627572536468506, Loss D: 0.042674969881772995\n",
      "Epoch 35/200, Batch 2/17, Loss G: 5.2319769859313965, Loss D: 0.022019796073436737\n",
      "Epoch 35/200, Batch 3/17, Loss G: 5.184388160705566, Loss D: 0.02373976819217205\n",
      "Epoch 35/200, Batch 4/17, Loss G: 5.255500793457031, Loss D: 0.04297254979610443\n",
      "Epoch 35/200, Batch 5/17, Loss G: 4.838437080383301, Loss D: 0.06474698334932327\n",
      "Epoch 35/200, Batch 6/17, Loss G: 5.224111080169678, Loss D: 0.12411495298147202\n",
      "Epoch 35/200, Batch 7/17, Loss G: 4.828815460205078, Loss D: 0.0768197700381279\n",
      "Epoch 35/200, Batch 8/17, Loss G: 5.143234729766846, Loss D: 0.04627405107021332\n",
      "Epoch 35/200, Batch 9/17, Loss G: 4.972510814666748, Loss D: 0.015247000381350517\n",
      "Epoch 35/200, Batch 10/17, Loss G: 4.976687431335449, Loss D: 0.027312420308589935\n",
      "Epoch 35/200, Batch 11/17, Loss G: 5.382284164428711, Loss D: 0.048939451575279236\n",
      "Epoch 35/200, Batch 12/17, Loss G: 4.6637983322143555, Loss D: 0.09915385395288467\n",
      "Epoch 35/200, Batch 13/17, Loss G: 5.329020977020264, Loss D: 0.1380252242088318\n",
      "Epoch 35/200, Batch 14/17, Loss G: 4.571235656738281, Loss D: 0.21182243525981903\n",
      "Epoch 35/200, Batch 15/17, Loss G: 5.224738597869873, Loss D: 0.09384174644947052\n",
      "Epoch 35/200, Batch 16/17, Loss G: 5.295180320739746, Loss D: 0.026216672733426094\n",
      "Epoch 36/200, Batch 0/17, Loss G: 4.990312576293945, Loss D: 0.0193130224943161\n",
      "Epoch 36/200, Batch 1/17, Loss G: 5.184250831604004, Loss D: 0.03040616773068905\n",
      "Epoch 36/200, Batch 2/17, Loss G: 5.024645805358887, Loss D: 0.0193866565823555\n",
      "Epoch 36/200, Batch 3/17, Loss G: 5.072366237640381, Loss D: 0.013779900968074799\n",
      "Epoch 36/200, Batch 4/17, Loss G: 5.251507759094238, Loss D: 0.013074411079287529\n",
      "Epoch 36/200, Batch 5/17, Loss G: 4.989189147949219, Loss D: 0.03258008137345314\n",
      "Epoch 36/200, Batch 6/17, Loss G: 4.802651405334473, Loss D: 0.040060050785541534\n",
      "Epoch 36/200, Batch 7/17, Loss G: 5.069478988647461, Loss D: 0.048200253397226334\n",
      "Epoch 36/200, Batch 8/17, Loss G: 5.279666900634766, Loss D: 0.03826558217406273\n",
      "Epoch 36/200, Batch 9/17, Loss G: 5.329489707946777, Loss D: 0.03683657944202423\n",
      "Epoch 36/200, Batch 10/17, Loss G: 5.224442481994629, Loss D: 0.03053157404065132\n",
      "Epoch 36/200, Batch 11/17, Loss G: 5.112433433532715, Loss D: 0.027595650404691696\n",
      "Epoch 36/200, Batch 12/17, Loss G: 5.5292510986328125, Loss D: 0.016354016959667206\n",
      "Epoch 36/200, Batch 13/17, Loss G: 5.05416202545166, Loss D: 0.02353709377348423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200, Batch 14/17, Loss G: 4.948360443115234, Loss D: 0.03164392709732056\n",
      "Epoch 36/200, Batch 15/17, Loss G: 5.1939496994018555, Loss D: 0.026470616459846497\n",
      "Epoch 36/200, Batch 16/17, Loss G: 5.275650978088379, Loss D: 0.030102252960205078\n",
      "Epoch 37/200, Batch 0/17, Loss G: 4.653081893920898, Loss D: 0.09202602505683899\n",
      "Epoch 37/200, Batch 1/17, Loss G: 5.224762916564941, Loss D: 0.1834382265806198\n",
      "Epoch 37/200, Batch 2/17, Loss G: 4.515722274780273, Loss D: 0.12247521430253983\n",
      "Epoch 37/200, Batch 3/17, Loss G: 5.0599565505981445, Loss D: 0.02615806832909584\n",
      "Epoch 37/200, Batch 4/17, Loss G: 5.341421127319336, Loss D: 0.018581224605441093\n",
      "Epoch 37/200, Batch 5/17, Loss G: 4.824453353881836, Loss D: 0.032998669892549515\n",
      "Epoch 37/200, Batch 6/17, Loss G: 5.120455741882324, Loss D: 0.0454341396689415\n",
      "Epoch 37/200, Batch 7/17, Loss G: 4.765242576599121, Loss D: 0.06206143647432327\n",
      "Epoch 37/200, Batch 8/17, Loss G: 5.292365550994873, Loss D: 0.08120623230934143\n",
      "Epoch 37/200, Batch 9/17, Loss G: 4.85038948059082, Loss D: 0.049830637872219086\n",
      "Epoch 37/200, Batch 10/17, Loss G: 5.057140350341797, Loss D: 0.027169227600097656\n",
      "Epoch 37/200, Batch 11/17, Loss G: 5.309702396392822, Loss D: 0.03343558311462402\n",
      "Epoch 37/200, Batch 12/17, Loss G: 5.034969329833984, Loss D: 0.03755880892276764\n",
      "Epoch 37/200, Batch 13/17, Loss G: 5.183248519897461, Loss D: 0.02572573721408844\n",
      "Epoch 37/200, Batch 14/17, Loss G: 5.110182762145996, Loss D: 0.02069753035902977\n",
      "Epoch 37/200, Batch 15/17, Loss G: 4.99594783782959, Loss D: 0.024549653753638268\n",
      "Epoch 37/200, Batch 16/17, Loss G: 4.9412078857421875, Loss D: 0.021039322018623352\n",
      "Epoch 38/200, Batch 0/17, Loss G: 4.954113006591797, Loss D: 0.040462836623191833\n",
      "Epoch 38/200, Batch 1/17, Loss G: 5.179617881774902, Loss D: 0.07064353674650192\n",
      "Epoch 38/200, Batch 2/17, Loss G: 4.916751861572266, Loss D: 0.06924396753311157\n",
      "Epoch 38/200, Batch 3/17, Loss G: 5.360376358032227, Loss D: 0.03668741509318352\n",
      "Epoch 38/200, Batch 4/17, Loss G: 5.078889846801758, Loss D: 0.02333766222000122\n",
      "Epoch 38/200, Batch 5/17, Loss G: 5.2014851570129395, Loss D: 0.018279800191521645\n",
      "Epoch 38/200, Batch 6/17, Loss G: 5.114988327026367, Loss D: 0.018541190773248672\n",
      "Epoch 38/200, Batch 7/17, Loss G: 4.8550333976745605, Loss D: 0.018582208082079887\n",
      "Epoch 38/200, Batch 8/17, Loss G: 4.991638660430908, Loss D: 0.014658852480351925\n",
      "Epoch 38/200, Batch 9/17, Loss G: 4.82539176940918, Loss D: 0.017175748944282532\n",
      "Epoch 38/200, Batch 10/17, Loss G: 4.967889785766602, Loss D: 0.027352187782526016\n",
      "Epoch 38/200, Batch 11/17, Loss G: 5.014382362365723, Loss D: 0.017768744379281998\n",
      "Epoch 38/200, Batch 12/17, Loss G: 5.127374649047852, Loss D: 0.024146821349859238\n",
      "Epoch 38/200, Batch 13/17, Loss G: 4.808671951293945, Loss D: 0.026705853641033173\n",
      "Epoch 38/200, Batch 14/17, Loss G: 5.104249954223633, Loss D: 0.015296559780836105\n",
      "Epoch 38/200, Batch 15/17, Loss G: 5.195587635040283, Loss D: 0.028082121163606644\n",
      "Epoch 38/200, Batch 16/17, Loss G: 5.170997619628906, Loss D: 0.03118961676955223\n",
      "Epoch 39/200, Batch 0/17, Loss G: 4.761966705322266, Loss D: 0.0445760115981102\n",
      "Epoch 39/200, Batch 1/17, Loss G: 5.559123992919922, Loss D: 0.07654763758182526\n",
      "Epoch 39/200, Batch 2/17, Loss G: 4.784749507904053, Loss D: 0.09032856673002243\n",
      "Epoch 39/200, Batch 3/17, Loss G: 5.113422393798828, Loss D: 0.05206508934497833\n",
      "Epoch 39/200, Batch 4/17, Loss G: 5.226558208465576, Loss D: 0.026898160576820374\n",
      "Epoch 39/200, Batch 5/17, Loss G: 5.032167434692383, Loss D: 0.02756209298968315\n",
      "Epoch 39/200, Batch 6/17, Loss G: 5.199027061462402, Loss D: 0.02702437900006771\n",
      "Epoch 39/200, Batch 7/17, Loss G: 4.91828727722168, Loss D: 0.013093646615743637\n",
      "Epoch 39/200, Batch 8/17, Loss G: 4.946241855621338, Loss D: 0.0237510334700346\n",
      "Epoch 39/200, Batch 9/17, Loss G: 5.054899215698242, Loss D: 0.01764800399541855\n",
      "Epoch 39/200, Batch 10/17, Loss G: 4.923028945922852, Loss D: 0.01666385866701603\n",
      "Epoch 39/200, Batch 11/17, Loss G: 4.912586212158203, Loss D: 0.025779860094189644\n",
      "Epoch 39/200, Batch 12/17, Loss G: 4.9473772048950195, Loss D: 0.015023607760667801\n",
      "Epoch 39/200, Batch 13/17, Loss G: 4.892575263977051, Loss D: 0.022834528237581253\n",
      "Epoch 39/200, Batch 14/17, Loss G: 5.1016387939453125, Loss D: 0.02930019050836563\n",
      "Epoch 39/200, Batch 15/17, Loss G: 5.228610038757324, Loss D: 0.020588036626577377\n",
      "Epoch 39/200, Batch 16/17, Loss G: 5.094270706176758, Loss D: 0.04042315483093262\n",
      "Epoch 40/200, Batch 0/17, Loss G: 4.587030410766602, Loss D: 0.099890798330307\n",
      "Epoch 40/200, Batch 1/17, Loss G: 5.279645919799805, Loss D: 0.18251100182533264\n",
      "Epoch 40/200, Batch 2/17, Loss G: 4.6317338943481445, Loss D: 0.04565903916954994\n",
      "Epoch 40/200, Batch 3/17, Loss G: 4.975880146026611, Loss D: 0.06259549409151077\n",
      "Epoch 40/200, Batch 4/17, Loss G: 4.467979907989502, Loss D: 0.16782715916633606\n",
      "Epoch 40/200, Batch 5/17, Loss G: 5.229113578796387, Loss D: 0.1806238293647766\n",
      "Epoch 40/200, Batch 6/17, Loss G: 4.540431976318359, Loss D: 0.21756817400455475\n",
      "Epoch 40/200, Batch 7/17, Loss G: 5.2373738288879395, Loss D: 0.17509743571281433\n",
      "Epoch 40/200, Batch 8/17, Loss G: 4.509930610656738, Loss D: 0.10990652441978455\n",
      "Epoch 40/200, Batch 9/17, Loss G: 4.907599449157715, Loss D: 0.0686546191573143\n",
      "Epoch 40/200, Batch 10/17, Loss G: 4.879405975341797, Loss D: 0.09434173256158829\n",
      "Epoch 40/200, Batch 11/17, Loss G: 4.906053066253662, Loss D: 0.13725124299526215\n",
      "Epoch 40/200, Batch 12/17, Loss G: 5.128674507141113, Loss D: 0.06565242260694504\n",
      "Epoch 40/200, Batch 13/17, Loss G: 4.7294416427612305, Loss D: 0.08330611139535904\n",
      "Epoch 40/200, Batch 14/17, Loss G: 5.12546968460083, Loss D: 0.06437723338603973\n",
      "Epoch 40/200, Batch 15/17, Loss G: 4.875077724456787, Loss D: 0.07594820857048035\n",
      "Epoch 40/200, Batch 16/17, Loss G: 4.869691848754883, Loss D: 0.04824460297822952\n",
      "Epoch 41/200, Batch 0/17, Loss G: 5.229194641113281, Loss D: 0.07698331028223038\n",
      "Epoch 41/200, Batch 1/17, Loss G: 4.546993255615234, Loss D: 0.09066405892372131\n",
      "Epoch 41/200, Batch 2/17, Loss G: 5.16835880279541, Loss D: 0.046994324773550034\n",
      "Epoch 41/200, Batch 3/17, Loss G: 5.04656982421875, Loss D: 0.025354605168104172\n",
      "Epoch 41/200, Batch 4/17, Loss G: 4.78000545501709, Loss D: 0.03383321687579155\n",
      "Epoch 41/200, Batch 5/17, Loss G: 5.098215103149414, Loss D: 0.03340362757444382\n",
      "Epoch 41/200, Batch 6/17, Loss G: 5.016031265258789, Loss D: 0.04879096895456314\n",
      "Epoch 41/200, Batch 7/17, Loss G: 4.9088592529296875, Loss D: 0.05344441905617714\n",
      "Epoch 41/200, Batch 8/17, Loss G: 5.100179672241211, Loss D: 0.04474228620529175\n",
      "Epoch 41/200, Batch 9/17, Loss G: 5.134449005126953, Loss D: 0.05946187674999237\n",
      "Epoch 41/200, Batch 10/17, Loss G: 4.814146041870117, Loss D: 0.05963374674320221\n",
      "Epoch 41/200, Batch 11/17, Loss G: 5.238662242889404, Loss D: 0.05762612074613571\n",
      "Epoch 41/200, Batch 12/17, Loss G: 5.096549987792969, Loss D: 0.03584183752536774\n",
      "Epoch 41/200, Batch 13/17, Loss G: 4.843744277954102, Loss D: 0.0387222021818161\n",
      "Epoch 41/200, Batch 14/17, Loss G: 5.268886566162109, Loss D: 0.04140535742044449\n",
      "Epoch 41/200, Batch 15/17, Loss G: 5.0028300285339355, Loss D: 0.028351306915283203\n",
      "Epoch 41/200, Batch 16/17, Loss G: 5.253373146057129, Loss D: 0.027522847056388855\n",
      "Epoch 42/200, Batch 0/17, Loss G: 4.84257698059082, Loss D: 0.029545392841100693\n",
      "Epoch 42/200, Batch 1/17, Loss G: 5.25340461730957, Loss D: 0.028012018650770187\n",
      "Epoch 42/200, Batch 2/17, Loss G: 5.014798641204834, Loss D: 0.033063482493162155\n",
      "Epoch 42/200, Batch 3/17, Loss G: 5.081456661224365, Loss D: 0.0414564423263073\n",
      "Epoch 42/200, Batch 4/17, Loss G: 4.847306728363037, Loss D: 0.03702437877655029\n",
      "Epoch 42/200, Batch 5/17, Loss G: 4.940690040588379, Loss D: 0.030984805896878242\n",
      "Epoch 42/200, Batch 6/17, Loss G: 5.185975551605225, Loss D: 0.03323087468743324\n",
      "Epoch 42/200, Batch 7/17, Loss G: 5.093139171600342, Loss D: 0.028543177992105484\n",
      "Epoch 42/200, Batch 8/17, Loss G: 4.977601051330566, Loss D: 0.02347712405025959\n",
      "Epoch 42/200, Batch 9/17, Loss G: 5.0620527267456055, Loss D: 0.027274394407868385\n",
      "Epoch 42/200, Batch 10/17, Loss G: 4.785572052001953, Loss D: 0.03535070642828941\n",
      "Epoch 42/200, Batch 11/17, Loss G: 4.9291558265686035, Loss D: 0.08235052973031998\n",
      "Epoch 42/200, Batch 12/17, Loss G: 4.669646263122559, Loss D: 0.16082419455051422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200, Batch 13/17, Loss G: 5.198346138000488, Loss D: 0.1248120442032814\n",
      "Epoch 42/200, Batch 14/17, Loss G: 4.852414608001709, Loss D: 0.020932350307703018\n",
      "Epoch 42/200, Batch 15/17, Loss G: 4.712251663208008, Loss D: 0.03669790178537369\n",
      "Epoch 42/200, Batch 16/17, Loss G: 5.168920516967773, Loss D: 0.029536955058574677\n",
      "Epoch 43/200, Batch 0/17, Loss G: 4.967220306396484, Loss D: 0.033613141626119614\n",
      "Epoch 43/200, Batch 1/17, Loss G: 4.853064060211182, Loss D: 0.05482766032218933\n",
      "Epoch 43/200, Batch 2/17, Loss G: 5.134084701538086, Loss D: 0.05406983196735382\n",
      "Epoch 43/200, Batch 3/17, Loss G: 4.6721510887146, Loss D: 0.1487119048833847\n",
      "Epoch 43/200, Batch 4/17, Loss G: 5.512759208679199, Loss D: 0.18132026493549347\n",
      "Epoch 43/200, Batch 5/17, Loss G: 4.846752643585205, Loss D: 0.0325855016708374\n",
      "Epoch 43/200, Batch 6/17, Loss G: 4.76280403137207, Loss D: 0.05135708674788475\n",
      "Epoch 43/200, Batch 7/17, Loss G: 4.937763214111328, Loss D: 0.0475601851940155\n",
      "Epoch 43/200, Batch 8/17, Loss G: 4.860769271850586, Loss D: 0.0280265212059021\n",
      "Epoch 43/200, Batch 9/17, Loss G: 4.787120819091797, Loss D: 0.04431459307670593\n",
      "Epoch 43/200, Batch 10/17, Loss G: 5.286074638366699, Loss D: 0.08467384427785873\n",
      "Epoch 43/200, Batch 11/17, Loss G: 4.543850898742676, Loss D: 0.09492400288581848\n",
      "Epoch 43/200, Batch 12/17, Loss G: 5.084166526794434, Loss D: 0.060938745737075806\n",
      "Epoch 43/200, Batch 13/17, Loss G: 4.73844051361084, Loss D: 0.0413503535091877\n",
      "Epoch 43/200, Batch 14/17, Loss G: 5.0321197509765625, Loss D: 0.028596041724085808\n",
      "Epoch 43/200, Batch 15/17, Loss G: 4.750186920166016, Loss D: 0.02319730818271637\n",
      "Epoch 43/200, Batch 16/17, Loss G: 4.80344295501709, Loss D: 0.01987385004758835\n",
      "Epoch 44/200, Batch 0/17, Loss G: 4.844675064086914, Loss D: 0.03748396784067154\n",
      "Epoch 44/200, Batch 1/17, Loss G: 4.919380187988281, Loss D: 0.046335369348526\n",
      "Epoch 44/200, Batch 2/17, Loss G: 5.026119232177734, Loss D: 0.06252709031105042\n",
      "Epoch 44/200, Batch 3/17, Loss G: 4.618753433227539, Loss D: 0.13059383630752563\n",
      "Epoch 44/200, Batch 4/17, Loss G: 5.373085975646973, Loss D: 0.20685820281505585\n",
      "Epoch 44/200, Batch 5/17, Loss G: 4.8832621574401855, Loss D: 0.0428900383412838\n",
      "Epoch 44/200, Batch 6/17, Loss G: 4.9392242431640625, Loss D: 0.03520136699080467\n",
      "Epoch 44/200, Batch 7/17, Loss G: 5.124356269836426, Loss D: 0.02722410298883915\n",
      "Epoch 44/200, Batch 8/17, Loss G: 4.929581642150879, Loss D: 0.020618263632059097\n",
      "Epoch 44/200, Batch 9/17, Loss G: 4.948312759399414, Loss D: 0.024147681891918182\n",
      "Epoch 44/200, Batch 10/17, Loss G: 4.813962936401367, Loss D: 0.04444991052150726\n",
      "Epoch 44/200, Batch 11/17, Loss G: 4.921914100646973, Loss D: 0.08756333589553833\n",
      "Epoch 44/200, Batch 12/17, Loss G: 5.24496603012085, Loss D: 0.13862229883670807\n",
      "Epoch 44/200, Batch 13/17, Loss G: 4.428142547607422, Loss D: 0.1744404435157776\n",
      "Epoch 44/200, Batch 14/17, Loss G: 5.242709159851074, Loss D: 0.06700609624385834\n",
      "Epoch 44/200, Batch 15/17, Loss G: 5.073122978210449, Loss D: 0.01578686013817787\n",
      "Epoch 44/200, Batch 16/17, Loss G: 4.457295894622803, Loss D: 0.03827504813671112\n",
      "Epoch 45/200, Batch 0/17, Loss G: 5.081234931945801, Loss D: 0.03184295818209648\n",
      "Epoch 45/200, Batch 1/17, Loss G: 5.039305686950684, Loss D: 0.01790217123925686\n",
      "Epoch 45/200, Batch 2/17, Loss G: 4.785133361816406, Loss D: 0.04133208468556404\n",
      "Epoch 45/200, Batch 3/17, Loss G: 5.055621147155762, Loss D: 0.039130568504333496\n",
      "Epoch 45/200, Batch 4/17, Loss G: 5.0114054679870605, Loss D: 0.045222289860248566\n",
      "Epoch 45/200, Batch 5/17, Loss G: 4.611331939697266, Loss D: 0.04896538332104683\n",
      "Epoch 45/200, Batch 6/17, Loss G: 4.904916763305664, Loss D: 0.05095034837722778\n",
      "Epoch 45/200, Batch 7/17, Loss G: 4.710894584655762, Loss D: 0.03726404532790184\n",
      "Epoch 45/200, Batch 8/17, Loss G: 4.859836101531982, Loss D: 0.03180947154760361\n",
      "Epoch 45/200, Batch 9/17, Loss G: 4.865676403045654, Loss D: 0.024002306163311005\n",
      "Epoch 45/200, Batch 10/17, Loss G: 4.815269470214844, Loss D: 0.028835227712988853\n",
      "Epoch 45/200, Batch 11/17, Loss G: 5.103527545928955, Loss D: 0.031286343932151794\n",
      "Epoch 45/200, Batch 12/17, Loss G: 4.733820915222168, Loss D: 0.011226407252252102\n",
      "Epoch 45/200, Batch 13/17, Loss G: 4.753133296966553, Loss D: 0.01706768572330475\n",
      "Epoch 45/200, Batch 14/17, Loss G: 4.858503818511963, Loss D: 0.014563685283064842\n",
      "Epoch 45/200, Batch 15/17, Loss G: 4.982775688171387, Loss D: 0.020588630810379982\n",
      "Epoch 45/200, Batch 16/17, Loss G: 4.838689804077148, Loss D: 0.027055319398641586\n",
      "Epoch 46/200, Batch 0/17, Loss G: 4.819846153259277, Loss D: 0.03717399016022682\n",
      "Epoch 46/200, Batch 1/17, Loss G: 4.945106506347656, Loss D: 0.06518709659576416\n",
      "Epoch 46/200, Batch 2/17, Loss G: 4.5998687744140625, Loss D: 0.1254080832004547\n",
      "Epoch 46/200, Batch 3/17, Loss G: 5.20644474029541, Loss D: 0.10928923636674881\n",
      "Epoch 46/200, Batch 4/17, Loss G: 4.9711408615112305, Loss D: 0.020348627120256424\n",
      "Epoch 46/200, Batch 5/17, Loss G: 4.571434020996094, Loss D: 0.04464774206280708\n",
      "Epoch 46/200, Batch 6/17, Loss G: 4.878576278686523, Loss D: 0.03960064426064491\n",
      "Epoch 46/200, Batch 7/17, Loss G: 4.9994001388549805, Loss D: 0.013163315132260323\n",
      "Epoch 46/200, Batch 8/17, Loss G: 4.738759994506836, Loss D: 0.022360162809491158\n",
      "Epoch 46/200, Batch 9/17, Loss G: 4.945186614990234, Loss D: 0.02002703584730625\n",
      "Epoch 46/200, Batch 10/17, Loss G: 5.053481101989746, Loss D: 0.04373372718691826\n",
      "Epoch 46/200, Batch 11/17, Loss G: 4.832056999206543, Loss D: 0.029569603502750397\n",
      "Epoch 46/200, Batch 12/17, Loss G: 4.782660961151123, Loss D: 0.026288308203220367\n",
      "Epoch 46/200, Batch 13/17, Loss G: 5.036718368530273, Loss D: 0.03186755254864693\n",
      "Epoch 46/200, Batch 14/17, Loss G: 4.7433929443359375, Loss D: 0.029228530824184418\n",
      "Epoch 46/200, Batch 15/17, Loss G: 4.852929592132568, Loss D: 0.024333612993359566\n",
      "Epoch 46/200, Batch 16/17, Loss G: 4.836297035217285, Loss D: 0.013691650703549385\n",
      "Epoch 47/200, Batch 0/17, Loss G: 4.900766372680664, Loss D: 0.011654812842607498\n",
      "Epoch 47/200, Batch 1/17, Loss G: 5.1141157150268555, Loss D: 0.01079804077744484\n",
      "Epoch 47/200, Batch 2/17, Loss G: 4.883018493652344, Loss D: 0.01456533744931221\n",
      "Epoch 47/200, Batch 3/17, Loss G: 5.001836776733398, Loss D: 0.017753269523382187\n",
      "Epoch 47/200, Batch 4/17, Loss G: 4.7935099601745605, Loss D: 0.02181110344827175\n",
      "Epoch 47/200, Batch 5/17, Loss G: 4.987124443054199, Loss D: 0.037151068449020386\n",
      "Epoch 47/200, Batch 6/17, Loss G: 4.61235237121582, Loss D: 0.05410975217819214\n",
      "Epoch 47/200, Batch 7/17, Loss G: 5.206077575683594, Loss D: 0.06393513083457947\n",
      "Epoch 47/200, Batch 8/17, Loss G: 4.8536272048950195, Loss D: 0.03245393931865692\n",
      "Epoch 47/200, Batch 9/17, Loss G: 4.7749457359313965, Loss D: 0.012903088703751564\n",
      "Epoch 47/200, Batch 10/17, Loss G: 5.02340030670166, Loss D: 0.029371537268161774\n",
      "Epoch 47/200, Batch 11/17, Loss G: 4.8472795486450195, Loss D: 0.016160907223820686\n",
      "Epoch 47/200, Batch 12/17, Loss G: 4.741374969482422, Loss D: 0.017631979659199715\n",
      "Epoch 47/200, Batch 13/17, Loss G: 4.918215751647949, Loss D: 0.025298763066530228\n",
      "Epoch 47/200, Batch 14/17, Loss G: 4.9934539794921875, Loss D: 0.019004851579666138\n",
      "Epoch 47/200, Batch 15/17, Loss G: 4.660073280334473, Loss D: 0.01963871344923973\n",
      "Epoch 47/200, Batch 16/17, Loss G: 4.715329170227051, Loss D: 0.031129438430070877\n",
      "Epoch 48/200, Batch 0/17, Loss G: 5.09729528427124, Loss D: 0.041586983948946\n",
      "Epoch 48/200, Batch 1/17, Loss G: 4.696432590484619, Loss D: 0.039676565676927567\n",
      "Epoch 48/200, Batch 2/17, Loss G: 4.935492515563965, Loss D: 0.03211202844977379\n",
      "Epoch 48/200, Batch 3/17, Loss G: 4.932690143585205, Loss D: 0.019831117242574692\n",
      "Epoch 48/200, Batch 4/17, Loss G: 4.717430114746094, Loss D: 0.021172622218728065\n",
      "Epoch 48/200, Batch 5/17, Loss G: 4.890102863311768, Loss D: 0.022653372958302498\n",
      "Epoch 48/200, Batch 6/17, Loss G: 4.785265922546387, Loss D: 0.025207921862602234\n",
      "Epoch 48/200, Batch 7/17, Loss G: 4.888980388641357, Loss D: 0.01905686780810356\n",
      "Epoch 48/200, Batch 8/17, Loss G: 4.833573818206787, Loss D: 0.029508471488952637\n",
      "Epoch 48/200, Batch 9/17, Loss G: 4.63695764541626, Loss D: 0.07171633839607239\n",
      "Epoch 48/200, Batch 10/17, Loss G: 5.0245041847229, Loss D: 0.09547819942235947\n",
      "Epoch 48/200, Batch 11/17, Loss G: 4.5495147705078125, Loss D: 0.060345400124788284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200, Batch 12/17, Loss G: 4.858222007751465, Loss D: 0.02444663643836975\n",
      "Epoch 48/200, Batch 13/17, Loss G: 5.073529243469238, Loss D: 0.00959450751543045\n",
      "Epoch 48/200, Batch 14/17, Loss G: 4.78538703918457, Loss D: 0.020846646279096603\n",
      "Epoch 48/200, Batch 15/17, Loss G: 4.953668594360352, Loss D: 0.014454368501901627\n",
      "Epoch 48/200, Batch 16/17, Loss G: 4.877221584320068, Loss D: 0.023169375956058502\n",
      "Epoch 49/200, Batch 0/17, Loss G: 4.681484699249268, Loss D: 0.025827642530202866\n",
      "Epoch 49/200, Batch 1/17, Loss G: 4.8989033699035645, Loss D: 0.01781011000275612\n",
      "Epoch 49/200, Batch 2/17, Loss G: 4.786625385284424, Loss D: 0.008435016497969627\n",
      "Epoch 49/200, Batch 3/17, Loss G: 4.979122161865234, Loss D: 0.014781982637941837\n",
      "Epoch 49/200, Batch 4/17, Loss G: 5.008680820465088, Loss D: 0.019462088122963905\n",
      "Epoch 49/200, Batch 5/17, Loss G: 4.880311489105225, Loss D: 0.02344437874853611\n",
      "Epoch 49/200, Batch 6/17, Loss G: 4.956204891204834, Loss D: 0.02264050953090191\n",
      "Epoch 49/200, Batch 7/17, Loss G: 4.4405198097229, Loss D: 0.03553546220064163\n",
      "Epoch 49/200, Batch 8/17, Loss G: 4.907549858093262, Loss D: 0.025252584367990494\n",
      "Epoch 49/200, Batch 9/17, Loss G: 4.766900062561035, Loss D: 0.014751729555428028\n",
      "Epoch 49/200, Batch 10/17, Loss G: 5.012240409851074, Loss D: 0.016543950885534286\n",
      "Epoch 49/200, Batch 11/17, Loss G: 4.773320198059082, Loss D: 0.018999136984348297\n",
      "Epoch 49/200, Batch 12/17, Loss G: 4.807694435119629, Loss D: 0.01817391812801361\n",
      "Epoch 49/200, Batch 13/17, Loss G: 5.2022552490234375, Loss D: 0.01770824007689953\n",
      "Epoch 49/200, Batch 14/17, Loss G: 4.7091217041015625, Loss D: 0.03414991870522499\n",
      "Epoch 49/200, Batch 15/17, Loss G: 4.989735126495361, Loss D: 0.07336879521608353\n",
      "Epoch 49/200, Batch 16/17, Loss G: 4.1251935958862305, Loss D: 0.11265663802623749\n",
      "Epoch 50/200, Batch 0/17, Loss G: 4.945903778076172, Loss D: 0.03287066891789436\n",
      "Epoch 50/200, Batch 1/17, Loss G: 4.916753768920898, Loss D: 0.06894904375076294\n",
      "Epoch 50/200, Batch 2/17, Loss G: 4.028103828430176, Loss D: 0.31741106510162354\n",
      "Epoch 50/200, Batch 3/17, Loss G: 4.769632339477539, Loss D: 0.24668638408184052\n",
      "Epoch 50/200, Batch 4/17, Loss G: 4.619088172912598, Loss D: 0.11105228960514069\n",
      "Epoch 50/200, Batch 5/17, Loss G: 4.825905799865723, Loss D: 0.04830207675695419\n",
      "Epoch 50/200, Batch 6/17, Loss G: 4.675257682800293, Loss D: 0.02484060637652874\n",
      "Epoch 50/200, Batch 7/17, Loss G: 4.874971389770508, Loss D: 0.04529264569282532\n",
      "Epoch 50/200, Batch 8/17, Loss G: 4.719658851623535, Loss D: 0.03201550245285034\n",
      "Epoch 50/200, Batch 9/17, Loss G: 4.815343856811523, Loss D: 0.03855512663722038\n",
      "Epoch 50/200, Batch 10/17, Loss G: 4.695228576660156, Loss D: 0.03395083174109459\n",
      "Epoch 50/200, Batch 11/17, Loss G: 4.763235569000244, Loss D: 0.02412203699350357\n",
      "Epoch 50/200, Batch 12/17, Loss G: 4.810073375701904, Loss D: 0.03713592141866684\n",
      "Epoch 50/200, Batch 13/17, Loss G: 4.903968811035156, Loss D: 0.031301699578762054\n",
      "Epoch 50/200, Batch 14/17, Loss G: 4.857898235321045, Loss D: 0.036296822130680084\n",
      "Epoch 50/200, Batch 15/17, Loss G: 4.884685516357422, Loss D: 0.037362344563007355\n",
      "Epoch 50/200, Batch 16/17, Loss G: 4.62896203994751, Loss D: 0.042733293026685715\n",
      "Epoch 51/200, Batch 0/17, Loss G: 5.035778045654297, Loss D: 0.021109990775585175\n",
      "Epoch 51/200, Batch 1/17, Loss G: 4.85672664642334, Loss D: 0.013389919884502888\n",
      "Epoch 51/200, Batch 2/17, Loss G: 4.7484517097473145, Loss D: 0.013636710122227669\n",
      "Epoch 51/200, Batch 3/17, Loss G: 4.884279727935791, Loss D: 0.010845783166587353\n",
      "Epoch 51/200, Batch 4/17, Loss G: 4.877854824066162, Loss D: 0.012211043387651443\n",
      "Epoch 51/200, Batch 5/17, Loss G: 4.736538887023926, Loss D: 0.013468839228153229\n",
      "Epoch 51/200, Batch 6/17, Loss G: 4.778580665588379, Loss D: 0.03167542815208435\n",
      "Epoch 51/200, Batch 7/17, Loss G: 4.653223991394043, Loss D: 0.029227519407868385\n",
      "Epoch 51/200, Batch 8/17, Loss G: 4.9090094566345215, Loss D: 0.023519504815340042\n",
      "Epoch 51/200, Batch 9/17, Loss G: 4.77079439163208, Loss D: 0.01787993125617504\n",
      "Epoch 51/200, Batch 10/17, Loss G: 4.497764587402344, Loss D: 0.04761449620127678\n",
      "Epoch 51/200, Batch 11/17, Loss G: 4.820903778076172, Loss D: 0.06500784307718277\n",
      "Epoch 51/200, Batch 12/17, Loss G: 4.761378288269043, Loss D: 0.012647658586502075\n",
      "Epoch 51/200, Batch 13/17, Loss G: 4.7351393699646, Loss D: 0.03813944756984711\n",
      "Epoch 51/200, Batch 14/17, Loss G: 5.171036243438721, Loss D: 0.12277927249670029\n",
      "Epoch 51/200, Batch 15/17, Loss G: 4.173788547515869, Loss D: 0.2575393617153168\n",
      "Epoch 51/200, Batch 16/17, Loss G: 4.95693302154541, Loss D: 0.20719033479690552\n",
      "Epoch 52/200, Batch 0/17, Loss G: 3.77691650390625, Loss D: 0.41296547651290894\n",
      "Epoch 52/200, Batch 1/17, Loss G: 4.312257766723633, Loss D: 0.13428233563899994\n",
      "Epoch 52/200, Batch 2/17, Loss G: 4.840604782104492, Loss D: 0.3070640563964844\n",
      "Epoch 52/200, Batch 3/17, Loss G: 4.559075832366943, Loss D: 0.15963418781757355\n",
      "Epoch 52/200, Batch 4/17, Loss G: 4.575685501098633, Loss D: 0.07223834097385406\n",
      "Epoch 52/200, Batch 5/17, Loss G: 4.73326301574707, Loss D: 0.06717857718467712\n",
      "Epoch 52/200, Batch 6/17, Loss G: 4.598114967346191, Loss D: 0.10614567250013351\n",
      "Epoch 52/200, Batch 7/17, Loss G: 4.852167129516602, Loss D: 0.11280156672000885\n",
      "Epoch 52/200, Batch 8/17, Loss G: 4.595941543579102, Loss D: 0.07485050708055496\n",
      "Epoch 52/200, Batch 9/17, Loss G: 4.678999900817871, Loss D: 0.07905116677284241\n",
      "Epoch 52/200, Batch 10/17, Loss G: 4.721132278442383, Loss D: 0.06686727702617645\n",
      "Epoch 52/200, Batch 11/17, Loss G: 4.285621166229248, Loss D: 0.15086986124515533\n",
      "Epoch 52/200, Batch 12/17, Loss G: 5.149652004241943, Loss D: 0.20961885154247284\n",
      "Epoch 52/200, Batch 13/17, Loss G: 4.733145713806152, Loss D: 0.06932984292507172\n",
      "Epoch 52/200, Batch 14/17, Loss G: 4.294663429260254, Loss D: 0.09699404239654541\n",
      "Epoch 52/200, Batch 15/17, Loss G: 4.85482120513916, Loss D: 0.11705990135669708\n",
      "Epoch 52/200, Batch 16/17, Loss G: 4.700185298919678, Loss D: 0.04946577176451683\n",
      "Epoch 53/200, Batch 0/17, Loss G: 4.900885105133057, Loss D: 0.03569542244076729\n",
      "Epoch 53/200, Batch 1/17, Loss G: 5.011260032653809, Loss D: 0.036972109228372574\n",
      "Epoch 53/200, Batch 2/17, Loss G: 4.587608337402344, Loss D: 0.0404081791639328\n",
      "Epoch 53/200, Batch 3/17, Loss G: 4.851165294647217, Loss D: 0.02308548055589199\n",
      "Epoch 53/200, Batch 4/17, Loss G: 4.859198570251465, Loss D: 0.04191138967871666\n",
      "Epoch 53/200, Batch 5/17, Loss G: 4.6427106857299805, Loss D: 0.03438456356525421\n",
      "Epoch 53/200, Batch 6/17, Loss G: 4.684540748596191, Loss D: 0.037546247243881226\n",
      "Epoch 53/200, Batch 7/17, Loss G: 4.881884574890137, Loss D: 0.02494647167623043\n",
      "Epoch 53/200, Batch 8/17, Loss G: 4.642988681793213, Loss D: 0.036048710346221924\n",
      "Epoch 53/200, Batch 9/17, Loss G: 4.574726104736328, Loss D: 0.0347316674888134\n",
      "Epoch 53/200, Batch 10/17, Loss G: 4.7619829177856445, Loss D: 0.04703126102685928\n",
      "Epoch 53/200, Batch 11/17, Loss G: 4.469569206237793, Loss D: 0.07179779559373856\n",
      "Epoch 53/200, Batch 12/17, Loss G: 5.111341953277588, Loss D: 0.04797818139195442\n",
      "Epoch 53/200, Batch 13/17, Loss G: 4.7814531326293945, Loss D: 0.01819652132689953\n",
      "Epoch 53/200, Batch 14/17, Loss G: 4.626755714416504, Loss D: 0.019952604547142982\n",
      "Epoch 53/200, Batch 15/17, Loss G: 4.7205400466918945, Loss D: 0.0327080562710762\n",
      "Epoch 53/200, Batch 16/17, Loss G: 4.968391418457031, Loss D: 0.03347796946763992\n",
      "Epoch 54/200, Batch 0/17, Loss G: 4.956073760986328, Loss D: 0.015714308246970177\n",
      "Epoch 54/200, Batch 1/17, Loss G: 4.810449600219727, Loss D: 0.017045818269252777\n",
      "Epoch 54/200, Batch 2/17, Loss G: 4.6638593673706055, Loss D: 0.015385309234261513\n",
      "Epoch 54/200, Batch 3/17, Loss G: 4.734210014343262, Loss D: 0.02934948354959488\n",
      "Epoch 54/200, Batch 4/17, Loss G: 4.757126808166504, Loss D: 0.03529857099056244\n",
      "Epoch 54/200, Batch 5/17, Loss G: 4.511599063873291, Loss D: 0.026792723685503006\n",
      "Epoch 54/200, Batch 6/17, Loss G: 4.8719162940979, Loss D: 0.027166815474629402\n",
      "Epoch 54/200, Batch 7/17, Loss G: 4.876277923583984, Loss D: 0.046866875141859055\n",
      "Epoch 54/200, Batch 8/17, Loss G: 4.643819808959961, Loss D: 0.045348573476076126\n",
      "Epoch 54/200, Batch 9/17, Loss G: 4.984499931335449, Loss D: 0.025743328034877777\n",
      "Epoch 54/200, Batch 10/17, Loss G: 4.621584892272949, Loss D: 0.01831725984811783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200, Batch 11/17, Loss G: 4.812921524047852, Loss D: 0.018922746181488037\n",
      "Epoch 54/200, Batch 12/17, Loss G: 4.672669410705566, Loss D: 0.014206690713763237\n",
      "Epoch 54/200, Batch 13/17, Loss G: 4.704191207885742, Loss D: 0.022168990224599838\n",
      "Epoch 54/200, Batch 14/17, Loss G: 4.693058013916016, Loss D: 0.014849845319986343\n",
      "Epoch 54/200, Batch 15/17, Loss G: 4.627791404724121, Loss D: 0.02070584148168564\n",
      "Epoch 54/200, Batch 16/17, Loss G: 4.75831937789917, Loss D: 0.05027076601982117\n",
      "Epoch 55/200, Batch 0/17, Loss G: 4.6872453689575195, Loss D: 0.023063289001584053\n",
      "Epoch 55/200, Batch 1/17, Loss G: 4.769160270690918, Loss D: 0.011872617527842522\n",
      "Epoch 55/200, Batch 2/17, Loss G: 4.863604545593262, Loss D: 0.02415977045893669\n",
      "Epoch 55/200, Batch 3/17, Loss G: 4.812727928161621, Loss D: 0.017072387039661407\n",
      "Epoch 55/200, Batch 4/17, Loss G: 4.438904762268066, Loss D: 0.036327097564935684\n",
      "Epoch 55/200, Batch 5/17, Loss G: 5.242630958557129, Loss D: 0.10731205344200134\n",
      "Epoch 55/200, Batch 6/17, Loss G: 4.364797592163086, Loss D: 0.19402867555618286\n",
      "Epoch 55/200, Batch 7/17, Loss G: 4.860390663146973, Loss D: 0.05417720228433609\n",
      "Epoch 55/200, Batch 8/17, Loss G: 4.77243185043335, Loss D: 0.01807212457060814\n",
      "Epoch 55/200, Batch 9/17, Loss G: 4.460290908813477, Loss D: 0.03855038806796074\n",
      "Epoch 55/200, Batch 10/17, Loss G: 4.8429856300354, Loss D: 0.02530234307050705\n",
      "Epoch 55/200, Batch 11/17, Loss G: 4.772030830383301, Loss D: 0.02698291838169098\n",
      "Epoch 55/200, Batch 12/17, Loss G: 4.6562323570251465, Loss D: 0.03252395614981651\n",
      "Epoch 55/200, Batch 13/17, Loss G: 4.6004743576049805, Loss D: 0.0193574670702219\n",
      "Epoch 55/200, Batch 14/17, Loss G: 4.827714920043945, Loss D: 0.039289604872465134\n",
      "Epoch 55/200, Batch 15/17, Loss G: 4.531867980957031, Loss D: 0.03980031609535217\n",
      "Epoch 55/200, Batch 16/17, Loss G: 4.934366703033447, Loss D: 0.009290956892073154\n",
      "Epoch 56/200, Batch 0/17, Loss G: 4.664917945861816, Loss D: 0.012557734735310078\n",
      "Epoch 56/200, Batch 1/17, Loss G: 4.769380569458008, Loss D: 0.013621686026453972\n",
      "Epoch 56/200, Batch 2/17, Loss G: 4.396910667419434, Loss D: 0.01849306747317314\n",
      "Epoch 56/200, Batch 3/17, Loss G: 4.649367332458496, Loss D: 0.014999079518020153\n",
      "Epoch 56/200, Batch 4/17, Loss G: 4.714798927307129, Loss D: 0.015823662281036377\n",
      "Epoch 56/200, Batch 5/17, Loss G: 4.787250995635986, Loss D: 0.04619714245200157\n",
      "Epoch 56/200, Batch 6/17, Loss G: 4.207370758056641, Loss D: 0.0689282938838005\n",
      "Epoch 56/200, Batch 7/17, Loss G: 4.821413040161133, Loss D: 0.09249772131443024\n",
      "Epoch 56/200, Batch 8/17, Loss G: 4.4347333908081055, Loss D: 0.07089779525995255\n",
      "Epoch 56/200, Batch 9/17, Loss G: 4.958505153656006, Loss D: 0.04665297269821167\n",
      "Epoch 56/200, Batch 10/17, Loss G: 4.686804294586182, Loss D: 0.022388610988855362\n",
      "Epoch 56/200, Batch 11/17, Loss G: 4.807605266571045, Loss D: 0.017860613763332367\n",
      "Epoch 56/200, Batch 12/17, Loss G: 4.820752143859863, Loss D: 0.027061518281698227\n",
      "Epoch 56/200, Batch 13/17, Loss G: 4.7720746994018555, Loss D: 0.01432749442756176\n",
      "Epoch 56/200, Batch 14/17, Loss G: 4.540579319000244, Loss D: 0.016965460032224655\n",
      "Epoch 56/200, Batch 15/17, Loss G: 4.79958438873291, Loss D: 0.02298858016729355\n",
      "Epoch 56/200, Batch 16/17, Loss G: 4.4865522384643555, Loss D: 0.045497991144657135\n",
      "Epoch 57/200, Batch 0/17, Loss G: 4.715865135192871, Loss D: 0.05293320119380951\n",
      "Epoch 57/200, Batch 1/17, Loss G: 4.430484771728516, Loss D: 0.04538441076874733\n",
      "Epoch 57/200, Batch 2/17, Loss G: 4.785370826721191, Loss D: 0.017572257667779922\n",
      "Epoch 57/200, Batch 3/17, Loss G: 4.854344367980957, Loss D: 0.01068742573261261\n",
      "Epoch 57/200, Batch 4/17, Loss G: 4.7441864013671875, Loss D: 0.02271641045808792\n",
      "Epoch 57/200, Batch 5/17, Loss G: 4.678797721862793, Loss D: 0.026759330183267593\n",
      "Epoch 57/200, Batch 6/17, Loss G: 4.85733699798584, Loss D: 0.050954267382621765\n",
      "Epoch 57/200, Batch 7/17, Loss G: 4.537793159484863, Loss D: 0.0560457669198513\n",
      "Epoch 57/200, Batch 8/17, Loss G: 4.803271293640137, Loss D: 0.023486584424972534\n",
      "Epoch 57/200, Batch 9/17, Loss G: 4.772322654724121, Loss D: 0.01644582860171795\n",
      "Epoch 57/200, Batch 10/17, Loss G: 4.746137619018555, Loss D: 0.011468899436295033\n",
      "Epoch 57/200, Batch 11/17, Loss G: 4.596782684326172, Loss D: 0.016914669424295425\n",
      "Epoch 57/200, Batch 12/17, Loss G: 4.682782173156738, Loss D: 0.02182566002011299\n",
      "Epoch 57/200, Batch 13/17, Loss G: 4.679286003112793, Loss D: 0.018423430621623993\n",
      "Epoch 57/200, Batch 14/17, Loss G: 4.649500846862793, Loss D: 0.022961076349020004\n",
      "Epoch 57/200, Batch 15/17, Loss G: 4.842555046081543, Loss D: 0.010663308203220367\n",
      "Epoch 57/200, Batch 16/17, Loss G: 4.624789237976074, Loss D: 0.014196468517184258\n",
      "Epoch 58/200, Batch 0/17, Loss G: 4.7212815284729, Loss D: 0.01221456378698349\n",
      "Epoch 58/200, Batch 1/17, Loss G: 4.778651237487793, Loss D: 0.014878871850669384\n",
      "Epoch 58/200, Batch 2/17, Loss G: 4.520173072814941, Loss D: 0.01731869950890541\n",
      "Epoch 58/200, Batch 3/17, Loss G: 4.507077217102051, Loss D: 0.016253918409347534\n",
      "Epoch 58/200, Batch 4/17, Loss G: 4.806356430053711, Loss D: 0.014622421935200691\n",
      "Epoch 58/200, Batch 5/17, Loss G: 4.725363731384277, Loss D: 0.010499944910407066\n",
      "Epoch 58/200, Batch 6/17, Loss G: 4.7643327713012695, Loss D: 0.009011298418045044\n",
      "Epoch 58/200, Batch 7/17, Loss G: 4.723036766052246, Loss D: 0.022183937951922417\n",
      "Epoch 58/200, Batch 8/17, Loss G: 4.486733436584473, Loss D: 0.03452537953853607\n",
      "Epoch 58/200, Batch 9/17, Loss G: 4.661786079406738, Loss D: 0.02617642469704151\n",
      "Epoch 58/200, Batch 10/17, Loss G: 4.695922374725342, Loss D: 0.01267928071320057\n",
      "Epoch 58/200, Batch 11/17, Loss G: 4.491337776184082, Loss D: 0.013893656432628632\n",
      "Epoch 58/200, Batch 12/17, Loss G: 4.817709922790527, Loss D: 0.019030380994081497\n",
      "Epoch 58/200, Batch 13/17, Loss G: 4.661470413208008, Loss D: 0.011791137978434563\n",
      "Epoch 58/200, Batch 14/17, Loss G: 4.720454216003418, Loss D: 0.005539551377296448\n",
      "Epoch 58/200, Batch 15/17, Loss G: 4.903382301330566, Loss D: 0.005682932212948799\n",
      "Epoch 58/200, Batch 16/17, Loss G: 4.905279159545898, Loss D: 0.013442621566355228\n",
      "Epoch 59/200, Batch 0/17, Loss G: 4.538608074188232, Loss D: 0.01447156723588705\n",
      "Epoch 59/200, Batch 1/17, Loss G: 4.811585426330566, Loss D: 0.009371966123580933\n",
      "Epoch 59/200, Batch 2/17, Loss G: 4.598917007446289, Loss D: 0.011352877132594585\n",
      "Epoch 59/200, Batch 3/17, Loss G: 4.644334316253662, Loss D: 0.01696711592376232\n",
      "Epoch 59/200, Batch 4/17, Loss G: 4.809259414672852, Loss D: 0.009191760793328285\n",
      "Epoch 59/200, Batch 5/17, Loss G: 4.695005416870117, Loss D: 0.0063364640809595585\n",
      "Epoch 59/200, Batch 6/17, Loss G: 4.574308395385742, Loss D: 0.016604108735919\n",
      "Epoch 59/200, Batch 7/17, Loss G: 4.824766159057617, Loss D: 0.012132778763771057\n",
      "Epoch 59/200, Batch 8/17, Loss G: 4.561086177825928, Loss D: 0.013318542391061783\n",
      "Epoch 59/200, Batch 9/17, Loss G: 4.703191757202148, Loss D: 0.008104182779788971\n",
      "Epoch 59/200, Batch 10/17, Loss G: 4.391633987426758, Loss D: 0.01175569836050272\n",
      "Epoch 59/200, Batch 11/17, Loss G: 4.606176853179932, Loss D: 0.0069459183141589165\n",
      "Epoch 59/200, Batch 12/17, Loss G: 4.949702739715576, Loss D: 0.005636529065668583\n",
      "Epoch 59/200, Batch 13/17, Loss G: 4.793776988983154, Loss D: 0.013363965786993504\n",
      "Epoch 59/200, Batch 14/17, Loss G: 4.541980266571045, Loss D: 0.035472992807626724\n",
      "Epoch 59/200, Batch 15/17, Loss G: 4.9340314865112305, Loss D: 0.051915135234594345\n",
      "Epoch 59/200, Batch 16/17, Loss G: 4.637701034545898, Loss D: 0.015717843547463417\n",
      "Epoch 60/200, Batch 0/17, Loss G: 4.588545322418213, Loss D: 0.04751107469201088\n",
      "Epoch 60/200, Batch 1/17, Loss G: 4.8834028244018555, Loss D: 0.15747873485088348\n",
      "Epoch 60/200, Batch 2/17, Loss G: 4.051965236663818, Loss D: 0.25957855582237244\n",
      "Epoch 60/200, Batch 3/17, Loss G: 4.864055633544922, Loss D: 0.19357354938983917\n",
      "Epoch 60/200, Batch 4/17, Loss G: 3.709913730621338, Loss D: 0.4398922920227051\n",
      "Epoch 60/200, Batch 5/17, Loss G: 4.055072784423828, Loss D: 0.36075645685195923\n",
      "Epoch 60/200, Batch 6/17, Loss G: 4.1967034339904785, Loss D: 0.27419471740722656\n",
      "Epoch 60/200, Batch 7/17, Loss G: 4.377581596374512, Loss D: 0.1607077717781067\n",
      "Epoch 60/200, Batch 8/17, Loss G: 4.605593681335449, Loss D: 0.18241530656814575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200, Batch 9/17, Loss G: 4.2122650146484375, Loss D: 0.13155055046081543\n",
      "Epoch 60/200, Batch 10/17, Loss G: 4.417691707611084, Loss D: 0.05195891484618187\n",
      "Epoch 60/200, Batch 11/17, Loss G: 4.598076820373535, Loss D: 0.052253976464271545\n",
      "Epoch 60/200, Batch 12/17, Loss G: 4.369256973266602, Loss D: 0.048794858157634735\n",
      "Epoch 60/200, Batch 13/17, Loss G: 4.31896448135376, Loss D: 0.08005757629871368\n",
      "Epoch 60/200, Batch 14/17, Loss G: 4.787261962890625, Loss D: 0.12534397840499878\n",
      "Epoch 60/200, Batch 15/17, Loss G: 4.101524353027344, Loss D: 0.21149003505706787\n",
      "Epoch 60/200, Batch 16/17, Loss G: 4.751282215118408, Loss D: 0.11311524361371994\n",
      "Epoch 61/200, Batch 0/17, Loss G: 4.38555908203125, Loss D: 0.06691761314868927\n",
      "Epoch 61/200, Batch 1/17, Loss G: 4.672447681427002, Loss D: 0.03460812196135521\n",
      "Epoch 61/200, Batch 2/17, Loss G: 4.588015079498291, Loss D: 0.0670945942401886\n",
      "Epoch 61/200, Batch 3/17, Loss G: 4.488257884979248, Loss D: 0.06419070810079575\n",
      "Epoch 61/200, Batch 4/17, Loss G: 4.809144020080566, Loss D: 0.07378797233104706\n",
      "Epoch 61/200, Batch 5/17, Loss G: 4.586820602416992, Loss D: 0.04552571102976799\n",
      "Epoch 61/200, Batch 6/17, Loss G: 4.542413711547852, Loss D: 0.054138049483299255\n",
      "Epoch 61/200, Batch 7/17, Loss G: 4.908670425415039, Loss D: 0.06335271149873734\n",
      "Epoch 61/200, Batch 8/17, Loss G: 4.240914344787598, Loss D: 0.07219576835632324\n",
      "Epoch 61/200, Batch 9/17, Loss G: 4.986212730407715, Loss D: 0.05153997614979744\n",
      "Epoch 61/200, Batch 10/17, Loss G: 4.420126438140869, Loss D: 0.036645106971263885\n",
      "Epoch 61/200, Batch 11/17, Loss G: 4.630982398986816, Loss D: 0.02455596625804901\n",
      "Epoch 61/200, Batch 12/17, Loss G: 4.835243225097656, Loss D: 0.014900620095431805\n",
      "Epoch 61/200, Batch 13/17, Loss G: 4.767400741577148, Loss D: 0.02106224000453949\n",
      "Epoch 61/200, Batch 14/17, Loss G: 4.493005752563477, Loss D: 0.027798350900411606\n",
      "Epoch 61/200, Batch 15/17, Loss G: 4.662484169006348, Loss D: 0.01698972098529339\n",
      "Epoch 61/200, Batch 16/17, Loss G: 4.45588493347168, Loss D: 0.030012572184205055\n",
      "Epoch 62/200, Batch 0/17, Loss G: 4.725897789001465, Loss D: 0.013255922123789787\n",
      "Epoch 62/200, Batch 1/17, Loss G: 4.638127326965332, Loss D: 0.021335996687412262\n",
      "Epoch 62/200, Batch 2/17, Loss G: 4.647538661956787, Loss D: 0.024360286071896553\n",
      "Epoch 62/200, Batch 3/17, Loss G: 4.3781561851501465, Loss D: 0.05295051261782646\n",
      "Epoch 62/200, Batch 4/17, Loss G: 4.789134979248047, Loss D: 0.0721638947725296\n",
      "Epoch 62/200, Batch 5/17, Loss G: 4.332571029663086, Loss D: 0.040899042040109634\n",
      "Epoch 62/200, Batch 6/17, Loss G: 4.696793556213379, Loss D: 0.012758990749716759\n",
      "Epoch 62/200, Batch 7/17, Loss G: 4.657018184661865, Loss D: 0.01744573563337326\n",
      "Epoch 62/200, Batch 8/17, Loss G: 4.525190353393555, Loss D: 0.019151367247104645\n",
      "Epoch 62/200, Batch 9/17, Loss G: 4.3698930740356445, Loss D: 0.015056838281452656\n",
      "Epoch 62/200, Batch 10/17, Loss G: 4.404657363891602, Loss D: 0.021273545920848846\n",
      "Epoch 62/200, Batch 11/17, Loss G: 4.681900978088379, Loss D: 0.02152351289987564\n",
      "Epoch 62/200, Batch 12/17, Loss G: 4.518965721130371, Loss D: 0.01830216497182846\n",
      "Epoch 62/200, Batch 13/17, Loss G: 4.680901050567627, Loss D: 0.017517775297164917\n",
      "Epoch 62/200, Batch 14/17, Loss G: 4.7751970291137695, Loss D: 0.02359423041343689\n",
      "Epoch 62/200, Batch 15/17, Loss G: 4.463894844055176, Loss D: 0.02042257785797119\n",
      "Epoch 62/200, Batch 16/17, Loss G: 4.873843193054199, Loss D: 0.01443767361342907\n",
      "Epoch 63/200, Batch 0/17, Loss G: 4.670403480529785, Loss D: 0.014502458274364471\n",
      "Epoch 63/200, Batch 1/17, Loss G: 4.800225257873535, Loss D: 0.011931844055652618\n",
      "Epoch 63/200, Batch 2/17, Loss G: 4.6840667724609375, Loss D: 0.016514044255018234\n",
      "Epoch 63/200, Batch 3/17, Loss G: 4.56734561920166, Loss D: 0.028437556698918343\n",
      "Epoch 63/200, Batch 4/17, Loss G: 4.777738094329834, Loss D: 0.014798318967223167\n",
      "Epoch 63/200, Batch 5/17, Loss G: 4.83726692199707, Loss D: 0.011345765553414822\n",
      "Epoch 63/200, Batch 6/17, Loss G: 4.503777503967285, Loss D: 0.01220131665468216\n",
      "Epoch 63/200, Batch 7/17, Loss G: 4.770371437072754, Loss D: 0.0037983288057148457\n",
      "Epoch 63/200, Batch 8/17, Loss G: 4.54041051864624, Loss D: 0.002481706440448761\n",
      "Epoch 63/200, Batch 9/17, Loss G: 4.493978023529053, Loss D: 0.007135421968996525\n",
      "Epoch 63/200, Batch 10/17, Loss G: 4.426639556884766, Loss D: 0.006140246521681547\n",
      "Epoch 63/200, Batch 11/17, Loss G: 4.554880142211914, Loss D: 0.009352143853902817\n",
      "Epoch 63/200, Batch 12/17, Loss G: 4.612331390380859, Loss D: 0.014021575450897217\n",
      "Epoch 63/200, Batch 13/17, Loss G: 4.605440139770508, Loss D: 0.007585376966744661\n",
      "Epoch 63/200, Batch 14/17, Loss G: 4.3976898193359375, Loss D: 0.009138206019997597\n",
      "Epoch 63/200, Batch 15/17, Loss G: 4.635481357574463, Loss D: 0.018213417381048203\n",
      "Epoch 63/200, Batch 16/17, Loss G: 4.559052467346191, Loss D: 0.025923598557710648\n",
      "Epoch 64/200, Batch 0/17, Loss G: 4.622645854949951, Loss D: 0.01791882887482643\n",
      "Epoch 64/200, Batch 1/17, Loss G: 4.693918228149414, Loss D: 0.009133754298090935\n",
      "Epoch 64/200, Batch 2/17, Loss G: 4.407958984375, Loss D: 0.030813291668891907\n",
      "Epoch 64/200, Batch 3/17, Loss G: 4.893853187561035, Loss D: 0.04840654879808426\n",
      "Epoch 64/200, Batch 4/17, Loss G: 4.259355068206787, Loss D: 0.04108563810586929\n",
      "Epoch 64/200, Batch 5/17, Loss G: 4.664634704589844, Loss D: 0.009408418089151382\n",
      "Epoch 64/200, Batch 6/17, Loss G: 4.752399921417236, Loss D: 0.014447296969592571\n",
      "Epoch 64/200, Batch 7/17, Loss G: 4.463291168212891, Loss D: 0.004303593188524246\n",
      "Epoch 64/200, Batch 8/17, Loss G: 4.416396141052246, Loss D: 0.0504741333425045\n",
      "Epoch 64/200, Batch 9/17, Loss G: 4.920431137084961, Loss D: 0.12990739941596985\n",
      "Epoch 64/200, Batch 10/17, Loss G: 4.149477005004883, Loss D: 0.07479277998209\n",
      "Epoch 64/200, Batch 11/17, Loss G: 4.320909023284912, Loss D: 0.043261703103780746\n",
      "Epoch 64/200, Batch 12/17, Loss G: 4.891071319580078, Loss D: 0.20081166923046112\n",
      "Epoch 64/200, Batch 13/17, Loss G: 4.2568511962890625, Loss D: 0.09787347912788391\n",
      "Epoch 64/200, Batch 14/17, Loss G: 4.438327789306641, Loss D: 0.06199569255113602\n",
      "Epoch 64/200, Batch 15/17, Loss G: 4.756510257720947, Loss D: 0.38767534494400024\n",
      "Epoch 64/200, Batch 16/17, Loss G: 4.43312931060791, Loss D: 0.06852563470602036\n",
      "Epoch 65/200, Batch 0/17, Loss G: 4.050516605377197, Loss D: 0.15215501189231873\n",
      "Epoch 65/200, Batch 1/17, Loss G: 4.743443012237549, Loss D: 0.25735408067703247\n",
      "Epoch 65/200, Batch 2/17, Loss G: 4.247208595275879, Loss D: 0.08830903470516205\n",
      "Epoch 65/200, Batch 3/17, Loss G: 4.441392421722412, Loss D: 0.04946140944957733\n",
      "Epoch 65/200, Batch 4/17, Loss G: 4.584783554077148, Loss D: 0.19412706792354584\n",
      "Epoch 65/200, Batch 5/17, Loss G: 4.235846519470215, Loss D: 0.18103553354740143\n",
      "Epoch 65/200, Batch 6/17, Loss G: 4.7372846603393555, Loss D: 0.05443459004163742\n",
      "Epoch 65/200, Batch 7/17, Loss G: 4.8329572677612305, Loss D: 0.017498517408967018\n",
      "Epoch 65/200, Batch 8/17, Loss G: 4.277365207672119, Loss D: 0.06477592140436172\n",
      "Epoch 65/200, Batch 9/17, Loss G: 4.724175453186035, Loss D: 0.06994432210922241\n",
      "Epoch 65/200, Batch 10/17, Loss G: 4.214847564697266, Loss D: 0.0291250292211771\n",
      "Epoch 65/200, Batch 11/17, Loss G: 4.422767639160156, Loss D: 0.03507514297962189\n",
      "Epoch 65/200, Batch 12/17, Loss G: 4.698848247528076, Loss D: 0.022163087502121925\n",
      "Epoch 65/200, Batch 13/17, Loss G: 4.817577838897705, Loss D: 0.013653597794473171\n",
      "Epoch 65/200, Batch 14/17, Loss G: 4.452567100524902, Loss D: 0.034649595618247986\n",
      "Epoch 65/200, Batch 15/17, Loss G: 4.72113037109375, Loss D: 0.028227664530277252\n",
      "Epoch 65/200, Batch 16/17, Loss G: 4.319809436798096, Loss D: 0.055269867181777954\n",
      "Epoch 66/200, Batch 0/17, Loss G: 4.756680965423584, Loss D: 0.07128164172172546\n",
      "Epoch 66/200, Batch 1/17, Loss G: 4.390010833740234, Loss D: 0.021596793085336685\n",
      "Epoch 66/200, Batch 2/17, Loss G: 4.5789923667907715, Loss D: 0.01847836747765541\n",
      "Epoch 66/200, Batch 3/17, Loss G: 4.56926155090332, Loss D: 0.01680484414100647\n",
      "Epoch 66/200, Batch 4/17, Loss G: 4.8117828369140625, Loss D: 0.010185444727540016\n",
      "Epoch 66/200, Batch 5/17, Loss G: 4.474545955657959, Loss D: 0.023728733882308006\n",
      "Epoch 66/200, Batch 6/17, Loss G: 4.381896495819092, Loss D: 0.0133134126663208\n",
      "Epoch 66/200, Batch 7/17, Loss G: 4.595680236816406, Loss D: 0.013354959897696972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200, Batch 8/17, Loss G: 4.588832855224609, Loss D: 0.015115863643586636\n",
      "Epoch 66/200, Batch 9/17, Loss G: 4.621053695678711, Loss D: 0.014310418628156185\n",
      "Epoch 66/200, Batch 10/17, Loss G: 4.675909996032715, Loss D: 0.010593377985060215\n",
      "Epoch 66/200, Batch 11/17, Loss G: 4.436333656311035, Loss D: 0.012392012402415276\n",
      "Epoch 66/200, Batch 12/17, Loss G: 4.503229141235352, Loss D: 0.007165049202740192\n",
      "Epoch 66/200, Batch 13/17, Loss G: 4.368137836456299, Loss D: 0.01930476725101471\n",
      "Epoch 66/200, Batch 14/17, Loss G: 4.616664886474609, Loss D: 0.009447310119867325\n",
      "Epoch 66/200, Batch 15/17, Loss G: 4.545426368713379, Loss D: 0.01259587798267603\n",
      "Epoch 66/200, Batch 16/17, Loss G: 4.72504997253418, Loss D: 0.01107664406299591\n",
      "Epoch 67/200, Batch 0/17, Loss G: 4.54244327545166, Loss D: 0.014400774613022804\n",
      "Epoch 67/200, Batch 1/17, Loss G: 4.588339328765869, Loss D: 0.014961021021008492\n",
      "Epoch 67/200, Batch 2/17, Loss G: 4.726019859313965, Loss D: 0.008063706569373608\n",
      "Epoch 67/200, Batch 3/17, Loss G: 4.587968826293945, Loss D: 0.01481299102306366\n",
      "Epoch 67/200, Batch 4/17, Loss G: 4.480851173400879, Loss D: 0.009022952057421207\n",
      "Epoch 67/200, Batch 5/17, Loss G: 4.460063934326172, Loss D: 0.010682966560125351\n",
      "Epoch 67/200, Batch 6/17, Loss G: 4.5496721267700195, Loss D: 0.008857685141265392\n",
      "Epoch 67/200, Batch 7/17, Loss G: 4.561784744262695, Loss D: 0.012796360068023205\n",
      "Epoch 67/200, Batch 8/17, Loss G: 4.474296569824219, Loss D: 0.01363874040544033\n",
      "Epoch 67/200, Batch 9/17, Loss G: 4.659206390380859, Loss D: 0.009010352194309235\n",
      "Epoch 67/200, Batch 10/17, Loss G: 4.385994911193848, Loss D: 0.012513767927885056\n",
      "Epoch 67/200, Batch 11/17, Loss G: 4.498825550079346, Loss D: 0.010968279093503952\n",
      "Epoch 67/200, Batch 12/17, Loss G: 4.464847564697266, Loss D: 0.017504118382930756\n",
      "Epoch 67/200, Batch 13/17, Loss G: 4.544498443603516, Loss D: 0.010400010272860527\n",
      "Epoch 67/200, Batch 14/17, Loss G: 4.494903564453125, Loss D: 0.007908977568149567\n",
      "Epoch 67/200, Batch 15/17, Loss G: 4.524786472320557, Loss D: 0.011701278388500214\n",
      "Epoch 67/200, Batch 16/17, Loss G: 4.46610164642334, Loss D: 0.024516986683011055\n",
      "Epoch 68/200, Batch 0/17, Loss G: 4.647169589996338, Loss D: 0.04195503145456314\n",
      "Epoch 68/200, Batch 1/17, Loss G: 4.1588544845581055, Loss D: 0.050296079367399216\n",
      "Epoch 68/200, Batch 2/17, Loss G: 4.682967185974121, Loss D: 0.02603781409561634\n",
      "Epoch 68/200, Batch 3/17, Loss G: 4.603126525878906, Loss D: 0.00858053844422102\n",
      "Epoch 68/200, Batch 4/17, Loss G: 4.426745414733887, Loss D: 0.017988426610827446\n",
      "Epoch 68/200, Batch 5/17, Loss G: 4.673906326293945, Loss D: 0.01596377231180668\n",
      "Epoch 68/200, Batch 6/17, Loss G: 4.667965412139893, Loss D: 0.03274780511856079\n",
      "Epoch 68/200, Batch 7/17, Loss G: 4.64154052734375, Loss D: 0.010677061975002289\n",
      "Epoch 68/200, Batch 8/17, Loss G: 4.38241720199585, Loss D: 0.008113653399050236\n",
      "Epoch 68/200, Batch 9/17, Loss G: 4.302732944488525, Loss D: 0.022488219663500786\n",
      "Epoch 68/200, Batch 10/17, Loss G: 4.562117576599121, Loss D: 0.055801160633563995\n",
      "Epoch 68/200, Batch 11/17, Loss G: 4.460466384887695, Loss D: 0.04993775114417076\n",
      "Epoch 68/200, Batch 12/17, Loss G: 4.630701065063477, Loss D: 0.016246460378170013\n",
      "Epoch 68/200, Batch 13/17, Loss G: 4.62435245513916, Loss D: 0.009611779823899269\n",
      "Epoch 68/200, Batch 14/17, Loss G: 4.629545211791992, Loss D: 0.013713019900023937\n",
      "Epoch 68/200, Batch 15/17, Loss G: 4.539529323577881, Loss D: 0.009615622460842133\n",
      "Epoch 68/200, Batch 16/17, Loss G: 4.643732070922852, Loss D: 0.013724606484174728\n",
      "Epoch 69/200, Batch 0/17, Loss G: 4.512584686279297, Loss D: 0.024737920612096786\n",
      "Epoch 69/200, Batch 1/17, Loss G: 4.55465030670166, Loss D: 0.019716136157512665\n",
      "Epoch 69/200, Batch 2/17, Loss G: 4.555743217468262, Loss D: 0.019832350313663483\n",
      "Epoch 69/200, Batch 3/17, Loss G: 4.461451053619385, Loss D: 0.00880042091012001\n",
      "Epoch 69/200, Batch 4/17, Loss G: 4.6333465576171875, Loss D: 0.016207925975322723\n",
      "Epoch 69/200, Batch 5/17, Loss G: 4.401568412780762, Loss D: 0.01585274003446102\n",
      "Epoch 69/200, Batch 6/17, Loss G: 4.487420082092285, Loss D: 0.012312771752476692\n",
      "Epoch 69/200, Batch 7/17, Loss G: 4.500843524932861, Loss D: 0.014141397550702095\n",
      "Epoch 69/200, Batch 8/17, Loss G: 4.655344009399414, Loss D: 0.011926851235330105\n",
      "Epoch 69/200, Batch 9/17, Loss G: 4.480928421020508, Loss D: 0.010676387697458267\n",
      "Epoch 69/200, Batch 10/17, Loss G: 4.385735988616943, Loss D: 0.013943458907306194\n",
      "Epoch 69/200, Batch 11/17, Loss G: 4.799531936645508, Loss D: 0.01174141839146614\n",
      "Epoch 69/200, Batch 12/17, Loss G: 4.651947975158691, Loss D: 0.014909174293279648\n",
      "Epoch 69/200, Batch 13/17, Loss G: 4.364527702331543, Loss D: 0.014480434358119965\n",
      "Epoch 69/200, Batch 14/17, Loss G: 4.5355424880981445, Loss D: 0.01571880280971527\n",
      "Epoch 69/200, Batch 15/17, Loss G: 4.655501842498779, Loss D: 0.0276234932243824\n",
      "Epoch 69/200, Batch 16/17, Loss G: 4.448935508728027, Loss D: 0.008425666019320488\n",
      "Epoch 70/200, Batch 0/17, Loss G: 4.3871235847473145, Loss D: 0.020699674263596535\n",
      "Epoch 70/200, Batch 1/17, Loss G: 4.580850601196289, Loss D: 0.010394878685474396\n",
      "Epoch 70/200, Batch 2/17, Loss G: 4.545293807983398, Loss D: 0.015227203257381916\n",
      "Epoch 70/200, Batch 3/17, Loss G: 4.401744842529297, Loss D: 0.008918482810258865\n",
      "Epoch 70/200, Batch 4/17, Loss G: 4.261189937591553, Loss D: 0.019347362220287323\n",
      "Epoch 70/200, Batch 5/17, Loss G: 4.693500518798828, Loss D: 0.08389812707901001\n",
      "Epoch 70/200, Batch 6/17, Loss G: 3.633085250854492, Loss D: 0.31119194626808167\n",
      "Epoch 70/200, Batch 7/17, Loss G: 4.487573623657227, Loss D: 0.03307540714740753\n",
      "Epoch 70/200, Batch 8/17, Loss G: 4.495142936706543, Loss D: 0.10412269085645676\n",
      "Epoch 70/200, Batch 9/17, Loss G: 3.851576805114746, Loss D: 0.3722257912158966\n",
      "Epoch 70/200, Batch 10/17, Loss G: 3.8374226093292236, Loss D: 0.37888261675834656\n",
      "Epoch 70/200, Batch 11/17, Loss G: 3.993058443069458, Loss D: 0.22991199791431427\n",
      "Epoch 70/200, Batch 12/17, Loss G: 4.494919776916504, Loss D: 0.25338616967201233\n",
      "Epoch 70/200, Batch 13/17, Loss G: 4.368539810180664, Loss D: 0.07510162889957428\n",
      "Epoch 70/200, Batch 14/17, Loss G: 4.249205589294434, Loss D: 0.05483464524149895\n",
      "Epoch 70/200, Batch 15/17, Loss G: 4.038007736206055, Loss D: 0.23035188019275665\n",
      "Epoch 70/200, Batch 16/17, Loss G: 4.742444038391113, Loss D: 0.13053101301193237\n",
      "Epoch 71/200, Batch 0/17, Loss G: 3.7682433128356934, Loss D: 0.2543778717517853\n",
      "Epoch 71/200, Batch 1/17, Loss G: 4.250876426696777, Loss D: 0.11343726515769958\n",
      "Epoch 71/200, Batch 2/17, Loss G: 4.427783966064453, Loss D: 0.22928504645824432\n",
      "Epoch 71/200, Batch 3/17, Loss G: 3.771294593811035, Loss D: 0.2534853518009186\n",
      "Epoch 71/200, Batch 4/17, Loss G: 4.2857465744018555, Loss D: 0.1102268248796463\n",
      "Epoch 71/200, Batch 5/17, Loss G: 4.384331226348877, Loss D: 0.05312643200159073\n",
      "Epoch 71/200, Batch 6/17, Loss G: 4.309732913970947, Loss D: 0.07548880577087402\n",
      "Epoch 71/200, Batch 7/17, Loss G: 4.680907249450684, Loss D: 0.1649547815322876\n",
      "Epoch 71/200, Batch 8/17, Loss G: 3.6890861988067627, Loss D: 0.29324278235435486\n",
      "Epoch 71/200, Batch 9/17, Loss G: 4.459807395935059, Loss D: 0.04397475719451904\n",
      "Epoch 71/200, Batch 10/17, Loss G: 4.563257217407227, Loss D: 0.08518221974372864\n",
      "Epoch 71/200, Batch 11/17, Loss G: 3.9133386611938477, Loss D: 0.07148101925849915\n",
      "Epoch 71/200, Batch 12/17, Loss G: 4.579252243041992, Loss D: 0.0346597284078598\n",
      "Epoch 71/200, Batch 13/17, Loss G: 4.190274715423584, Loss D: 0.05323208123445511\n",
      "Epoch 71/200, Batch 14/17, Loss G: 4.604471206665039, Loss D: 0.09948676824569702\n",
      "Epoch 71/200, Batch 15/17, Loss G: 4.133129596710205, Loss D: 0.10866094380617142\n",
      "Epoch 71/200, Batch 16/17, Loss G: 4.604870796203613, Loss D: 0.05325991287827492\n",
      "Epoch 72/200, Batch 0/17, Loss G: 4.391347885131836, Loss D: 0.029652025550603867\n",
      "Epoch 72/200, Batch 1/17, Loss G: 4.246194362640381, Loss D: 0.03762919828295708\n",
      "Epoch 72/200, Batch 2/17, Loss G: 4.528690814971924, Loss D: 0.06942759454250336\n",
      "Epoch 72/200, Batch 3/17, Loss G: 4.278003692626953, Loss D: 0.034510016441345215\n",
      "Epoch 72/200, Batch 4/17, Loss G: 4.253400802612305, Loss D: 0.024303674697875977\n",
      "Epoch 72/200, Batch 5/17, Loss G: 4.521148204803467, Loss D: 0.021569840610027313\n",
      "Epoch 72/200, Batch 6/17, Loss G: 4.582250595092773, Loss D: 0.018735148012638092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200, Batch 7/17, Loss G: 4.517178058624268, Loss D: 0.033157091587781906\n",
      "Epoch 72/200, Batch 8/17, Loss G: 4.582704067230225, Loss D: 0.01450501661747694\n",
      "Epoch 72/200, Batch 9/17, Loss G: 4.362730026245117, Loss D: 0.02669171802699566\n",
      "Epoch 72/200, Batch 10/17, Loss G: 4.5924882888793945, Loss D: 0.011185038834810257\n",
      "Epoch 72/200, Batch 11/17, Loss G: 4.570583343505859, Loss D: 0.01758730597794056\n",
      "Epoch 72/200, Batch 12/17, Loss G: 4.591968536376953, Loss D: 0.016478199511766434\n",
      "Epoch 72/200, Batch 13/17, Loss G: 4.456267833709717, Loss D: 0.017189571633934975\n",
      "Epoch 72/200, Batch 14/17, Loss G: 4.393080234527588, Loss D: 0.011170394718647003\n",
      "Epoch 72/200, Batch 15/17, Loss G: 4.543628215789795, Loss D: 0.011674918234348297\n",
      "Epoch 72/200, Batch 16/17, Loss G: 4.679813385009766, Loss D: 0.027953550219535828\n",
      "Epoch 73/200, Batch 0/17, Loss G: 4.161099910736084, Loss D: 0.015571407042443752\n",
      "Epoch 73/200, Batch 1/17, Loss G: 4.51446533203125, Loss D: 0.010550905019044876\n",
      "Epoch 73/200, Batch 2/17, Loss G: 4.450976371765137, Loss D: 0.010193519294261932\n",
      "Epoch 73/200, Batch 3/17, Loss G: 4.341710090637207, Loss D: 0.018597211688756943\n",
      "Epoch 73/200, Batch 4/17, Loss G: 4.452791690826416, Loss D: 0.019427362829446793\n",
      "Epoch 73/200, Batch 5/17, Loss G: 4.267998218536377, Loss D: 0.04717116430401802\n",
      "Epoch 73/200, Batch 6/17, Loss G: 4.888677597045898, Loss D: 0.04973330721259117\n",
      "Epoch 73/200, Batch 7/17, Loss G: 4.281609535217285, Loss D: 0.026068933308124542\n",
      "Epoch 73/200, Batch 8/17, Loss G: 4.434053421020508, Loss D: 0.016193527728319168\n",
      "Epoch 73/200, Batch 9/17, Loss G: 4.582850933074951, Loss D: 0.014887071214616299\n",
      "Epoch 73/200, Batch 10/17, Loss G: 4.325891017913818, Loss D: 0.012700823135674\n",
      "Epoch 73/200, Batch 11/17, Loss G: 4.567474842071533, Loss D: 0.010394740849733353\n",
      "Epoch 73/200, Batch 12/17, Loss G: 4.394662857055664, Loss D: 0.015514791943132877\n",
      "Epoch 73/200, Batch 13/17, Loss G: 4.541558265686035, Loss D: 0.008012840524315834\n",
      "Epoch 73/200, Batch 14/17, Loss G: 4.48859977722168, Loss D: 0.00627589225769043\n",
      "Epoch 73/200, Batch 15/17, Loss G: 4.530586242675781, Loss D: 0.015132434666156769\n",
      "Epoch 73/200, Batch 16/17, Loss G: 4.578794002532959, Loss D: 0.01107574813067913\n",
      "Epoch 74/200, Batch 0/17, Loss G: 4.405625343322754, Loss D: 0.00931586790829897\n",
      "Epoch 74/200, Batch 1/17, Loss G: 4.533841133117676, Loss D: 0.01402867678552866\n",
      "Epoch 74/200, Batch 2/17, Loss G: 4.349584102630615, Loss D: 0.007813536562025547\n",
      "Epoch 74/200, Batch 3/17, Loss G: 4.324439525604248, Loss D: 0.010646658018231392\n",
      "Epoch 74/200, Batch 4/17, Loss G: 4.366276741027832, Loss D: 0.006572698708623648\n",
      "Epoch 74/200, Batch 5/17, Loss G: 4.452848434448242, Loss D: 0.009843667969107628\n",
      "Epoch 74/200, Batch 6/17, Loss G: 4.384795188903809, Loss D: 0.011879946105182171\n",
      "Epoch 74/200, Batch 7/17, Loss G: 4.446744918823242, Loss D: 0.010408024303615093\n",
      "Epoch 74/200, Batch 8/17, Loss G: 4.5649733543396, Loss D: 0.012590767815709114\n",
      "Epoch 74/200, Batch 9/17, Loss G: 4.5324788093566895, Loss D: 0.04021492600440979\n",
      "Epoch 74/200, Batch 10/17, Loss G: 4.093472480773926, Loss D: 0.06305307894945145\n",
      "Epoch 74/200, Batch 11/17, Loss G: 4.707846641540527, Loss D: 0.014498403295874596\n",
      "Epoch 74/200, Batch 12/17, Loss G: 4.627209186553955, Loss D: 0.038714535534381866\n",
      "Epoch 74/200, Batch 13/17, Loss G: 4.118884086608887, Loss D: 0.06032775342464447\n",
      "Epoch 74/200, Batch 14/17, Loss G: 4.647787570953369, Loss D: 0.06045224890112877\n",
      "Epoch 74/200, Batch 15/17, Loss G: 3.8994300365448, Loss D: 0.18085677921772003\n",
      "Epoch 74/200, Batch 16/17, Loss G: 4.620541572570801, Loss D: 0.3369945287704468\n",
      "Epoch 75/200, Batch 0/17, Loss G: 4.244295597076416, Loss D: 0.14810433983802795\n",
      "Epoch 75/200, Batch 1/17, Loss G: 4.272582054138184, Loss D: 0.14077234268188477\n",
      "Epoch 75/200, Batch 2/17, Loss G: 4.644113540649414, Loss D: 0.09750324487686157\n",
      "Epoch 75/200, Batch 3/17, Loss G: 4.077517032623291, Loss D: 0.06615432351827621\n",
      "Epoch 75/200, Batch 4/17, Loss G: 4.519975662231445, Loss D: 0.07352064549922943\n",
      "Epoch 75/200, Batch 5/17, Loss G: 4.168143272399902, Loss D: 0.045154426246881485\n",
      "Epoch 75/200, Batch 6/17, Loss G: 4.576385498046875, Loss D: 0.024369195103645325\n",
      "Epoch 75/200, Batch 7/17, Loss G: 4.221323490142822, Loss D: 0.03295324370265007\n",
      "Epoch 75/200, Batch 8/17, Loss G: 4.4698052406311035, Loss D: 0.015709441155195236\n",
      "Epoch 75/200, Batch 9/17, Loss G: 4.259561538696289, Loss D: 0.045854464173316956\n",
      "Epoch 75/200, Batch 10/17, Loss G: 4.605495452880859, Loss D: 0.06333385407924652\n",
      "Epoch 75/200, Batch 11/17, Loss G: 4.5164947509765625, Loss D: 0.030880441889166832\n",
      "Epoch 75/200, Batch 12/17, Loss G: 4.419987678527832, Loss D: 0.018317922949790955\n",
      "Epoch 75/200, Batch 13/17, Loss G: 4.537110328674316, Loss D: 0.022557659074664116\n",
      "Epoch 75/200, Batch 14/17, Loss G: 4.309481143951416, Loss D: 0.015562603250145912\n",
      "Epoch 75/200, Batch 15/17, Loss G: 4.280035972595215, Loss D: 0.01466699130833149\n",
      "Epoch 75/200, Batch 16/17, Loss G: 4.333395481109619, Loss D: 0.008081917650997639\n",
      "Epoch 76/200, Batch 0/17, Loss G: 4.564202785491943, Loss D: 0.009311887435615063\n",
      "Epoch 76/200, Batch 1/17, Loss G: 4.237545967102051, Loss D: 0.014490311965346336\n",
      "Epoch 76/200, Batch 2/17, Loss G: 4.55834436416626, Loss D: 0.02921699546277523\n",
      "Epoch 76/200, Batch 3/17, Loss G: 4.215673446655273, Loss D: 0.029048925265669823\n",
      "Epoch 76/200, Batch 4/17, Loss G: 4.520743370056152, Loss D: 0.004539542365819216\n",
      "Epoch 76/200, Batch 5/17, Loss G: 4.572305679321289, Loss D: 0.00691148079931736\n",
      "Epoch 76/200, Batch 6/17, Loss G: 4.51276969909668, Loss D: 0.0013134939363226295\n",
      "Epoch 76/200, Batch 7/17, Loss G: 4.66168212890625, Loss D: 0.0005836914060637355\n",
      "Epoch 76/200, Batch 8/17, Loss G: 4.409607410430908, Loss D: 0.00035700263106264174\n",
      "Epoch 76/200, Batch 9/17, Loss G: 4.528862953186035, Loss D: 0.0003754668578039855\n",
      "Epoch 76/200, Batch 10/17, Loss G: 4.464338302612305, Loss D: 0.0007665975717827678\n",
      "Epoch 76/200, Batch 11/17, Loss G: 4.4203081130981445, Loss D: 0.005623888224363327\n",
      "Epoch 76/200, Batch 12/17, Loss G: 4.306124210357666, Loss D: 0.010093379765748978\n",
      "Epoch 76/200, Batch 13/17, Loss G: 4.360374450683594, Loss D: 0.007978730835020542\n",
      "Epoch 76/200, Batch 14/17, Loss G: 4.438388824462891, Loss D: 0.010415958240628242\n",
      "Epoch 76/200, Batch 15/17, Loss G: 4.30982780456543, Loss D: 0.00683427881449461\n",
      "Epoch 76/200, Batch 16/17, Loss G: 4.2052483558654785, Loss D: 0.006627315655350685\n",
      "Epoch 77/200, Batch 0/17, Loss G: 4.417845726013184, Loss D: 0.010393617674708366\n",
      "Epoch 77/200, Batch 1/17, Loss G: 4.389084815979004, Loss D: 0.008066928014159203\n",
      "Epoch 77/200, Batch 2/17, Loss G: 4.487578392028809, Loss D: 0.007398463785648346\n",
      "Epoch 77/200, Batch 3/17, Loss G: 4.537502765655518, Loss D: 0.009700547903776169\n",
      "Epoch 77/200, Batch 4/17, Loss G: 4.3977556228637695, Loss D: 0.005023983307182789\n",
      "Epoch 77/200, Batch 5/17, Loss G: 4.457758903503418, Loss D: 0.010733528062701225\n",
      "Epoch 77/200, Batch 6/17, Loss G: 4.3525710105896, Loss D: 0.01296953298151493\n",
      "Epoch 77/200, Batch 7/17, Loss G: 4.4895219802856445, Loss D: 0.008770108222961426\n",
      "Epoch 77/200, Batch 8/17, Loss G: 4.383191108703613, Loss D: 0.00722159119322896\n",
      "Epoch 77/200, Batch 9/17, Loss G: 4.249515056610107, Loss D: 0.020348507910966873\n",
      "Epoch 77/200, Batch 10/17, Loss G: 4.588573932647705, Loss D: 0.029782462865114212\n",
      "Epoch 77/200, Batch 11/17, Loss G: 4.047649383544922, Loss D: 0.0520048551261425\n",
      "Epoch 77/200, Batch 12/17, Loss G: 4.581089973449707, Loss D: 0.08477795869112015\n",
      "Epoch 77/200, Batch 13/17, Loss G: 3.818147897720337, Loss D: 0.14117485284805298\n",
      "Epoch 77/200, Batch 14/17, Loss G: 4.388777732849121, Loss D: 0.3432500958442688\n",
      "Epoch 77/200, Batch 15/17, Loss G: 3.8831777572631836, Loss D: 0.14460742473602295\n",
      "Epoch 77/200, Batch 16/17, Loss G: 4.302811145782471, Loss D: 0.09017392992973328\n",
      "Epoch 78/200, Batch 0/17, Loss G: 3.705953598022461, Loss D: 0.23451383411884308\n",
      "Epoch 78/200, Batch 1/17, Loss G: 4.570219993591309, Loss D: 0.206365168094635\n",
      "Epoch 78/200, Batch 2/17, Loss G: 4.025394439697266, Loss D: 0.07926858216524124\n",
      "Epoch 78/200, Batch 3/17, Loss G: 4.442435264587402, Loss D: 0.044675491750240326\n",
      "Epoch 78/200, Batch 4/17, Loss G: 4.308782577514648, Loss D: 0.06907841563224792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200, Batch 5/17, Loss G: 4.225655555725098, Loss D: 0.056278128176927567\n",
      "Epoch 78/200, Batch 6/17, Loss G: 4.313928127288818, Loss D: 0.03902721405029297\n",
      "Epoch 78/200, Batch 7/17, Loss G: 4.328165531158447, Loss D: 0.0306811835616827\n",
      "Epoch 78/200, Batch 8/17, Loss G: 4.615058422088623, Loss D: 0.034675098955631256\n",
      "Epoch 78/200, Batch 9/17, Loss G: 4.310275077819824, Loss D: 0.033617053180933\n",
      "Epoch 78/200, Batch 10/17, Loss G: 4.415863037109375, Loss D: 0.027153462171554565\n",
      "Epoch 78/200, Batch 11/17, Loss G: 4.346216678619385, Loss D: 0.023267880082130432\n",
      "Epoch 78/200, Batch 12/17, Loss G: 4.425691604614258, Loss D: 0.025280367583036423\n",
      "Epoch 78/200, Batch 13/17, Loss G: 4.297197341918945, Loss D: 0.018095944076776505\n",
      "Epoch 78/200, Batch 14/17, Loss G: 4.387395858764648, Loss D: 0.016809504479169846\n",
      "Epoch 78/200, Batch 15/17, Loss G: 4.556657791137695, Loss D: 0.026019012555480003\n",
      "Epoch 78/200, Batch 16/17, Loss G: 4.488399505615234, Loss D: 0.0339408814907074\n",
      "Epoch 79/200, Batch 0/17, Loss G: 4.586526393890381, Loss D: 0.017236629500985146\n",
      "Epoch 79/200, Batch 1/17, Loss G: 4.77403450012207, Loss D: 0.005997706204652786\n",
      "Epoch 79/200, Batch 2/17, Loss G: 4.731330871582031, Loss D: 0.008638732135295868\n",
      "Epoch 79/200, Batch 3/17, Loss G: 4.4641594886779785, Loss D: 0.017478255555033684\n",
      "Epoch 79/200, Batch 4/17, Loss G: 4.554230690002441, Loss D: 0.021451322361826897\n",
      "Epoch 79/200, Batch 5/17, Loss G: 4.518213272094727, Loss D: 0.01244991272687912\n",
      "Epoch 79/200, Batch 6/17, Loss G: 4.284575462341309, Loss D: 0.013175765983760357\n",
      "Epoch 79/200, Batch 7/17, Loss G: 4.555098056793213, Loss D: 0.016011347994208336\n",
      "Epoch 79/200, Batch 8/17, Loss G: 4.339193344116211, Loss D: 0.02727191336452961\n",
      "Epoch 79/200, Batch 9/17, Loss G: 4.611466407775879, Loss D: 0.03009754791855812\n",
      "Epoch 79/200, Batch 10/17, Loss G: 4.217103958129883, Loss D: 0.020913274958729744\n",
      "Epoch 79/200, Batch 11/17, Loss G: 4.466588020324707, Loss D: 0.01575305685400963\n",
      "Epoch 79/200, Batch 12/17, Loss G: 4.3847503662109375, Loss D: 0.014081163331866264\n",
      "Epoch 79/200, Batch 13/17, Loss G: 4.535194396972656, Loss D: 0.006674786098301411\n",
      "Epoch 79/200, Batch 14/17, Loss G: 4.185750961303711, Loss D: 0.018566392362117767\n",
      "Epoch 79/200, Batch 15/17, Loss G: 4.451897621154785, Loss D: 0.010122162289917469\n",
      "Epoch 79/200, Batch 16/17, Loss G: 4.76085090637207, Loss D: 0.009069460444152355\n",
      "Epoch 80/200, Batch 0/17, Loss G: 4.530471324920654, Loss D: 0.009419631212949753\n",
      "Epoch 80/200, Batch 1/17, Loss G: 4.307468891143799, Loss D: 0.009856294840574265\n",
      "Epoch 80/200, Batch 2/17, Loss G: 4.560515880584717, Loss D: 0.009245309047400951\n",
      "Epoch 80/200, Batch 3/17, Loss G: 4.481183052062988, Loss D: 0.016818631440401077\n",
      "Epoch 80/200, Batch 4/17, Loss G: 4.309526443481445, Loss D: 0.01291453093290329\n",
      "Epoch 80/200, Batch 5/17, Loss G: 4.506312370300293, Loss D: 0.015568912029266357\n",
      "Epoch 80/200, Batch 6/17, Loss G: 4.291376113891602, Loss D: 0.018904272466897964\n",
      "Epoch 80/200, Batch 7/17, Loss G: 4.455856800079346, Loss D: 0.013682561926543713\n",
      "Epoch 80/200, Batch 8/17, Loss G: 4.227031707763672, Loss D: 0.008308990858495235\n",
      "Epoch 80/200, Batch 9/17, Loss G: 4.365451812744141, Loss D: 0.009452810510993004\n",
      "Epoch 80/200, Batch 10/17, Loss G: 4.294425010681152, Loss D: 0.005071669351309538\n",
      "Epoch 80/200, Batch 11/17, Loss G: 4.450304985046387, Loss D: 0.007462901063263416\n",
      "Epoch 80/200, Batch 12/17, Loss G: 4.489016056060791, Loss D: 0.005807030946016312\n",
      "Epoch 80/200, Batch 13/17, Loss G: 4.344992160797119, Loss D: 0.009782910346984863\n",
      "Epoch 80/200, Batch 14/17, Loss G: 4.384006500244141, Loss D: 0.010170443914830685\n",
      "Epoch 80/200, Batch 15/17, Loss G: 4.335482597351074, Loss D: 0.01449623890221119\n",
      "Epoch 80/200, Batch 16/17, Loss G: 4.296600818634033, Loss D: 0.01848677173256874\n",
      "Epoch 81/200, Batch 0/17, Loss G: 4.352203845977783, Loss D: 0.01120771560817957\n",
      "Epoch 81/200, Batch 1/17, Loss G: 4.449563503265381, Loss D: 0.012783333659172058\n",
      "Epoch 81/200, Batch 2/17, Loss G: 4.647674560546875, Loss D: 0.010881281457841396\n",
      "Epoch 81/200, Batch 3/17, Loss G: 4.321804523468018, Loss D: 0.019864758476614952\n",
      "Epoch 81/200, Batch 4/17, Loss G: 4.586644172668457, Loss D: 0.0445128008723259\n",
      "Epoch 81/200, Batch 5/17, Loss G: 3.7635512351989746, Loss D: 0.11589647829532623\n",
      "Epoch 81/200, Batch 6/17, Loss G: 4.5815277099609375, Loss D: 0.12548555433750153\n",
      "Epoch 81/200, Batch 7/17, Loss G: 3.917635917663574, Loss D: 0.15281040966510773\n",
      "Epoch 81/200, Batch 8/17, Loss G: 4.537480354309082, Loss D: 0.1138572245836258\n",
      "Epoch 81/200, Batch 9/17, Loss G: 3.7634692192077637, Loss D: 0.22615820169448853\n",
      "Epoch 81/200, Batch 10/17, Loss G: 4.341319561004639, Loss D: 0.17295987904071808\n",
      "Epoch 81/200, Batch 11/17, Loss G: 4.159935474395752, Loss D: 0.034078147262334824\n",
      "Epoch 81/200, Batch 12/17, Loss G: 4.125117301940918, Loss D: 0.11609894782304764\n",
      "Epoch 81/200, Batch 13/17, Loss G: 4.680319786071777, Loss D: 0.15009889006614685\n",
      "Epoch 81/200, Batch 14/17, Loss G: 3.850973129272461, Loss D: 0.08781154453754425\n",
      "Epoch 81/200, Batch 15/17, Loss G: 4.342281341552734, Loss D: 0.03652948513627052\n",
      "Epoch 81/200, Batch 16/17, Loss G: 4.6242289543151855, Loss D: 0.03588671237230301\n",
      "Epoch 82/200, Batch 0/17, Loss G: 4.041136741638184, Loss D: 0.09421885758638382\n",
      "Epoch 82/200, Batch 1/17, Loss G: 4.617153167724609, Loss D: 0.10794229805469513\n",
      "Epoch 82/200, Batch 2/17, Loss G: 4.13139009475708, Loss D: 0.09524717181921005\n",
      "Epoch 82/200, Batch 3/17, Loss G: 4.279155731201172, Loss D: 0.03923457860946655\n",
      "Epoch 82/200, Batch 4/17, Loss G: 4.255949020385742, Loss D: 0.029796643182635307\n",
      "Epoch 82/200, Batch 5/17, Loss G: 4.250801086425781, Loss D: 0.012450132519006729\n",
      "Epoch 82/200, Batch 6/17, Loss G: 4.625130653381348, Loss D: 0.01800847239792347\n",
      "Epoch 82/200, Batch 7/17, Loss G: 4.222238540649414, Loss D: 0.025437751784920692\n",
      "Epoch 82/200, Batch 8/17, Loss G: 4.279304504394531, Loss D: 0.02512776479125023\n",
      "Epoch 82/200, Batch 9/17, Loss G: 4.3015666007995605, Loss D: 0.025266725569963455\n",
      "Epoch 82/200, Batch 10/17, Loss G: 4.490699768066406, Loss D: 0.01823853887617588\n",
      "Epoch 82/200, Batch 11/17, Loss G: 4.221138000488281, Loss D: 0.025699175894260406\n",
      "Epoch 82/200, Batch 12/17, Loss G: 4.580996513366699, Loss D: 0.02020918019115925\n",
      "Epoch 82/200, Batch 13/17, Loss G: 4.112889289855957, Loss D: 0.03299044445157051\n",
      "Epoch 82/200, Batch 14/17, Loss G: 4.450065612792969, Loss D: 0.04334467276930809\n",
      "Epoch 82/200, Batch 15/17, Loss G: 4.220368385314941, Loss D: 0.03216174617409706\n",
      "Epoch 82/200, Batch 16/17, Loss G: 4.527828216552734, Loss D: 0.02359732985496521\n",
      "Epoch 83/200, Batch 0/17, Loss G: 4.2872724533081055, Loss D: 0.049172718077898026\n",
      "Epoch 83/200, Batch 1/17, Loss G: 4.776174545288086, Loss D: 0.011605508625507355\n",
      "Epoch 83/200, Batch 2/17, Loss G: 4.611985206604004, Loss D: 0.021021123975515366\n",
      "Epoch 83/200, Batch 3/17, Loss G: 4.557465553283691, Loss D: 0.009205125272274017\n",
      "Epoch 83/200, Batch 4/17, Loss G: 4.10911226272583, Loss D: 0.043134283274412155\n",
      "Epoch 83/200, Batch 5/17, Loss G: 4.338881969451904, Loss D: 0.02380126528441906\n",
      "Epoch 83/200, Batch 6/17, Loss G: 4.469597816467285, Loss D: 0.019481780007481575\n",
      "Epoch 83/200, Batch 7/17, Loss G: 4.187021255493164, Loss D: 0.0175465140491724\n",
      "Epoch 83/200, Batch 8/17, Loss G: 4.344136714935303, Loss D: 0.010451104491949081\n",
      "Epoch 83/200, Batch 9/17, Loss G: 4.317949295043945, Loss D: 0.013980185613036156\n",
      "Epoch 83/200, Batch 10/17, Loss G: 4.202365875244141, Loss D: 0.011833508498966694\n",
      "Epoch 83/200, Batch 11/17, Loss G: 4.356013774871826, Loss D: 0.008531560190021992\n",
      "Epoch 83/200, Batch 12/17, Loss G: 4.466351509094238, Loss D: 0.0077321589924395084\n",
      "Epoch 83/200, Batch 13/17, Loss G: 4.208658218383789, Loss D: 0.01166173629462719\n",
      "Epoch 83/200, Batch 14/17, Loss G: 4.238644599914551, Loss D: 0.008682274259626865\n",
      "Epoch 83/200, Batch 15/17, Loss G: 4.23119592666626, Loss D: 0.013384032063186169\n",
      "Epoch 83/200, Batch 16/17, Loss G: 4.270480155944824, Loss D: 0.009923061355948448\n",
      "Epoch 84/200, Batch 0/17, Loss G: 4.202084064483643, Loss D: 0.01092933863401413\n",
      "Epoch 84/200, Batch 1/17, Loss G: 4.284055709838867, Loss D: 0.004684329964220524\n",
      "Epoch 84/200, Batch 2/17, Loss G: 4.367595672607422, Loss D: 0.015028479509055614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200, Batch 3/17, Loss G: 4.499429702758789, Loss D: 0.010621532797813416\n",
      "Epoch 84/200, Batch 4/17, Loss G: 4.410321235656738, Loss D: 0.0390855073928833\n",
      "Epoch 84/200, Batch 5/17, Loss G: 3.8703694343566895, Loss D: 0.11879512667655945\n",
      "Epoch 84/200, Batch 6/17, Loss G: 4.624046325683594, Loss D: 0.15747280418872833\n",
      "Epoch 84/200, Batch 7/17, Loss G: 3.999399423599243, Loss D: 0.0692872405052185\n",
      "Epoch 84/200, Batch 8/17, Loss G: 4.670063495635986, Loss D: 0.004394850227981806\n",
      "Epoch 84/200, Batch 9/17, Loss G: 4.622603416442871, Loss D: 0.02036222815513611\n",
      "Epoch 84/200, Batch 10/17, Loss G: 4.319243431091309, Loss D: 0.0007877642055973411\n",
      "Epoch 84/200, Batch 11/17, Loss G: 4.337647914886475, Loss D: 0.002476879395544529\n",
      "Epoch 84/200, Batch 12/17, Loss G: 4.215325355529785, Loss D: 0.0065552289597690105\n",
      "Epoch 84/200, Batch 13/17, Loss G: 4.012679100036621, Loss D: 0.025156617164611816\n",
      "Epoch 84/200, Batch 14/17, Loss G: 4.474902629852295, Loss D: 0.009401826187968254\n",
      "Epoch 84/200, Batch 15/17, Loss G: 4.537076950073242, Loss D: 0.017529144883155823\n",
      "Epoch 84/200, Batch 16/17, Loss G: 4.283356666564941, Loss D: 0.005555654875934124\n",
      "Epoch 85/200, Batch 0/17, Loss G: 4.307679176330566, Loss D: 0.020281217992305756\n",
      "Epoch 85/200, Batch 1/17, Loss G: 4.332851886749268, Loss D: 0.008157452568411827\n",
      "Epoch 85/200, Batch 2/17, Loss G: 4.598193168640137, Loss D: 0.012101676315069199\n",
      "Epoch 85/200, Batch 3/17, Loss G: 4.55097770690918, Loss D: 0.007758834399282932\n",
      "Epoch 85/200, Batch 4/17, Loss G: 4.311404228210449, Loss D: 0.008406635373830795\n",
      "Epoch 85/200, Batch 5/17, Loss G: 4.369446754455566, Loss D: 0.008075974881649017\n",
      "Epoch 85/200, Batch 6/17, Loss G: 4.305097579956055, Loss D: 0.007615281734615564\n",
      "Epoch 85/200, Batch 7/17, Loss G: 4.258936882019043, Loss D: 0.005012319888919592\n",
      "Epoch 85/200, Batch 8/17, Loss G: 4.241325855255127, Loss D: 0.012096768245100975\n",
      "Epoch 85/200, Batch 9/17, Loss G: 4.246592998504639, Loss D: 0.015182842500507832\n",
      "Epoch 85/200, Batch 10/17, Loss G: 4.378774642944336, Loss D: 0.01317956205457449\n",
      "Epoch 85/200, Batch 11/17, Loss G: 4.501765251159668, Loss D: 0.014169244095683098\n",
      "Epoch 85/200, Batch 12/17, Loss G: 4.0644450187683105, Loss D: 0.01700405403971672\n",
      "Epoch 85/200, Batch 13/17, Loss G: 4.411097526550293, Loss D: 0.007016969379037619\n",
      "Epoch 85/200, Batch 14/17, Loss G: 4.347325325012207, Loss D: 0.04065069556236267\n",
      "Epoch 85/200, Batch 15/17, Loss G: 3.6642684936523438, Loss D: 0.09269219636917114\n",
      "Epoch 85/200, Batch 16/17, Loss G: 4.641124725341797, Loss D: 0.06441135704517365\n",
      "Epoch 86/200, Batch 0/17, Loss G: 4.203296661376953, Loss D: 0.017537472769618034\n",
      "Epoch 86/200, Batch 1/17, Loss G: 4.141238212585449, Loss D: 0.03552880883216858\n",
      "Epoch 86/200, Batch 2/17, Loss G: 4.5352888107299805, Loss D: 0.07357559353113174\n",
      "Epoch 86/200, Batch 3/17, Loss G: 4.248437881469727, Loss D: 0.04764740541577339\n",
      "Epoch 86/200, Batch 4/17, Loss G: 4.269179821014404, Loss D: 0.02291175350546837\n",
      "Epoch 86/200, Batch 5/17, Loss G: 4.619024276733398, Loss D: 0.046605274081230164\n",
      "Epoch 86/200, Batch 6/17, Loss G: 4.307578086853027, Loss D: 0.016224926337599754\n",
      "Epoch 86/200, Batch 7/17, Loss G: 3.9985649585723877, Loss D: 0.06039770320057869\n",
      "Epoch 86/200, Batch 8/17, Loss G: 4.586221218109131, Loss D: 0.0533045157790184\n",
      "Epoch 86/200, Batch 9/17, Loss G: 4.145745277404785, Loss D: 0.02753063291311264\n",
      "Epoch 86/200, Batch 10/17, Loss G: 4.155116081237793, Loss D: 0.014141764491796494\n",
      "Epoch 86/200, Batch 11/17, Loss G: 4.305792808532715, Loss D: 0.014106232672929764\n",
      "Epoch 86/200, Batch 12/17, Loss G: 4.275899410247803, Loss D: 0.010604308918118477\n",
      "Epoch 86/200, Batch 13/17, Loss G: 4.411796569824219, Loss D: 0.009604023769497871\n",
      "Epoch 86/200, Batch 14/17, Loss G: 4.329748153686523, Loss D: 0.008464677259325981\n",
      "Epoch 86/200, Batch 15/17, Loss G: 4.537960052490234, Loss D: 0.014680319465696812\n",
      "Epoch 86/200, Batch 16/17, Loss G: 4.267695903778076, Loss D: 0.018173370510339737\n",
      "Epoch 87/200, Batch 0/17, Loss G: 4.108016490936279, Loss D: 0.022066961973905563\n",
      "Epoch 87/200, Batch 1/17, Loss G: 4.50861930847168, Loss D: 0.041493531316518784\n",
      "Epoch 87/200, Batch 2/17, Loss G: 4.383434295654297, Loss D: 0.03129975125193596\n",
      "Epoch 87/200, Batch 3/17, Loss G: 4.335902690887451, Loss D: 0.005255211144685745\n",
      "Epoch 87/200, Batch 4/17, Loss G: 4.477055549621582, Loss D: 0.00722511624917388\n",
      "Epoch 87/200, Batch 5/17, Loss G: 4.234748840332031, Loss D: 0.012134783901274204\n",
      "Epoch 87/200, Batch 6/17, Loss G: 4.596246719360352, Loss D: 0.002463349374011159\n",
      "Epoch 87/200, Batch 7/17, Loss G: 4.408479690551758, Loss D: 0.005008867010474205\n",
      "Epoch 87/200, Batch 8/17, Loss G: 4.337854385375977, Loss D: 0.009158110246062279\n",
      "Epoch 87/200, Batch 9/17, Loss G: 4.281063079833984, Loss D: 0.005762887187302113\n",
      "Epoch 87/200, Batch 10/17, Loss G: 4.25846529006958, Loss D: 0.00773528590798378\n",
      "Epoch 87/200, Batch 11/17, Loss G: 4.166735649108887, Loss D: 0.02127448469400406\n",
      "Epoch 87/200, Batch 12/17, Loss G: 4.2620744705200195, Loss D: 0.013195928186178207\n",
      "Epoch 87/200, Batch 13/17, Loss G: 4.359151363372803, Loss D: 0.0038438981864601374\n",
      "Epoch 87/200, Batch 14/17, Loss G: 4.135472297668457, Loss D: 0.004651416093111038\n",
      "Epoch 87/200, Batch 15/17, Loss G: 4.228670120239258, Loss D: 0.01058527734130621\n",
      "Epoch 87/200, Batch 16/17, Loss G: 4.434837341308594, Loss D: 0.00888229813426733\n",
      "Epoch 88/200, Batch 0/17, Loss G: 4.125629901885986, Loss D: 0.018129050731658936\n",
      "Epoch 88/200, Batch 1/17, Loss G: 4.553175926208496, Loss D: 0.004631150513887405\n",
      "Epoch 88/200, Batch 2/17, Loss G: 4.637155532836914, Loss D: 0.010378037579357624\n",
      "Epoch 88/200, Batch 3/17, Loss G: 4.374642372131348, Loss D: 0.012197200208902359\n",
      "Epoch 88/200, Batch 4/17, Loss G: 4.407835006713867, Loss D: 0.0058478363789618015\n",
      "Epoch 88/200, Batch 5/17, Loss G: 4.28743839263916, Loss D: 0.0072905695997178555\n",
      "Epoch 88/200, Batch 6/17, Loss G: 4.097343921661377, Loss D: 0.007719563320279121\n",
      "Epoch 88/200, Batch 7/17, Loss G: 4.024503707885742, Loss D: 0.009928635321557522\n",
      "Epoch 88/200, Batch 8/17, Loss G: 4.346226692199707, Loss D: 0.012927593663334846\n",
      "Epoch 88/200, Batch 9/17, Loss G: 4.406833648681641, Loss D: 0.002907888498157263\n",
      "Epoch 88/200, Batch 10/17, Loss G: 4.149015426635742, Loss D: 0.008179855532944202\n",
      "Epoch 88/200, Batch 11/17, Loss G: 4.305710315704346, Loss D: 0.004749701824039221\n",
      "Epoch 88/200, Batch 12/17, Loss G: 4.536538124084473, Loss D: 0.0035429697018116713\n",
      "Epoch 88/200, Batch 13/17, Loss G: 4.391257286071777, Loss D: 0.004099029116332531\n",
      "Epoch 88/200, Batch 14/17, Loss G: 4.373366355895996, Loss D: 0.006372078321874142\n",
      "Epoch 88/200, Batch 15/17, Loss G: 4.1526055335998535, Loss D: 0.008473904803395271\n",
      "Epoch 88/200, Batch 16/17, Loss G: 4.633919715881348, Loss D: 0.01637287251651287\n",
      "Epoch 89/200, Batch 0/17, Loss G: 4.199452877044678, Loss D: 0.024737922474741936\n",
      "Epoch 89/200, Batch 1/17, Loss G: 4.4314751625061035, Loss D: 0.01005491241812706\n",
      "Epoch 89/200, Batch 2/17, Loss G: 4.304275035858154, Loss D: 0.006581577472388744\n",
      "Epoch 89/200, Batch 3/17, Loss G: 4.33786678314209, Loss D: 0.003158300882205367\n",
      "Epoch 89/200, Batch 4/17, Loss G: 4.246738910675049, Loss D: 0.009055085480213165\n",
      "Epoch 89/200, Batch 5/17, Loss G: 4.356204509735107, Loss D: 0.010685990564525127\n",
      "Epoch 89/200, Batch 6/17, Loss G: 4.29149866104126, Loss D: 0.009617630392313004\n",
      "Epoch 89/200, Batch 7/17, Loss G: 4.261923789978027, Loss D: 0.021610748022794724\n",
      "Epoch 89/200, Batch 8/17, Loss G: 4.121264457702637, Loss D: 0.06783399730920792\n",
      "Epoch 89/200, Batch 9/17, Loss G: 4.490772724151611, Loss D: 0.08385653793811798\n",
      "Epoch 89/200, Batch 10/17, Loss G: 4.0403242111206055, Loss D: 0.0231128241866827\n",
      "Epoch 89/200, Batch 11/17, Loss G: 4.430074214935303, Loss D: 0.0050738658756017685\n",
      "Epoch 89/200, Batch 12/17, Loss G: 4.422843933105469, Loss D: 0.004358974285423756\n",
      "Epoch 89/200, Batch 13/17, Loss G: 4.023202419281006, Loss D: 0.019505200907588005\n",
      "Epoch 89/200, Batch 14/17, Loss G: 4.300134658813477, Loss D: 0.029709795489907265\n",
      "Epoch 89/200, Batch 15/17, Loss G: 3.987537145614624, Loss D: 0.05101366713643074\n",
      "Epoch 89/200, Batch 16/17, Loss G: 4.474354267120361, Loss D: 0.02352277748286724\n",
      "Epoch 90/200, Batch 0/17, Loss G: 4.419497013092041, Loss D: 0.014189397916197777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Batch 1/17, Loss G: 4.098085403442383, Loss D: 0.025838300585746765\n",
      "Epoch 90/200, Batch 2/17, Loss G: 4.297093868255615, Loss D: 0.028332626447081566\n",
      "Epoch 90/200, Batch 3/17, Loss G: 4.155313014984131, Loss D: 0.0238653514534235\n",
      "Epoch 90/200, Batch 4/17, Loss G: 4.135200500488281, Loss D: 0.00755327008664608\n",
      "Epoch 90/200, Batch 5/17, Loss G: 4.285192489624023, Loss D: 0.012568192556500435\n",
      "Epoch 90/200, Batch 6/17, Loss G: 4.38227653503418, Loss D: 0.028853634372353554\n",
      "Epoch 90/200, Batch 7/17, Loss G: 4.022224426269531, Loss D: 0.023263288661837578\n",
      "Epoch 90/200, Batch 8/17, Loss G: 4.386837005615234, Loss D: 0.00574297271668911\n",
      "Epoch 90/200, Batch 9/17, Loss G: 4.379308223724365, Loss D: 0.0020691007375717163\n",
      "Epoch 90/200, Batch 10/17, Loss G: 4.295347213745117, Loss D: 0.014208044856786728\n",
      "Epoch 90/200, Batch 11/17, Loss G: 4.345318794250488, Loss D: 0.008410406298935413\n",
      "Epoch 90/200, Batch 12/17, Loss G: 4.091886520385742, Loss D: 0.025055576115846634\n",
      "Epoch 90/200, Batch 13/17, Loss G: 4.337377548217773, Loss D: 0.03652813658118248\n",
      "Epoch 90/200, Batch 14/17, Loss G: 4.0153350830078125, Loss D: 0.07664022594690323\n",
      "Epoch 90/200, Batch 15/17, Loss G: 4.5445780754089355, Loss D: 0.1598099023103714\n",
      "Epoch 90/200, Batch 16/17, Loss G: 3.6262664794921875, Loss D: 0.31931573152542114\n",
      "Epoch 91/200, Batch 0/17, Loss G: 4.406205177307129, Loss D: 0.04671816900372505\n",
      "Epoch 91/200, Batch 1/17, Loss G: 4.092074871063232, Loss D: 0.022638797760009766\n",
      "Epoch 91/200, Batch 2/17, Loss G: 4.077548980712891, Loss D: 0.08130946755409241\n",
      "Epoch 91/200, Batch 3/17, Loss G: 4.557436466217041, Loss D: 0.3886042535305023\n",
      "Epoch 91/200, Batch 4/17, Loss G: 4.109160900115967, Loss D: 0.08591191470623016\n",
      "Epoch 91/200, Batch 5/17, Loss G: 3.911691427230835, Loss D: 0.11485615372657776\n",
      "Epoch 91/200, Batch 6/17, Loss G: 4.391345500946045, Loss D: 0.1959833949804306\n",
      "Epoch 91/200, Batch 7/17, Loss G: 3.915388584136963, Loss D: 0.13190647959709167\n",
      "Epoch 91/200, Batch 8/17, Loss G: 4.154933452606201, Loss D: 0.04927203059196472\n",
      "Epoch 91/200, Batch 9/17, Loss G: 4.132589817047119, Loss D: 0.0392189547419548\n",
      "Epoch 91/200, Batch 10/17, Loss G: 4.437736988067627, Loss D: 0.041876859962940216\n",
      "Epoch 91/200, Batch 11/17, Loss G: 3.6264028549194336, Loss D: 0.20178435742855072\n",
      "Epoch 91/200, Batch 12/17, Loss G: 4.293595314025879, Loss D: 0.2427515685558319\n",
      "Epoch 91/200, Batch 13/17, Loss G: 3.9552645683288574, Loss D: 0.22217130661010742\n",
      "Epoch 91/200, Batch 14/17, Loss G: 4.298236846923828, Loss D: 0.09002514183521271\n",
      "Epoch 91/200, Batch 15/17, Loss G: 3.91683292388916, Loss D: 0.07386497408151627\n",
      "Epoch 91/200, Batch 16/17, Loss G: 4.090213298797607, Loss D: 0.06126291677355766\n",
      "Epoch 92/200, Batch 0/17, Loss G: 4.102283477783203, Loss D: 0.038716066628694534\n",
      "Epoch 92/200, Batch 1/17, Loss G: 4.066595554351807, Loss D: 0.03696820139884949\n",
      "Epoch 92/200, Batch 2/17, Loss G: 4.435915470123291, Loss D: 0.027486974373459816\n",
      "Epoch 92/200, Batch 3/17, Loss G: 4.426519393920898, Loss D: 0.022003447636961937\n",
      "Epoch 92/200, Batch 4/17, Loss G: 4.265192985534668, Loss D: 0.017325222492218018\n",
      "Epoch 92/200, Batch 5/17, Loss G: 4.215609550476074, Loss D: 0.021134579554200172\n",
      "Epoch 92/200, Batch 6/17, Loss G: 4.484816551208496, Loss D: 0.015249205753207207\n",
      "Epoch 92/200, Batch 7/17, Loss G: 4.107081890106201, Loss D: 0.017378484830260277\n",
      "Epoch 92/200, Batch 8/17, Loss G: 4.265528202056885, Loss D: 0.02132328227162361\n",
      "Epoch 92/200, Batch 9/17, Loss G: 4.412877082824707, Loss D: 0.026997223496437073\n",
      "Epoch 92/200, Batch 10/17, Loss G: 4.166911602020264, Loss D: 0.04883382096886635\n",
      "Epoch 92/200, Batch 11/17, Loss G: 4.345118522644043, Loss D: 0.03359583020210266\n",
      "Epoch 92/200, Batch 12/17, Loss G: 4.069572925567627, Loss D: 0.036230456084012985\n",
      "Epoch 92/200, Batch 13/17, Loss G: 4.102572441101074, Loss D: 0.030174076557159424\n",
      "Epoch 92/200, Batch 14/17, Loss G: 4.2425384521484375, Loss D: 0.009853310883045197\n",
      "Epoch 92/200, Batch 15/17, Loss G: 4.282553195953369, Loss D: 0.006435412913560867\n",
      "Epoch 92/200, Batch 16/17, Loss G: 4.227571487426758, Loss D: 0.023367421701550484\n",
      "Epoch 93/200, Batch 0/17, Loss G: 4.414933681488037, Loss D: 0.02147163823246956\n",
      "Epoch 93/200, Batch 1/17, Loss G: 4.351946830749512, Loss D: 0.022235453128814697\n",
      "Epoch 93/200, Batch 2/17, Loss G: 4.109231948852539, Loss D: 0.045638199895620346\n",
      "Epoch 93/200, Batch 3/17, Loss G: 4.302117347717285, Loss D: 0.008792179636657238\n",
      "Epoch 93/200, Batch 4/17, Loss G: 4.440920829772949, Loss D: 0.029221149161458015\n",
      "Epoch 93/200, Batch 5/17, Loss G: 4.149045944213867, Loss D: 0.027230191975831985\n",
      "Epoch 93/200, Batch 6/17, Loss G: 4.435643672943115, Loss D: 0.009835600852966309\n",
      "Epoch 93/200, Batch 7/17, Loss G: 4.266030788421631, Loss D: 0.02260724827647209\n",
      "Epoch 93/200, Batch 8/17, Loss G: 4.231630325317383, Loss D: 0.016400031745433807\n",
      "Epoch 93/200, Batch 9/17, Loss G: 4.2486982345581055, Loss D: 0.006284274160861969\n",
      "Epoch 93/200, Batch 10/17, Loss G: 4.35209321975708, Loss D: 0.005689624231308699\n",
      "Epoch 93/200, Batch 11/17, Loss G: 4.452986717224121, Loss D: 0.0034182651434093714\n",
      "Epoch 93/200, Batch 12/17, Loss G: 4.31446647644043, Loss D: 0.012183232232928276\n",
      "Epoch 93/200, Batch 13/17, Loss G: 4.161345481872559, Loss D: 0.006804226897656918\n",
      "Epoch 93/200, Batch 14/17, Loss G: 3.887629508972168, Loss D: 0.020728226751089096\n",
      "Epoch 93/200, Batch 15/17, Loss G: 4.298362731933594, Loss D: 0.00768527714535594\n",
      "Epoch 93/200, Batch 16/17, Loss G: 4.327396392822266, Loss D: 0.012557080015540123\n",
      "Epoch 94/200, Batch 0/17, Loss G: 4.2092084884643555, Loss D: 0.007969297468662262\n",
      "Epoch 94/200, Batch 1/17, Loss G: 4.14943790435791, Loss D: 0.01446574181318283\n",
      "Epoch 94/200, Batch 2/17, Loss G: 4.264721393585205, Loss D: 0.00718537624925375\n",
      "Epoch 94/200, Batch 3/17, Loss G: 4.2626190185546875, Loss D: 0.02407640404999256\n",
      "Epoch 94/200, Batch 4/17, Loss G: 4.486026763916016, Loss D: 0.030091529712080956\n",
      "Epoch 94/200, Batch 5/17, Loss G: 4.166563987731934, Loss D: 0.007531040348112583\n",
      "Epoch 94/200, Batch 6/17, Loss G: 3.946216106414795, Loss D: 0.02234923467040062\n",
      "Epoch 94/200, Batch 7/17, Loss G: 4.303136348724365, Loss D: 0.015686605125665665\n",
      "Epoch 94/200, Batch 8/17, Loss G: 4.376928806304932, Loss D: 0.021424829959869385\n",
      "Epoch 94/200, Batch 9/17, Loss G: 4.10078763961792, Loss D: 0.005171319469809532\n",
      "Epoch 94/200, Batch 10/17, Loss G: 3.967550277709961, Loss D: 0.03375861793756485\n",
      "Epoch 94/200, Batch 11/17, Loss G: 4.470783710479736, Loss D: 0.03539719060063362\n",
      "Epoch 94/200, Batch 12/17, Loss G: 4.374602794647217, Loss D: 0.0023409444838762283\n",
      "Epoch 94/200, Batch 13/17, Loss G: 4.04190731048584, Loss D: 0.020889392122626305\n",
      "Epoch 94/200, Batch 14/17, Loss G: 4.42285680770874, Loss D: 0.004624843597412109\n",
      "Epoch 94/200, Batch 15/17, Loss G: 4.347808837890625, Loss D: 0.018350202590227127\n",
      "Epoch 94/200, Batch 16/17, Loss G: 4.216943740844727, Loss D: 0.011619037948548794\n",
      "Epoch 95/200, Batch 0/17, Loss G: 4.168202877044678, Loss D: 0.010641442611813545\n",
      "Epoch 95/200, Batch 1/17, Loss G: 4.285731792449951, Loss D: 0.007204961497336626\n",
      "Epoch 95/200, Batch 2/17, Loss G: 4.142611503601074, Loss D: 0.00621142890304327\n",
      "Epoch 95/200, Batch 3/17, Loss G: 4.282342910766602, Loss D: 0.0038621029816567898\n",
      "Epoch 95/200, Batch 4/17, Loss G: 4.09553861618042, Loss D: 0.007992942817509174\n",
      "Epoch 95/200, Batch 5/17, Loss G: 4.366477966308594, Loss D: 0.003719452302902937\n",
      "Epoch 95/200, Batch 6/17, Loss G: 4.07826042175293, Loss D: 0.00837195198982954\n",
      "Epoch 95/200, Batch 7/17, Loss G: 4.346457004547119, Loss D: 0.007854237221181393\n",
      "Epoch 95/200, Batch 8/17, Loss G: 4.189223289489746, Loss D: 0.007437500171363354\n",
      "Epoch 95/200, Batch 9/17, Loss G: 4.14412784576416, Loss D: 0.007755724713206291\n",
      "Epoch 95/200, Batch 10/17, Loss G: 4.275594234466553, Loss D: 0.011410466395318508\n",
      "Epoch 95/200, Batch 11/17, Loss G: 4.296752452850342, Loss D: 0.0075030094012618065\n",
      "Epoch 95/200, Batch 12/17, Loss G: 4.257798671722412, Loss D: 0.01105749886482954\n",
      "Epoch 95/200, Batch 13/17, Loss G: 4.113807678222656, Loss D: 0.01094362698495388\n",
      "Epoch 95/200, Batch 14/17, Loss G: 4.204543113708496, Loss D: 0.008121348917484283\n",
      "Epoch 95/200, Batch 15/17, Loss G: 4.207677364349365, Loss D: 0.01474195346236229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200, Batch 16/17, Loss G: 4.271454811096191, Loss D: 0.006054470781236887\n",
      "Epoch 96/200, Batch 0/17, Loss G: 4.074648857116699, Loss D: 0.02994760312139988\n",
      "Epoch 96/200, Batch 1/17, Loss G: 4.3248467445373535, Loss D: 0.028357164934277534\n",
      "Epoch 96/200, Batch 2/17, Loss G: 4.331065654754639, Loss D: 0.001271544024348259\n",
      "Epoch 96/200, Batch 3/17, Loss G: 4.272613525390625, Loss D: 0.0004463598597794771\n",
      "Epoch 96/200, Batch 4/17, Loss G: 4.324268341064453, Loss D: 0.0005641120369546115\n",
      "Epoch 96/200, Batch 5/17, Loss G: 4.457279205322266, Loss D: 0.00052791484631598\n",
      "Epoch 96/200, Batch 6/17, Loss G: 4.268189430236816, Loss D: 0.0010165665298700333\n",
      "Epoch 96/200, Batch 7/17, Loss G: 4.232720851898193, Loss D: 0.005630248226225376\n",
      "Epoch 96/200, Batch 8/17, Loss G: 3.9899802207946777, Loss D: 0.020991764962673187\n",
      "Epoch 96/200, Batch 9/17, Loss G: 4.20079231262207, Loss D: 0.01949230208992958\n",
      "Epoch 96/200, Batch 10/17, Loss G: 4.254301071166992, Loss D: 0.0033015981316566467\n",
      "Epoch 96/200, Batch 11/17, Loss G: 4.104902267456055, Loss D: 0.012700245715677738\n",
      "Epoch 96/200, Batch 12/17, Loss G: 4.266523838043213, Loss D: 0.004952734336256981\n",
      "Epoch 96/200, Batch 13/17, Loss G: 4.226543426513672, Loss D: 0.011121794581413269\n",
      "Epoch 96/200, Batch 14/17, Loss G: 4.211088180541992, Loss D: 0.008650519885122776\n",
      "Epoch 96/200, Batch 15/17, Loss G: 4.163681507110596, Loss D: 0.012123683467507362\n",
      "Epoch 96/200, Batch 16/17, Loss G: 4.250553131103516, Loss D: 0.00901716761291027\n",
      "Epoch 97/200, Batch 0/17, Loss G: 4.099611282348633, Loss D: 0.0058424039743840694\n",
      "Epoch 97/200, Batch 1/17, Loss G: 4.354294300079346, Loss D: 0.007911412976682186\n",
      "Epoch 97/200, Batch 2/17, Loss G: 4.382918834686279, Loss D: 0.006015212740749121\n",
      "Epoch 97/200, Batch 3/17, Loss G: 4.2887983322143555, Loss D: 0.004326414316892624\n",
      "Epoch 97/200, Batch 4/17, Loss G: 4.115665435791016, Loss D: 0.009730229154229164\n",
      "Epoch 97/200, Batch 5/17, Loss G: 3.9967498779296875, Loss D: 0.00708642927929759\n",
      "Epoch 97/200, Batch 6/17, Loss G: 4.05812931060791, Loss D: 0.006636748090386391\n",
      "Epoch 97/200, Batch 7/17, Loss G: 4.196956157684326, Loss D: 0.010582240298390388\n",
      "Epoch 97/200, Batch 8/17, Loss G: 4.343655586242676, Loss D: 0.004482745658606291\n",
      "Epoch 97/200, Batch 9/17, Loss G: 4.3996734619140625, Loss D: 0.00293614505790174\n",
      "Epoch 97/200, Batch 10/17, Loss G: 4.321172714233398, Loss D: 0.003142478410154581\n",
      "Epoch 97/200, Batch 11/17, Loss G: 4.0361223220825195, Loss D: 0.003883947152644396\n",
      "Epoch 97/200, Batch 12/17, Loss G: 4.402332305908203, Loss D: 0.00367743824608624\n",
      "Epoch 97/200, Batch 13/17, Loss G: 4.376267433166504, Loss D: 0.005845081061124802\n",
      "Epoch 97/200, Batch 14/17, Loss G: 4.184750080108643, Loss D: 0.011297546327114105\n",
      "Epoch 97/200, Batch 15/17, Loss G: 4.520845413208008, Loss D: 0.020387791097164154\n",
      "Epoch 97/200, Batch 16/17, Loss G: 4.064058303833008, Loss D: 0.002620199229568243\n",
      "Epoch 98/200, Batch 0/17, Loss G: 3.845512628555298, Loss D: 0.07525710761547089\n",
      "Epoch 98/200, Batch 1/17, Loss G: 4.548574447631836, Loss D: 0.307791531085968\n",
      "Epoch 98/200, Batch 2/17, Loss G: 3.865553855895996, Loss D: 0.12410425394773483\n",
      "Epoch 98/200, Batch 3/17, Loss G: 4.27772331237793, Loss D: 0.09239131957292557\n",
      "Epoch 98/200, Batch 4/17, Loss G: 3.7547483444213867, Loss D: 0.13728471100330353\n",
      "Epoch 98/200, Batch 5/17, Loss G: 4.285542011260986, Loss D: 0.042912807315588\n",
      "Epoch 98/200, Batch 6/17, Loss G: 4.134235858917236, Loss D: 0.027957726269960403\n",
      "Epoch 98/200, Batch 7/17, Loss G: 4.015580654144287, Loss D: 0.032370977103710175\n",
      "Epoch 98/200, Batch 8/17, Loss G: 4.279989242553711, Loss D: 0.1346680223941803\n",
      "Epoch 98/200, Batch 9/17, Loss G: 3.4078593254089355, Loss D: 0.4156911373138428\n",
      "Epoch 98/200, Batch 10/17, Loss G: 3.9554495811462402, Loss D: 0.08713416010141373\n",
      "Epoch 98/200, Batch 11/17, Loss G: 4.347126007080078, Loss D: 0.4153309464454651\n",
      "Epoch 98/200, Batch 12/17, Loss G: 4.0407304763793945, Loss D: 0.18174515664577484\n",
      "Epoch 98/200, Batch 13/17, Loss G: 3.6126699447631836, Loss D: 0.28741511702537537\n",
      "Epoch 98/200, Batch 14/17, Loss G: 4.153315544128418, Loss D: 0.14771689474582672\n",
      "Epoch 98/200, Batch 15/17, Loss G: 4.282036304473877, Loss D: 0.16603000462055206\n",
      "Epoch 98/200, Batch 16/17, Loss G: 3.895137310028076, Loss D: 0.10133763402700424\n",
      "Epoch 99/200, Batch 0/17, Loss G: 4.041009426116943, Loss D: 0.08872625231742859\n",
      "Epoch 99/200, Batch 1/17, Loss G: 4.284661293029785, Loss D: 0.05378225818276405\n",
      "Epoch 99/200, Batch 2/17, Loss G: 4.055431365966797, Loss D: 0.03899742662906647\n",
      "Epoch 99/200, Batch 3/17, Loss G: 4.426086902618408, Loss D: 0.04967627301812172\n",
      "Epoch 99/200, Batch 4/17, Loss G: 3.814328908920288, Loss D: 0.07687804102897644\n",
      "Epoch 99/200, Batch 5/17, Loss G: 4.038547515869141, Loss D: 0.048271190375089645\n",
      "Epoch 99/200, Batch 6/17, Loss G: 3.9893300533294678, Loss D: 0.025636933743953705\n",
      "Epoch 99/200, Batch 7/17, Loss G: 4.185918807983398, Loss D: 0.027324244379997253\n",
      "Epoch 99/200, Batch 8/17, Loss G: 4.2033305168151855, Loss D: 0.0432279109954834\n",
      "Epoch 99/200, Batch 9/17, Loss G: 3.9611525535583496, Loss D: 0.08185619115829468\n",
      "Epoch 99/200, Batch 10/17, Loss G: 4.322957992553711, Loss D: 0.11542320251464844\n",
      "Epoch 99/200, Batch 11/17, Loss G: 3.913950204849243, Loss D: 0.04798635467886925\n",
      "Epoch 99/200, Batch 12/17, Loss G: 3.9051103591918945, Loss D: 0.03805148974061012\n",
      "Epoch 99/200, Batch 13/17, Loss G: 4.398709297180176, Loss D: 0.04971122741699219\n",
      "Epoch 99/200, Batch 14/17, Loss G: 4.215917587280273, Loss D: 0.02558153122663498\n",
      "Epoch 99/200, Batch 15/17, Loss G: 4.128163814544678, Loss D: 0.03275201842188835\n",
      "Epoch 99/200, Batch 16/17, Loss G: 4.402867794036865, Loss D: 0.049604739993810654\n",
      "Epoch 100/200, Batch 0/17, Loss G: 4.098435878753662, Loss D: 0.05150596424937248\n",
      "Epoch 100/200, Batch 1/17, Loss G: 4.268651962280273, Loss D: 0.03075929917395115\n",
      "Epoch 100/200, Batch 2/17, Loss G: 4.35536003112793, Loss D: 0.022737594321370125\n",
      "Epoch 100/200, Batch 3/17, Loss G: 4.227152347564697, Loss D: 0.024440057575702667\n",
      "Epoch 100/200, Batch 4/17, Loss G: 3.870293617248535, Loss D: 0.04673785716295242\n",
      "Epoch 100/200, Batch 5/17, Loss G: 4.290624141693115, Loss D: 0.013829439878463745\n",
      "Epoch 100/200, Batch 6/17, Loss G: 4.502490997314453, Loss D: 0.033014342188835144\n",
      "Epoch 100/200, Batch 7/17, Loss G: 3.8090381622314453, Loss D: 0.09733299911022186\n",
      "Epoch 100/200, Batch 8/17, Loss G: 4.378256797790527, Loss D: 0.09627771377563477\n",
      "Epoch 100/200, Batch 9/17, Loss G: 3.7647037506103516, Loss D: 0.04044731333851814\n",
      "Epoch 100/200, Batch 10/17, Loss G: 4.1242804527282715, Loss D: 0.024398580193519592\n",
      "Epoch 100/200, Batch 11/17, Loss G: 4.462789535522461, Loss D: 0.04864077642560005\n",
      "Epoch 100/200, Batch 12/17, Loss G: 4.071554183959961, Loss D: 0.019799673929810524\n",
      "Epoch 100/200, Batch 13/17, Loss G: 3.9880728721618652, Loss D: 0.028301820158958435\n",
      "Epoch 100/200, Batch 14/17, Loss G: 4.22575044631958, Loss D: 0.010321414098143578\n",
      "Epoch 100/200, Batch 15/17, Loss G: 4.135251045227051, Loss D: 0.03384528309106827\n",
      "Epoch 100/200, Batch 16/17, Loss G: 4.106220245361328, Loss D: 0.022950291633605957\n",
      "Epoch 101/200, Batch 0/17, Loss G: 4.173259735107422, Loss D: 0.02421424351632595\n",
      "Epoch 101/200, Batch 1/17, Loss G: 4.264485836029053, Loss D: 0.011405297555029392\n",
      "Epoch 101/200, Batch 2/17, Loss G: 4.253369331359863, Loss D: 0.025321971625089645\n",
      "Epoch 101/200, Batch 3/17, Loss G: 4.100294589996338, Loss D: 0.0129181407392025\n",
      "Epoch 101/200, Batch 4/17, Loss G: 4.038532733917236, Loss D: 0.03140702843666077\n",
      "Epoch 101/200, Batch 5/17, Loss G: 4.422561168670654, Loss D: 0.02807522378861904\n",
      "Epoch 101/200, Batch 6/17, Loss G: 4.044102191925049, Loss D: 0.008598780259490013\n",
      "Epoch 101/200, Batch 7/17, Loss G: 4.100668430328369, Loss D: 0.02014521136879921\n",
      "Epoch 101/200, Batch 8/17, Loss G: 4.151528358459473, Loss D: 0.011529834941029549\n",
      "Epoch 101/200, Batch 9/17, Loss G: 4.161569118499756, Loss D: 0.01339607872068882\n",
      "Epoch 101/200, Batch 10/17, Loss G: 4.177245616912842, Loss D: 0.00739927776157856\n",
      "Epoch 101/200, Batch 11/17, Loss G: 4.215302467346191, Loss D: 0.007203398738056421\n",
      "Epoch 101/200, Batch 12/17, Loss G: 4.296962738037109, Loss D: 0.007026431150734425\n",
      "Epoch 101/200, Batch 13/17, Loss G: 4.143004417419434, Loss D: 0.008553638122975826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200, Batch 14/17, Loss G: 4.073544025421143, Loss D: 0.018236802890896797\n",
      "Epoch 101/200, Batch 15/17, Loss G: 4.277355670928955, Loss D: 0.021831097081303596\n",
      "Epoch 101/200, Batch 16/17, Loss G: 4.113539218902588, Loss D: 0.015108561143279076\n",
      "Epoch 102/200, Batch 0/17, Loss G: 4.4142303466796875, Loss D: 0.010179946199059486\n",
      "Epoch 102/200, Batch 1/17, Loss G: 4.28577995300293, Loss D: 0.006824260577559471\n",
      "Epoch 102/200, Batch 2/17, Loss G: 4.421274185180664, Loss D: 0.006531785242259502\n",
      "Epoch 102/200, Batch 3/17, Loss G: 4.123842716217041, Loss D: 0.00804953183978796\n",
      "Epoch 102/200, Batch 4/17, Loss G: 4.238758563995361, Loss D: 0.00960333552211523\n",
      "Epoch 102/200, Batch 5/17, Loss G: 4.236399173736572, Loss D: 0.014915353618562222\n",
      "Epoch 102/200, Batch 6/17, Loss G: 4.196998596191406, Loss D: 0.009670675732195377\n",
      "Epoch 102/200, Batch 7/17, Loss G: 4.230207443237305, Loss D: 0.007333623245358467\n",
      "Epoch 102/200, Batch 8/17, Loss G: 4.2098774909973145, Loss D: 0.0046440474689006805\n",
      "Epoch 102/200, Batch 9/17, Loss G: 4.318864822387695, Loss D: 0.006740173790603876\n",
      "Epoch 102/200, Batch 10/17, Loss G: 4.168134689331055, Loss D: 0.008216165006160736\n",
      "Epoch 102/200, Batch 11/17, Loss G: 4.113572120666504, Loss D: 0.007815344259142876\n",
      "Epoch 102/200, Batch 12/17, Loss G: 4.0789713859558105, Loss D: 0.007205513305962086\n",
      "Epoch 102/200, Batch 13/17, Loss G: 3.8557097911834717, Loss D: 0.0072340816259384155\n",
      "Epoch 102/200, Batch 14/17, Loss G: 4.087556838989258, Loss D: 0.008255272172391415\n",
      "Epoch 102/200, Batch 15/17, Loss G: 4.158422946929932, Loss D: 0.012924512848258018\n",
      "Epoch 102/200, Batch 16/17, Loss G: 4.306393623352051, Loss D: 0.005402981303632259\n",
      "Epoch 103/200, Batch 0/17, Loss G: 4.085648536682129, Loss D: 0.026934677734971046\n",
      "Epoch 103/200, Batch 1/17, Loss G: 4.3627705574035645, Loss D: 0.07360556721687317\n",
      "Epoch 103/200, Batch 2/17, Loss G: 3.401494026184082, Loss D: 0.16072610020637512\n",
      "Epoch 103/200, Batch 3/17, Loss G: 4.12973690032959, Loss D: 0.07131189852952957\n",
      "Epoch 103/200, Batch 4/17, Loss G: 4.263159275054932, Loss D: 0.023319846019148827\n",
      "Epoch 103/200, Batch 5/17, Loss G: 4.135115623474121, Loss D: 0.017445262521505356\n",
      "Epoch 103/200, Batch 6/17, Loss G: 4.435462951660156, Loss D: 0.008030516095459461\n",
      "Epoch 103/200, Batch 7/17, Loss G: 4.33880615234375, Loss D: 0.014537308365106583\n",
      "Epoch 103/200, Batch 8/17, Loss G: 4.239833354949951, Loss D: 0.01413291972130537\n",
      "Epoch 103/200, Batch 9/17, Loss G: 4.115035533905029, Loss D: 0.008933852426707745\n",
      "Epoch 103/200, Batch 10/17, Loss G: 3.8756461143493652, Loss D: 0.028019241988658905\n",
      "Epoch 103/200, Batch 11/17, Loss G: 4.259956359863281, Loss D: 0.029248610138893127\n",
      "Epoch 103/200, Batch 12/17, Loss G: 3.948855400085449, Loss D: 0.017398832365870476\n",
      "Epoch 103/200, Batch 13/17, Loss G: 4.330411911010742, Loss D: 0.006825065240263939\n",
      "Epoch 103/200, Batch 14/17, Loss G: 4.353391647338867, Loss D: 0.01253717765212059\n",
      "Epoch 103/200, Batch 15/17, Loss G: 4.204093933105469, Loss D: 0.014566691592335701\n",
      "Epoch 103/200, Batch 16/17, Loss G: 3.9599552154541016, Loss D: 0.010040881112217903\n",
      "Epoch 104/200, Batch 0/17, Loss G: 4.284189701080322, Loss D: 0.005073246080428362\n",
      "Epoch 104/200, Batch 1/17, Loss G: 4.088143825531006, Loss D: 0.019183088093996048\n",
      "Epoch 104/200, Batch 2/17, Loss G: 4.4093756675720215, Loss D: 0.030516283586621284\n",
      "Epoch 104/200, Batch 3/17, Loss G: 4.075100421905518, Loss D: 0.02395748533308506\n",
      "Epoch 104/200, Batch 4/17, Loss G: 4.1453399658203125, Loss D: 0.006071265786886215\n",
      "Epoch 104/200, Batch 5/17, Loss G: 4.335653305053711, Loss D: 0.003996780142188072\n",
      "Epoch 104/200, Batch 6/17, Loss G: 4.15125036239624, Loss D: 0.01140342652797699\n",
      "Epoch 104/200, Batch 7/17, Loss G: 4.083665370941162, Loss D: 0.007422407623380423\n",
      "Epoch 104/200, Batch 8/17, Loss G: 4.119714736938477, Loss D: 0.004888600669801235\n",
      "Epoch 104/200, Batch 9/17, Loss G: 4.118331432342529, Loss D: 0.017505619674921036\n",
      "Epoch 104/200, Batch 10/17, Loss G: 4.447113990783691, Loss D: 0.005720476619899273\n",
      "Epoch 104/200, Batch 11/17, Loss G: 4.2547807693481445, Loss D: 0.008600535802543163\n",
      "Epoch 104/200, Batch 12/17, Loss G: 4.100092887878418, Loss D: 0.0054111904464662075\n",
      "Epoch 104/200, Batch 13/17, Loss G: 4.105162143707275, Loss D: 0.005432792939245701\n",
      "Epoch 104/200, Batch 14/17, Loss G: 3.9403233528137207, Loss D: 0.025849569588899612\n",
      "Epoch 104/200, Batch 15/17, Loss G: 4.338831901550293, Loss D: 0.01165807992219925\n",
      "Epoch 104/200, Batch 16/17, Loss G: 4.465065002441406, Loss D: 0.054515186697244644\n",
      "Epoch 105/200, Batch 0/17, Loss G: 3.7361927032470703, Loss D: 0.1003858670592308\n",
      "Epoch 105/200, Batch 1/17, Loss G: 4.3366241455078125, Loss D: 0.011509690433740616\n",
      "Epoch 105/200, Batch 2/17, Loss G: 4.178940296173096, Loss D: 0.013934596441686153\n",
      "Epoch 105/200, Batch 3/17, Loss G: 3.6737284660339355, Loss D: 0.10955964028835297\n",
      "Epoch 105/200, Batch 4/17, Loss G: 4.530246734619141, Loss D: 0.4296078383922577\n",
      "Epoch 105/200, Batch 5/17, Loss G: 4.111248016357422, Loss D: 0.11060632765293121\n",
      "Epoch 105/200, Batch 6/17, Loss G: 3.6084232330322266, Loss D: 0.16827204823493958\n",
      "Epoch 105/200, Batch 7/17, Loss G: 4.378795623779297, Loss D: 0.2101576030254364\n",
      "Epoch 105/200, Batch 8/17, Loss G: 3.4374232292175293, Loss D: 0.1744278371334076\n",
      "Epoch 105/200, Batch 9/17, Loss G: 4.12157678604126, Loss D: 0.13663505017757416\n",
      "Epoch 105/200, Batch 10/17, Loss G: 3.9287004470825195, Loss D: 0.06531431525945663\n",
      "Epoch 105/200, Batch 11/17, Loss G: 4.175856590270996, Loss D: 0.05818580463528633\n",
      "Epoch 105/200, Batch 12/17, Loss G: 3.8740456104278564, Loss D: 0.039532456547021866\n",
      "Epoch 105/200, Batch 13/17, Loss G: 4.401738166809082, Loss D: 0.0204999428242445\n",
      "Epoch 105/200, Batch 14/17, Loss G: 3.9183754920959473, Loss D: 0.022318940609693527\n",
      "Epoch 105/200, Batch 15/17, Loss G: 4.0820841789245605, Loss D: 0.01883227378129959\n",
      "Epoch 105/200, Batch 16/17, Loss G: 4.216450214385986, Loss D: 0.01332809403538704\n",
      "Epoch 106/200, Batch 0/17, Loss G: 4.280147552490234, Loss D: 0.02661486156284809\n",
      "Epoch 106/200, Batch 1/17, Loss G: 4.378055095672607, Loss D: 0.029722312465310097\n",
      "Epoch 106/200, Batch 2/17, Loss G: 3.846198081970215, Loss D: 0.1093963012099266\n",
      "Epoch 106/200, Batch 3/17, Loss G: 4.412020683288574, Loss D: 0.19258946180343628\n",
      "Epoch 106/200, Batch 4/17, Loss G: 4.050103664398193, Loss D: 0.03142677992582321\n",
      "Epoch 106/200, Batch 5/17, Loss G: 3.8784170150756836, Loss D: 0.07650988548994064\n",
      "Epoch 106/200, Batch 6/17, Loss G: 4.499636650085449, Loss D: 0.36895838379859924\n",
      "Epoch 106/200, Batch 7/17, Loss G: 4.0476861000061035, Loss D: 0.05839383229613304\n",
      "Epoch 106/200, Batch 8/17, Loss G: 3.5659677982330322, Loss D: 0.16328540444374084\n",
      "Epoch 106/200, Batch 9/17, Loss G: 4.521240711212158, Loss D: 0.2910759449005127\n",
      "Epoch 106/200, Batch 10/17, Loss G: 3.451307773590088, Loss D: 0.36179444193840027\n",
      "Epoch 106/200, Batch 11/17, Loss G: 3.720468521118164, Loss D: 0.27028244733810425\n",
      "Epoch 106/200, Batch 12/17, Loss G: 4.213038444519043, Loss D: 0.21115905046463013\n",
      "Epoch 106/200, Batch 13/17, Loss G: 3.8292813301086426, Loss D: 0.09442509710788727\n",
      "Epoch 106/200, Batch 14/17, Loss G: 3.640582323074341, Loss D: 0.12027490884065628\n",
      "Epoch 106/200, Batch 15/17, Loss G: 4.329513072967529, Loss D: 0.13117718696594238\n",
      "Epoch 106/200, Batch 16/17, Loss G: 4.19278621673584, Loss D: 0.04688281938433647\n",
      "Epoch 107/200, Batch 0/17, Loss G: 3.8544061183929443, Loss D: 0.0540589764714241\n",
      "Epoch 107/200, Batch 1/17, Loss G: 4.244901657104492, Loss D: 0.06469842046499252\n",
      "Epoch 107/200, Batch 2/17, Loss G: 3.920301914215088, Loss D: 0.038516271859407425\n",
      "Epoch 107/200, Batch 3/17, Loss G: 4.1658525466918945, Loss D: 0.0367753691971302\n",
      "Epoch 107/200, Batch 4/17, Loss G: 4.226753234863281, Loss D: 0.07409114390611649\n",
      "Epoch 107/200, Batch 5/17, Loss G: 3.76993465423584, Loss D: 0.06595892459154129\n",
      "Epoch 107/200, Batch 6/17, Loss G: 4.310483932495117, Loss D: 0.018001146614551544\n",
      "Epoch 107/200, Batch 7/17, Loss G: 4.165131568908691, Loss D: 0.016923192888498306\n",
      "Epoch 107/200, Batch 8/17, Loss G: 3.985644578933716, Loss D: 0.024702925235033035\n",
      "Epoch 107/200, Batch 9/17, Loss G: 4.0216064453125, Loss D: 0.02490808069705963\n",
      "Epoch 107/200, Batch 10/17, Loss G: 4.275562763214111, Loss D: 0.02678005024790764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200, Batch 11/17, Loss G: 4.088839530944824, Loss D: 0.015230797231197357\n",
      "Epoch 107/200, Batch 12/17, Loss G: 4.0306010246276855, Loss D: 0.012262728065252304\n",
      "Epoch 107/200, Batch 13/17, Loss G: 4.107202053070068, Loss D: 0.014885134994983673\n",
      "Epoch 107/200, Batch 14/17, Loss G: 4.133727550506592, Loss D: 0.017964588478207588\n",
      "Epoch 107/200, Batch 15/17, Loss G: 4.280640602111816, Loss D: 0.02520029805600643\n",
      "Epoch 107/200, Batch 16/17, Loss G: 4.360404014587402, Loss D: 0.008633876219391823\n",
      "Epoch 108/200, Batch 0/17, Loss G: 3.913774013519287, Loss D: 0.022207586094737053\n",
      "Epoch 108/200, Batch 1/17, Loss G: 4.085391998291016, Loss D: 0.009157919324934483\n",
      "Epoch 108/200, Batch 2/17, Loss G: 4.23113489151001, Loss D: 0.03277294337749481\n",
      "Epoch 108/200, Batch 3/17, Loss G: 4.1348161697387695, Loss D: 0.011364547535777092\n",
      "Epoch 108/200, Batch 4/17, Loss G: 3.9883384704589844, Loss D: 0.029583202674984932\n",
      "Epoch 108/200, Batch 5/17, Loss G: 4.305628299713135, Loss D: 0.020571956411004066\n",
      "Epoch 108/200, Batch 6/17, Loss G: 4.24301815032959, Loss D: 0.010535459965467453\n",
      "Epoch 108/200, Batch 7/17, Loss G: 4.163132667541504, Loss D: 0.013219929300248623\n",
      "Epoch 108/200, Batch 8/17, Loss G: 4.012359619140625, Loss D: 0.013636519201099873\n",
      "Epoch 108/200, Batch 9/17, Loss G: 4.3080363273620605, Loss D: 0.004659420810639858\n",
      "Epoch 108/200, Batch 10/17, Loss G: 4.08094596862793, Loss D: 0.019626658409833908\n",
      "Epoch 108/200, Batch 11/17, Loss G: 4.088196754455566, Loss D: 0.009143724106252193\n",
      "Epoch 108/200, Batch 12/17, Loss G: 3.816957950592041, Loss D: 0.022298287600278854\n",
      "Epoch 108/200, Batch 13/17, Loss G: 4.267424583435059, Loss D: 0.007994825020432472\n",
      "Epoch 108/200, Batch 14/17, Loss G: 4.0469512939453125, Loss D: 0.011521926149725914\n",
      "Epoch 108/200, Batch 15/17, Loss G: 4.245769500732422, Loss D: 0.01720372587442398\n",
      "Epoch 108/200, Batch 16/17, Loss G: 4.362275123596191, Loss D: 0.009839453734457493\n",
      "Epoch 109/200, Batch 0/17, Loss G: 3.7109451293945312, Loss D: 0.0747307762503624\n",
      "Epoch 109/200, Batch 1/17, Loss G: 4.498752593994141, Loss D: 0.09840753674507141\n",
      "Epoch 109/200, Batch 2/17, Loss G: 4.131412029266357, Loss D: 0.018865320831537247\n",
      "Epoch 109/200, Batch 3/17, Loss G: 4.067482948303223, Loss D: 0.012875856831669807\n",
      "Epoch 109/200, Batch 4/17, Loss G: 4.328906536102295, Loss D: 0.01369122602045536\n",
      "Epoch 109/200, Batch 5/17, Loss G: 4.268702507019043, Loss D: 0.022213613614439964\n",
      "Epoch 109/200, Batch 6/17, Loss G: 3.7641921043395996, Loss D: 0.07700978964567184\n",
      "Epoch 109/200, Batch 7/17, Loss G: 4.2525410652160645, Loss D: 0.08242973685264587\n",
      "Epoch 109/200, Batch 8/17, Loss G: 4.165581703186035, Loss D: 0.014015257358551025\n",
      "Epoch 109/200, Batch 9/17, Loss G: 3.9634499549865723, Loss D: 0.02969762496650219\n",
      "Epoch 109/200, Batch 10/17, Loss G: 4.0579071044921875, Loss D: 0.0215748380869627\n",
      "Epoch 109/200, Batch 11/17, Loss G: 4.370870590209961, Loss D: 0.03902452811598778\n",
      "Epoch 109/200, Batch 12/17, Loss G: 4.19349479675293, Loss D: 0.005608994513750076\n",
      "Epoch 109/200, Batch 13/17, Loss G: 3.8953943252563477, Loss D: 0.037146709859371185\n",
      "Epoch 109/200, Batch 14/17, Loss G: 4.140423774719238, Loss D: 0.010915611870586872\n",
      "Epoch 109/200, Batch 15/17, Loss G: 4.111138820648193, Loss D: 0.024789396673440933\n",
      "Epoch 109/200, Batch 16/17, Loss G: 3.982250452041626, Loss D: 0.01227201335132122\n",
      "Epoch 110/200, Batch 0/17, Loss G: 3.953115463256836, Loss D: 0.020307574421167374\n",
      "Epoch 110/200, Batch 1/17, Loss G: 4.241159439086914, Loss D: 0.011148862540721893\n",
      "Epoch 110/200, Batch 2/17, Loss G: 4.090025901794434, Loss D: 0.00959373451769352\n",
      "Epoch 110/200, Batch 3/17, Loss G: 4.030646324157715, Loss D: 0.00973607785999775\n",
      "Epoch 110/200, Batch 4/17, Loss G: 4.214768886566162, Loss D: 0.007510832976549864\n",
      "Epoch 110/200, Batch 5/17, Loss G: 3.9379563331604004, Loss D: 0.008038890548050404\n",
      "Epoch 110/200, Batch 6/17, Loss G: 4.230222225189209, Loss D: 0.00911608338356018\n",
      "Epoch 110/200, Batch 7/17, Loss G: 4.300119400024414, Loss D: 0.006086301524192095\n",
      "Epoch 110/200, Batch 8/17, Loss G: 4.216916084289551, Loss D: 0.02230668254196644\n",
      "Epoch 110/200, Batch 9/17, Loss G: 3.823612689971924, Loss D: 0.04746134951710701\n",
      "Epoch 110/200, Batch 10/17, Loss G: 4.211048126220703, Loss D: 0.04329441115260124\n",
      "Epoch 110/200, Batch 11/17, Loss G: 4.07270622253418, Loss D: 0.03333941474556923\n",
      "Epoch 110/200, Batch 12/17, Loss G: 4.146119594573975, Loss D: 0.00982639379799366\n",
      "Epoch 110/200, Batch 13/17, Loss G: 4.243133068084717, Loss D: 0.006343462970107794\n",
      "Epoch 110/200, Batch 14/17, Loss G: 4.011753559112549, Loss D: 0.010205447673797607\n",
      "Epoch 110/200, Batch 15/17, Loss G: 4.147852897644043, Loss D: 0.006888533476740122\n",
      "Epoch 110/200, Batch 16/17, Loss G: 4.077736854553223, Loss D: 0.004807140678167343\n",
      "Epoch 111/200, Batch 0/17, Loss G: 3.9634299278259277, Loss D: 0.01032361201941967\n",
      "Epoch 111/200, Batch 1/17, Loss G: 3.9494736194610596, Loss D: 0.016486231237649918\n",
      "Epoch 111/200, Batch 2/17, Loss G: 4.290067672729492, Loss D: 0.013233809731900692\n",
      "Epoch 111/200, Batch 3/17, Loss G: 3.9586338996887207, Loss D: 0.00852183811366558\n",
      "Epoch 111/200, Batch 4/17, Loss G: 4.107377052307129, Loss D: 0.0053388201631605625\n",
      "Epoch 111/200, Batch 5/17, Loss G: 4.153337478637695, Loss D: 0.006445008330047131\n",
      "Epoch 111/200, Batch 6/17, Loss G: 4.083719730377197, Loss D: 0.011737395077943802\n",
      "Epoch 111/200, Batch 7/17, Loss G: 4.291599273681641, Loss D: 0.006920462008565664\n",
      "Epoch 111/200, Batch 8/17, Loss G: 4.1894683837890625, Loss D: 0.007985216565430164\n",
      "Epoch 111/200, Batch 9/17, Loss G: 4.158857345581055, Loss D: 0.006828834768384695\n",
      "Epoch 111/200, Batch 10/17, Loss G: 4.091922283172607, Loss D: 0.0085647227242589\n",
      "Epoch 111/200, Batch 11/17, Loss G: 4.137062072753906, Loss D: 0.0047762347385287285\n",
      "Epoch 111/200, Batch 12/17, Loss G: 4.0189361572265625, Loss D: 0.013871853239834309\n",
      "Epoch 111/200, Batch 13/17, Loss G: 4.0832319259643555, Loss D: 0.005706385243684053\n",
      "Epoch 111/200, Batch 14/17, Loss G: 4.289622783660889, Loss D: 0.008723354898393154\n",
      "Epoch 111/200, Batch 15/17, Loss G: 4.096590995788574, Loss D: 0.0052818492986261845\n",
      "Epoch 111/200, Batch 16/17, Loss G: 4.14283561706543, Loss D: 0.005291305016726255\n",
      "Epoch 112/200, Batch 0/17, Loss G: 3.930765151977539, Loss D: 0.01240350492298603\n",
      "Epoch 112/200, Batch 1/17, Loss G: 4.161693096160889, Loss D: 0.006464041769504547\n",
      "Epoch 112/200, Batch 2/17, Loss G: 4.177122116088867, Loss D: 0.015414182096719742\n",
      "Epoch 112/200, Batch 3/17, Loss G: 4.211973190307617, Loss D: 0.008625321090221405\n",
      "Epoch 112/200, Batch 4/17, Loss G: 4.177247047424316, Loss D: 0.005643115844577551\n",
      "Epoch 112/200, Batch 5/17, Loss G: 4.297866344451904, Loss D: 0.0023326766677200794\n",
      "Epoch 112/200, Batch 6/17, Loss G: 3.997006416320801, Loss D: 0.006546393968164921\n",
      "Epoch 112/200, Batch 7/17, Loss G: 4.062422275543213, Loss D: 0.00787266343832016\n",
      "Epoch 112/200, Batch 8/17, Loss G: 4.0824809074401855, Loss D: 0.004561738111078739\n",
      "Epoch 112/200, Batch 9/17, Loss G: 4.025624752044678, Loss D: 0.013838620856404305\n",
      "Epoch 112/200, Batch 10/17, Loss G: 4.392886161804199, Loss D: 0.02734418399631977\n",
      "Epoch 112/200, Batch 11/17, Loss G: 3.869082450866699, Loss D: 0.02854972518980503\n",
      "Epoch 112/200, Batch 12/17, Loss G: 4.225107192993164, Loss D: 0.0032411012798547745\n",
      "Epoch 112/200, Batch 13/17, Loss G: 4.155282497406006, Loss D: 0.026117201894521713\n",
      "Epoch 112/200, Batch 14/17, Loss G: 3.866741180419922, Loss D: 0.03393799439072609\n",
      "Epoch 112/200, Batch 15/17, Loss G: 4.1995697021484375, Loss D: 0.0077294218353927135\n",
      "Epoch 112/200, Batch 16/17, Loss G: 4.25650691986084, Loss D: 0.0380486398935318\n",
      "Epoch 113/200, Batch 0/17, Loss G: 3.472909927368164, Loss D: 0.16201575100421906\n",
      "Epoch 113/200, Batch 1/17, Loss G: 4.375792026519775, Loss D: 0.15676017105579376\n",
      "Epoch 113/200, Batch 2/17, Loss G: 3.730471611022949, Loss D: 0.16688957810401917\n",
      "Epoch 113/200, Batch 3/17, Loss G: 4.17188835144043, Loss D: 0.18266436457633972\n",
      "Epoch 113/200, Batch 4/17, Loss G: 3.6302781105041504, Loss D: 0.1458703875541687\n",
      "Epoch 113/200, Batch 5/17, Loss G: 4.023289680480957, Loss D: 0.04842504486441612\n",
      "Epoch 113/200, Batch 6/17, Loss G: 4.25941276550293, Loss D: 0.20498767495155334\n",
      "Epoch 113/200, Batch 7/17, Loss G: 3.503892421722412, Loss D: 0.27431362867355347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200, Batch 8/17, Loss G: 4.04539680480957, Loss D: 0.05943625420331955\n",
      "Epoch 113/200, Batch 9/17, Loss G: 4.079538345336914, Loss D: 0.042533792555332184\n",
      "Epoch 113/200, Batch 10/17, Loss G: 3.7474746704101562, Loss D: 0.08297829329967499\n",
      "Epoch 113/200, Batch 11/17, Loss G: 4.096762657165527, Loss D: 0.07840164005756378\n",
      "Epoch 113/200, Batch 12/17, Loss G: 3.7872395515441895, Loss D: 0.0640546903014183\n",
      "Epoch 113/200, Batch 13/17, Loss G: 4.099830150604248, Loss D: 0.028908979147672653\n",
      "Epoch 113/200, Batch 14/17, Loss G: 4.112131118774414, Loss D: 0.025684820488095284\n",
      "Epoch 113/200, Batch 15/17, Loss G: 3.9304921627044678, Loss D: 0.02037632092833519\n",
      "Epoch 113/200, Batch 16/17, Loss G: 4.155684471130371, Loss D: 0.026460491120815277\n",
      "Epoch 114/200, Batch 0/17, Loss G: 4.127358436584473, Loss D: 0.015112883411347866\n",
      "Epoch 114/200, Batch 1/17, Loss G: 4.054907321929932, Loss D: 0.02251499518752098\n",
      "Epoch 114/200, Batch 2/17, Loss G: 4.153136730194092, Loss D: 0.010632755234837532\n",
      "Epoch 114/200, Batch 3/17, Loss G: 4.042380332946777, Loss D: 0.020913317799568176\n",
      "Epoch 114/200, Batch 4/17, Loss G: 4.191023826599121, Loss D: 0.014711732976138592\n",
      "Epoch 114/200, Batch 5/17, Loss G: 4.131214618682861, Loss D: 0.013502482324838638\n",
      "Epoch 114/200, Batch 6/17, Loss G: 4.111593723297119, Loss D: 0.016883661970496178\n",
      "Epoch 114/200, Batch 7/17, Loss G: 4.160691261291504, Loss D: 0.008878981694579124\n",
      "Epoch 114/200, Batch 8/17, Loss G: 4.010264873504639, Loss D: 0.03185271471738815\n",
      "Epoch 114/200, Batch 9/17, Loss G: 4.0728654861450195, Loss D: 0.01838928461074829\n",
      "Epoch 114/200, Batch 10/17, Loss G: 4.263211727142334, Loss D: 0.013689409010112286\n",
      "Epoch 114/200, Batch 11/17, Loss G: 4.070661544799805, Loss D: 0.009860746562480927\n",
      "Epoch 114/200, Batch 12/17, Loss G: 3.924778461456299, Loss D: 0.010568276047706604\n",
      "Epoch 114/200, Batch 13/17, Loss G: 4.076984882354736, Loss D: 0.006502564065158367\n",
      "Epoch 114/200, Batch 14/17, Loss G: 3.923950433731079, Loss D: 0.009893180802464485\n",
      "Epoch 114/200, Batch 15/17, Loss G: 4.293252468109131, Loss D: 0.012267634272575378\n",
      "Epoch 114/200, Batch 16/17, Loss G: 3.9695496559143066, Loss D: 0.022188492119312286\n",
      "Epoch 115/200, Batch 0/17, Loss G: 4.252811908721924, Loss D: 0.012431834824383259\n",
      "Epoch 115/200, Batch 1/17, Loss G: 3.9924838542938232, Loss D: 0.017559431493282318\n",
      "Epoch 115/200, Batch 2/17, Loss G: 3.9881362915039062, Loss D: 0.015743978321552277\n",
      "Epoch 115/200, Batch 3/17, Loss G: 4.112574577331543, Loss D: 0.009248586371541023\n",
      "Epoch 115/200, Batch 4/17, Loss G: 3.9439754486083984, Loss D: 0.0064939712174236774\n",
      "Epoch 115/200, Batch 5/17, Loss G: 3.9584288597106934, Loss D: 0.011914603412151337\n",
      "Epoch 115/200, Batch 6/17, Loss G: 3.9777345657348633, Loss D: 0.008388589136302471\n",
      "Epoch 115/200, Batch 7/17, Loss G: 4.0983567237854, Loss D: 0.012698284350335598\n",
      "Epoch 115/200, Batch 8/17, Loss G: 4.181681156158447, Loss D: 0.005780685227364302\n",
      "Epoch 115/200, Batch 9/17, Loss G: 4.265181541442871, Loss D: 0.007981638424098492\n",
      "Epoch 115/200, Batch 10/17, Loss G: 4.179352283477783, Loss D: 0.013101637363433838\n",
      "Epoch 115/200, Batch 11/17, Loss G: 4.038641929626465, Loss D: 0.003042601514607668\n",
      "Epoch 115/200, Batch 12/17, Loss G: 3.897754192352295, Loss D: 0.013308096677064896\n",
      "Epoch 115/200, Batch 13/17, Loss G: 4.103926658630371, Loss D: 0.0056738900020718575\n",
      "Epoch 115/200, Batch 14/17, Loss G: 4.150379180908203, Loss D: 0.012392699718475342\n",
      "Epoch 115/200, Batch 15/17, Loss G: 4.040221214294434, Loss D: 0.010198253206908703\n",
      "Epoch 115/200, Batch 16/17, Loss G: 4.30244255065918, Loss D: 0.007955232635140419\n",
      "Epoch 116/200, Batch 0/17, Loss G: 4.0230183601379395, Loss D: 0.010229493491351604\n",
      "Epoch 116/200, Batch 1/17, Loss G: 4.204765319824219, Loss D: 0.004642140120267868\n",
      "Epoch 116/200, Batch 2/17, Loss G: 4.121598720550537, Loss D: 0.008371174335479736\n",
      "Epoch 116/200, Batch 3/17, Loss G: 3.9197659492492676, Loss D: 0.011349589563906193\n",
      "Epoch 116/200, Batch 4/17, Loss G: 4.1020002365112305, Loss D: 0.0062422798946499825\n",
      "Epoch 116/200, Batch 5/17, Loss G: 3.886227607727051, Loss D: 0.059243544936180115\n",
      "Epoch 116/200, Batch 6/17, Loss G: 4.318878650665283, Loss D: 0.23424196243286133\n",
      "Epoch 116/200, Batch 7/17, Loss G: 3.94212007522583, Loss D: 0.08890318870544434\n",
      "Epoch 116/200, Batch 8/17, Loss G: 4.008781433105469, Loss D: 0.031054921448230743\n",
      "Epoch 116/200, Batch 9/17, Loss G: 4.30515193939209, Loss D: 0.043611735105514526\n",
      "Epoch 116/200, Batch 10/17, Loss G: 3.8904287815093994, Loss D: 0.04197588935494423\n",
      "Epoch 116/200, Batch 11/17, Loss G: 4.185274124145508, Loss D: 0.016132496297359467\n",
      "Epoch 116/200, Batch 12/17, Loss G: 4.13399600982666, Loss D: 0.004027653019875288\n",
      "Epoch 116/200, Batch 13/17, Loss G: 3.923509359359741, Loss D: 0.012784864753484726\n",
      "Epoch 116/200, Batch 14/17, Loss G: 4.115629196166992, Loss D: 0.007907744497060776\n",
      "Epoch 116/200, Batch 15/17, Loss G: 4.189255714416504, Loss D: 0.007848737761378288\n",
      "Epoch 116/200, Batch 16/17, Loss G: 3.9913809299468994, Loss D: 0.008326061069965363\n",
      "Epoch 117/200, Batch 0/17, Loss G: 3.9389071464538574, Loss D: 0.02204805240035057\n",
      "Epoch 117/200, Batch 1/17, Loss G: 4.076689720153809, Loss D: 0.04621213302016258\n",
      "Epoch 117/200, Batch 2/17, Loss G: 3.852923631668091, Loss D: 0.03869626298546791\n",
      "Epoch 117/200, Batch 3/17, Loss G: 4.209621429443359, Loss D: 0.010023048147559166\n",
      "Epoch 117/200, Batch 4/17, Loss G: 4.245924472808838, Loss D: 0.05138548091053963\n",
      "Epoch 117/200, Batch 5/17, Loss G: 3.506784677505493, Loss D: 0.17113368213176727\n",
      "Epoch 117/200, Batch 6/17, Loss G: 4.150007247924805, Loss D: 0.0865207314491272\n",
      "Epoch 117/200, Batch 7/17, Loss G: 3.925305128097534, Loss D: 0.015490692108869553\n",
      "Epoch 117/200, Batch 8/17, Loss G: 3.718921184539795, Loss D: 0.04611976072192192\n",
      "Epoch 117/200, Batch 9/17, Loss G: 4.093709945678711, Loss D: 0.08660214394330978\n",
      "Epoch 117/200, Batch 10/17, Loss G: 3.9088828563690186, Loss D: 0.03716970980167389\n",
      "Epoch 117/200, Batch 11/17, Loss G: 4.20789909362793, Loss D: 0.009338971227407455\n",
      "Epoch 117/200, Batch 12/17, Loss G: 4.288580894470215, Loss D: 0.009809977374970913\n",
      "Epoch 117/200, Batch 13/17, Loss G: 4.119818210601807, Loss D: 0.0171554833650589\n",
      "Epoch 117/200, Batch 14/17, Loss G: 4.024666786193848, Loss D: 0.015982812270522118\n",
      "Epoch 117/200, Batch 15/17, Loss G: 4.107772350311279, Loss D: 0.028474338352680206\n",
      "Epoch 117/200, Batch 16/17, Loss G: 3.822538375854492, Loss D: 0.03232625871896744\n",
      "Epoch 118/200, Batch 0/17, Loss G: 4.1206159591674805, Loss D: 0.01328615378588438\n",
      "Epoch 118/200, Batch 1/17, Loss G: 4.3545403480529785, Loss D: 0.04268956556916237\n",
      "Epoch 118/200, Batch 2/17, Loss G: 3.993471622467041, Loss D: 0.0184036735445261\n",
      "Epoch 118/200, Batch 3/17, Loss G: 4.1732096672058105, Loss D: 0.007429315242916346\n",
      "Epoch 118/200, Batch 4/17, Loss G: 4.127767086029053, Loss D: 0.015462925657629967\n",
      "Epoch 118/200, Batch 5/17, Loss G: 4.241343021392822, Loss D: 0.0687246024608612\n",
      "Epoch 118/200, Batch 6/17, Loss G: 3.6920924186706543, Loss D: 0.059648897498846054\n",
      "Epoch 118/200, Batch 7/17, Loss G: 4.056665897369385, Loss D: 0.02156206965446472\n",
      "Epoch 118/200, Batch 8/17, Loss G: 4.258879661560059, Loss D: 0.0354297049343586\n",
      "Epoch 118/200, Batch 9/17, Loss G: 4.104440689086914, Loss D: 0.004458258394151926\n",
      "Epoch 118/200, Batch 10/17, Loss G: 4.225261688232422, Loss D: 0.007762867026031017\n",
      "Epoch 118/200, Batch 11/17, Loss G: 3.9088492393493652, Loss D: 0.01748700439929962\n",
      "Epoch 118/200, Batch 12/17, Loss G: 4.286711692810059, Loss D: 0.0067232390865683556\n",
      "Epoch 118/200, Batch 13/17, Loss G: 4.216434955596924, Loss D: 0.005340675823390484\n",
      "Epoch 118/200, Batch 14/17, Loss G: 4.0359296798706055, Loss D: 0.012422851286828518\n",
      "Epoch 118/200, Batch 15/17, Loss G: 4.039614677429199, Loss D: 0.004565584473311901\n",
      "Epoch 118/200, Batch 16/17, Loss G: 3.9728102684020996, Loss D: 0.009735070168972015\n",
      "Epoch 119/200, Batch 0/17, Loss G: 4.031081199645996, Loss D: 0.014551304280757904\n",
      "Epoch 119/200, Batch 1/17, Loss G: 4.247265338897705, Loss D: 0.006814075633883476\n",
      "Epoch 119/200, Batch 2/17, Loss G: 4.131566524505615, Loss D: 0.009123110212385654\n",
      "Epoch 119/200, Batch 3/17, Loss G: 4.1411848068237305, Loss D: 0.006549754645675421\n",
      "Epoch 119/200, Batch 4/17, Loss G: 4.053689956665039, Loss D: 0.006844165734946728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200, Batch 5/17, Loss G: 4.053335189819336, Loss D: 0.00549577409401536\n",
      "Epoch 119/200, Batch 6/17, Loss G: 4.109676361083984, Loss D: 0.007943572476506233\n",
      "Epoch 119/200, Batch 7/17, Loss G: 3.9742746353149414, Loss D: 0.009078841656446457\n",
      "Epoch 119/200, Batch 8/17, Loss G: 4.050459861755371, Loss D: 0.005347664467990398\n",
      "Epoch 119/200, Batch 9/17, Loss G: 3.8628227710723877, Loss D: 0.005172126926481724\n",
      "Epoch 119/200, Batch 10/17, Loss G: 4.02692174911499, Loss D: 0.009510313160717487\n",
      "Epoch 119/200, Batch 11/17, Loss G: 4.2151079177856445, Loss D: 0.004771126434206963\n",
      "Epoch 119/200, Batch 12/17, Loss G: 4.091756343841553, Loss D: 0.008563125506043434\n",
      "Epoch 119/200, Batch 13/17, Loss G: 4.0361199378967285, Loss D: 0.0032092416658997536\n",
      "Epoch 119/200, Batch 14/17, Loss G: 4.108120441436768, Loss D: 0.006908097304403782\n",
      "Epoch 119/200, Batch 15/17, Loss G: 4.105666637420654, Loss D: 0.016958249732851982\n",
      "Epoch 119/200, Batch 16/17, Loss G: 3.816279888153076, Loss D: 0.02940385229885578\n",
      "Epoch 120/200, Batch 0/17, Loss G: 4.108948707580566, Loss D: 0.011957507580518723\n",
      "Epoch 120/200, Batch 1/17, Loss G: 4.004117488861084, Loss D: 0.008856546133756638\n",
      "Epoch 120/200, Batch 2/17, Loss G: 4.054472923278809, Loss D: 0.008574962615966797\n",
      "Epoch 120/200, Batch 3/17, Loss G: 4.161219596862793, Loss D: 0.004041449632495642\n",
      "Epoch 120/200, Batch 4/17, Loss G: 3.992612361907959, Loss D: 0.006633581593632698\n",
      "Epoch 120/200, Batch 5/17, Loss G: 4.117764949798584, Loss D: 0.01136989425867796\n",
      "Epoch 120/200, Batch 6/17, Loss G: 3.954411268234253, Loss D: 0.023826811462640762\n",
      "Epoch 120/200, Batch 7/17, Loss G: 4.022800445556641, Loss D: 0.005147163290530443\n",
      "Epoch 120/200, Batch 8/17, Loss G: 4.055572509765625, Loss D: 0.005471363198012114\n",
      "Epoch 120/200, Batch 9/17, Loss G: 4.050425052642822, Loss D: 0.0063959346152842045\n",
      "Epoch 120/200, Batch 10/17, Loss G: 4.119123935699463, Loss D: 0.013258850201964378\n",
      "Epoch 120/200, Batch 11/17, Loss G: 3.9806532859802246, Loss D: 0.00869483407586813\n",
      "Epoch 120/200, Batch 12/17, Loss G: 4.132491111755371, Loss D: 0.005955319851636887\n",
      "Epoch 120/200, Batch 13/17, Loss G: 3.9833626747131348, Loss D: 0.010465310886502266\n",
      "Epoch 120/200, Batch 14/17, Loss G: 3.955245018005371, Loss D: 0.0055500054731965065\n",
      "Epoch 120/200, Batch 15/17, Loss G: 4.014012336730957, Loss D: 0.018058914691209793\n",
      "Epoch 120/200, Batch 16/17, Loss G: 4.395171642303467, Loss D: 0.013195221312344074\n",
      "Epoch 121/200, Batch 0/17, Loss G: 4.135527610778809, Loss D: 0.0033368300646543503\n",
      "Epoch 121/200, Batch 1/17, Loss G: 4.062578201293945, Loss D: 0.0021774480119347572\n",
      "Epoch 121/200, Batch 2/17, Loss G: 4.069262504577637, Loss D: 0.003377041779458523\n",
      "Epoch 121/200, Batch 3/17, Loss G: 4.063960075378418, Loss D: 0.014066611416637897\n",
      "Epoch 121/200, Batch 4/17, Loss G: 4.139916896820068, Loss D: 0.012064534239470959\n",
      "Epoch 121/200, Batch 5/17, Loss G: 3.9121599197387695, Loss D: 0.005460129585117102\n",
      "Epoch 121/200, Batch 6/17, Loss G: 4.083513259887695, Loss D: 0.0032090479508042336\n",
      "Epoch 121/200, Batch 7/17, Loss G: 3.861745834350586, Loss D: 0.004050444345921278\n",
      "Epoch 121/200, Batch 8/17, Loss G: 4.100307941436768, Loss D: 0.003192682284861803\n",
      "Epoch 121/200, Batch 9/17, Loss G: 4.039268493652344, Loss D: 0.007077869027853012\n",
      "Epoch 121/200, Batch 10/17, Loss G: 4.138599395751953, Loss D: 0.003171611810103059\n",
      "Epoch 121/200, Batch 11/17, Loss G: 4.076299667358398, Loss D: 0.00787363387644291\n",
      "Epoch 121/200, Batch 12/17, Loss G: 4.038822650909424, Loss D: 0.005613887682557106\n",
      "Epoch 121/200, Batch 13/17, Loss G: 4.05265474319458, Loss D: 0.0032731301616877317\n",
      "Epoch 121/200, Batch 14/17, Loss G: 4.030429840087891, Loss D: 0.004020941909402609\n",
      "Epoch 121/200, Batch 15/17, Loss G: 4.042202949523926, Loss D: 0.005221704952418804\n",
      "Epoch 121/200, Batch 16/17, Loss G: 4.207836151123047, Loss D: 0.02507554553449154\n",
      "Epoch 122/200, Batch 0/17, Loss G: 3.743386745452881, Loss D: 0.04612765461206436\n",
      "Epoch 122/200, Batch 1/17, Loss G: 4.328795909881592, Loss D: 0.02889384515583515\n",
      "Epoch 122/200, Batch 2/17, Loss G: 4.13854455947876, Loss D: 0.003213421907275915\n",
      "Epoch 122/200, Batch 3/17, Loss G: 4.0722761154174805, Loss D: 0.018834512680768967\n",
      "Epoch 122/200, Batch 4/17, Loss G: 4.13618278503418, Loss D: 0.013591301627457142\n",
      "Epoch 122/200, Batch 5/17, Loss G: 4.083851337432861, Loss D: 0.029332172125577927\n",
      "Epoch 122/200, Batch 6/17, Loss G: 4.001118183135986, Loss D: 0.03832704573869705\n",
      "Epoch 122/200, Batch 7/17, Loss G: 4.096068382263184, Loss D: 0.002541658002883196\n",
      "Epoch 122/200, Batch 8/17, Loss G: 4.100785255432129, Loss D: 0.005893127992749214\n",
      "Epoch 122/200, Batch 9/17, Loss G: 4.150153636932373, Loss D: 0.006033272482454777\n",
      "Epoch 122/200, Batch 10/17, Loss G: 4.019123077392578, Loss D: 0.006496134679764509\n",
      "Epoch 122/200, Batch 11/17, Loss G: 4.088311195373535, Loss D: 0.0028310834895819426\n",
      "Epoch 122/200, Batch 12/17, Loss G: 3.8707997798919678, Loss D: 0.017646417021751404\n",
      "Epoch 122/200, Batch 13/17, Loss G: 4.031313896179199, Loss D: 0.029514441266655922\n",
      "Epoch 122/200, Batch 14/17, Loss G: 4.1952691078186035, Loss D: 0.0015519799198955297\n",
      "Epoch 122/200, Batch 15/17, Loss G: 3.888188123703003, Loss D: 0.013502277433872223\n",
      "Epoch 122/200, Batch 16/17, Loss G: 4.1970014572143555, Loss D: 0.0012836247915402055\n",
      "Epoch 123/200, Batch 0/17, Loss G: 4.187161445617676, Loss D: 0.0010167316067963839\n",
      "Epoch 123/200, Batch 1/17, Loss G: 4.066695213317871, Loss D: 0.001392007921822369\n",
      "Epoch 123/200, Batch 2/17, Loss G: 4.028462886810303, Loss D: 0.001937823137268424\n",
      "Epoch 123/200, Batch 3/17, Loss G: 3.9109601974487305, Loss D: 0.004578462336212397\n",
      "Epoch 123/200, Batch 4/17, Loss G: 3.982288360595703, Loss D: 0.004320516251027584\n",
      "Epoch 123/200, Batch 5/17, Loss G: 4.0212507247924805, Loss D: 0.004634261131286621\n",
      "Epoch 123/200, Batch 6/17, Loss G: 3.9985766410827637, Loss D: 0.012841970659792423\n",
      "Epoch 123/200, Batch 7/17, Loss G: 4.108950614929199, Loss D: 0.02266937494277954\n",
      "Epoch 123/200, Batch 8/17, Loss G: 4.086911201477051, Loss D: 0.003570136846974492\n",
      "Epoch 123/200, Batch 9/17, Loss G: 3.79958438873291, Loss D: 0.03917456045746803\n",
      "Epoch 123/200, Batch 10/17, Loss G: 4.253811836242676, Loss D: 0.10555492341518402\n",
      "Epoch 123/200, Batch 11/17, Loss G: 3.7936620712280273, Loss D: 0.06931472569704056\n",
      "Epoch 123/200, Batch 12/17, Loss G: 4.176352024078369, Loss D: 0.11018799245357513\n",
      "Epoch 123/200, Batch 13/17, Loss G: 3.3251028060913086, Loss D: 0.2171017974615097\n",
      "Epoch 123/200, Batch 14/17, Loss G: 3.942749500274658, Loss D: 0.059791866689920425\n",
      "Epoch 123/200, Batch 15/17, Loss G: 4.183966636657715, Loss D: 0.23631495237350464\n",
      "Epoch 123/200, Batch 16/17, Loss G: 3.462735652923584, Loss D: 0.2570514380931854\n",
      "Epoch 124/200, Batch 0/17, Loss G: 3.9320225715637207, Loss D: 0.04253346100449562\n",
      "Epoch 124/200, Batch 1/17, Loss G: 4.073178291320801, Loss D: 0.07464666664600372\n",
      "Epoch 124/200, Batch 2/17, Loss G: 3.494812488555908, Loss D: 0.17213959991931915\n",
      "Epoch 124/200, Batch 3/17, Loss G: 4.259921073913574, Loss D: 0.1600106954574585\n",
      "Epoch 124/200, Batch 4/17, Loss G: 3.5497546195983887, Loss D: 0.1433861255645752\n",
      "Epoch 124/200, Batch 5/17, Loss G: 4.056821346282959, Loss D: 0.052890628576278687\n",
      "Epoch 124/200, Batch 6/17, Loss G: 3.7643120288848877, Loss D: 0.048273004591464996\n",
      "Epoch 124/200, Batch 7/17, Loss G: 3.697843551635742, Loss D: 0.09502612054347992\n",
      "Epoch 124/200, Batch 8/17, Loss G: 4.244561195373535, Loss D: 0.13505715131759644\n",
      "Epoch 124/200, Batch 9/17, Loss G: 3.5954174995422363, Loss D: 0.07393847405910492\n",
      "Epoch 124/200, Batch 10/17, Loss G: 3.9248595237731934, Loss D: 0.036666903644800186\n",
      "Epoch 124/200, Batch 11/17, Loss G: 4.099066734313965, Loss D: 0.046078234910964966\n",
      "Epoch 124/200, Batch 12/17, Loss G: 3.8965327739715576, Loss D: 0.028129829093813896\n",
      "Epoch 124/200, Batch 13/17, Loss G: 3.8103132247924805, Loss D: 0.041103046387434006\n",
      "Epoch 124/200, Batch 14/17, Loss G: 4.195657730102539, Loss D: 0.04262019321322441\n",
      "Epoch 124/200, Batch 15/17, Loss G: 3.8157386779785156, Loss D: 0.022291533648967743\n",
      "Epoch 124/200, Batch 16/17, Loss G: 3.8551759719848633, Loss D: 0.012673788703978062\n",
      "Epoch 125/200, Batch 0/17, Loss G: 4.014021873474121, Loss D: 0.015928195789456367\n",
      "Epoch 125/200, Batch 1/17, Loss G: 3.9157938957214355, Loss D: 0.017688386142253876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200, Batch 2/17, Loss G: 3.922227621078491, Loss D: 0.016192786395549774\n",
      "Epoch 125/200, Batch 3/17, Loss G: 3.8146865367889404, Loss D: 0.019779494032263756\n",
      "Epoch 125/200, Batch 4/17, Loss G: 3.914121150970459, Loss D: 0.015353899449110031\n",
      "Epoch 125/200, Batch 5/17, Loss G: 4.159474849700928, Loss D: 0.009178776293992996\n",
      "Epoch 125/200, Batch 6/17, Loss G: 4.099644660949707, Loss D: 0.025198927149176598\n",
      "Epoch 125/200, Batch 7/17, Loss G: 4.078771591186523, Loss D: 0.009452547878026962\n",
      "Epoch 125/200, Batch 8/17, Loss G: 4.011124134063721, Loss D: 0.007833825424313545\n",
      "Epoch 125/200, Batch 9/17, Loss G: 3.8715600967407227, Loss D: 0.030004190281033516\n",
      "Epoch 125/200, Batch 10/17, Loss G: 4.14558219909668, Loss D: 0.02927478775382042\n",
      "Epoch 125/200, Batch 11/17, Loss G: 4.069204330444336, Loss D: 0.01393305417150259\n",
      "Epoch 125/200, Batch 12/17, Loss G: 4.052761077880859, Loss D: 0.020701253786683083\n",
      "Epoch 125/200, Batch 13/17, Loss G: 4.179387092590332, Loss D: 0.009354381822049618\n",
      "Epoch 125/200, Batch 14/17, Loss G: 3.9194118976593018, Loss D: 0.010584632866084576\n",
      "Epoch 125/200, Batch 15/17, Loss G: 4.07145881652832, Loss D: 0.007863285019993782\n",
      "Epoch 125/200, Batch 16/17, Loss G: 4.017904758453369, Loss D: 0.014263695105910301\n",
      "Epoch 126/200, Batch 0/17, Loss G: 4.224322319030762, Loss D: 0.009316674433648586\n",
      "Epoch 126/200, Batch 1/17, Loss G: 3.96522855758667, Loss D: 0.007348706014454365\n",
      "Epoch 126/200, Batch 2/17, Loss G: 4.161104202270508, Loss D: 0.017002709209918976\n",
      "Epoch 126/200, Batch 3/17, Loss G: 4.198826789855957, Loss D: 0.011436235159635544\n",
      "Epoch 126/200, Batch 4/17, Loss G: 4.034542083740234, Loss D: 0.015919920057058334\n",
      "Epoch 126/200, Batch 5/17, Loss G: 4.069327354431152, Loss D: 0.009065626189112663\n",
      "Epoch 126/200, Batch 6/17, Loss G: 4.074827671051025, Loss D: 0.0073873186483979225\n",
      "Epoch 126/200, Batch 7/17, Loss G: 4.058701992034912, Loss D: 0.0069331154227256775\n",
      "Epoch 126/200, Batch 8/17, Loss G: 4.409705638885498, Loss D: 0.00960955023765564\n",
      "Epoch 126/200, Batch 9/17, Loss G: 3.891741991043091, Loss D: 0.010763268917798996\n",
      "Epoch 126/200, Batch 10/17, Loss G: 4.123684406280518, Loss D: 0.00665931636467576\n",
      "Epoch 126/200, Batch 11/17, Loss G: 3.9779601097106934, Loss D: 0.0061608897522091866\n",
      "Epoch 126/200, Batch 12/17, Loss G: 3.902193307876587, Loss D: 0.005110044032335281\n",
      "Epoch 126/200, Batch 13/17, Loss G: 3.9239583015441895, Loss D: 0.004804989323019981\n",
      "Epoch 126/200, Batch 14/17, Loss G: 4.139725208282471, Loss D: 0.0056728823110461235\n",
      "Epoch 126/200, Batch 15/17, Loss G: 4.072707653045654, Loss D: 0.003245131578296423\n",
      "Epoch 126/200, Batch 16/17, Loss G: 4.073938846588135, Loss D: 0.01963799074292183\n",
      "Epoch 127/200, Batch 0/17, Loss G: 3.975583076477051, Loss D: 0.013275839388370514\n",
      "Epoch 127/200, Batch 1/17, Loss G: 4.115703582763672, Loss D: 0.006065042689442635\n",
      "Epoch 127/200, Batch 2/17, Loss G: 3.993182897567749, Loss D: 0.017445340752601624\n",
      "Epoch 127/200, Batch 3/17, Loss G: 4.2009429931640625, Loss D: 0.008753777481615543\n",
      "Epoch 127/200, Batch 4/17, Loss G: 4.058165073394775, Loss D: 0.014325667172670364\n",
      "Epoch 127/200, Batch 5/17, Loss G: 4.077629566192627, Loss D: 0.003934549167752266\n",
      "Epoch 127/200, Batch 6/17, Loss G: 4.035773754119873, Loss D: 0.005411058664321899\n",
      "Epoch 127/200, Batch 7/17, Loss G: 4.02181339263916, Loss D: 0.009523347951471806\n",
      "Epoch 127/200, Batch 8/17, Loss G: 3.822155714035034, Loss D: 0.006088102236390114\n",
      "Epoch 127/200, Batch 9/17, Loss G: 3.9247775077819824, Loss D: 0.0060314093716442585\n",
      "Epoch 127/200, Batch 10/17, Loss G: 3.9439501762390137, Loss D: 0.007293756119906902\n",
      "Epoch 127/200, Batch 11/17, Loss G: 4.042511463165283, Loss D: 0.0047050705179572105\n",
      "Epoch 127/200, Batch 12/17, Loss G: 3.978926181793213, Loss D: 0.007394179701805115\n",
      "Epoch 127/200, Batch 13/17, Loss G: 4.000904083251953, Loss D: 0.0052373381331563\n",
      "Epoch 127/200, Batch 14/17, Loss G: 3.8428382873535156, Loss D: 0.01626541092991829\n",
      "Epoch 127/200, Batch 15/17, Loss G: 4.151501655578613, Loss D: 0.0019114022143185139\n",
      "Epoch 127/200, Batch 16/17, Loss G: 3.9042768478393555, Loss D: 0.012840129435062408\n",
      "Epoch 128/200, Batch 0/17, Loss G: 4.065086841583252, Loss D: 0.0065038176253438\n",
      "Epoch 128/200, Batch 1/17, Loss G: 4.051131248474121, Loss D: 0.007020794786512852\n",
      "Epoch 128/200, Batch 2/17, Loss G: 4.050365447998047, Loss D: 0.006483268924057484\n",
      "Epoch 128/200, Batch 3/17, Loss G: 4.0233917236328125, Loss D: 0.008248759433627129\n",
      "Epoch 128/200, Batch 4/17, Loss G: 3.857219696044922, Loss D: 0.005945154931396246\n",
      "Epoch 128/200, Batch 5/17, Loss G: 3.802584171295166, Loss D: 0.006820417940616608\n",
      "Epoch 128/200, Batch 6/17, Loss G: 4.204305648803711, Loss D: 0.006138375028967857\n",
      "Epoch 128/200, Batch 7/17, Loss G: 4.040482997894287, Loss D: 0.004273982718586922\n",
      "Epoch 128/200, Batch 8/17, Loss G: 4.152174949645996, Loss D: 0.010125232860445976\n",
      "Epoch 128/200, Batch 9/17, Loss G: 4.065634727478027, Loss D: 0.007131450343877077\n",
      "Epoch 128/200, Batch 10/17, Loss G: 4.099677085876465, Loss D: 0.0030334359034895897\n",
      "Epoch 128/200, Batch 11/17, Loss G: 3.9913482666015625, Loss D: 0.014945045113563538\n",
      "Epoch 128/200, Batch 12/17, Loss G: 4.3654561042785645, Loss D: 0.017014777287840843\n",
      "Epoch 128/200, Batch 13/17, Loss G: 3.9009416103363037, Loss D: 0.01353397686034441\n",
      "Epoch 128/200, Batch 14/17, Loss G: 4.168821334838867, Loss D: 0.0059088426642119884\n",
      "Epoch 128/200, Batch 15/17, Loss G: 4.059481143951416, Loss D: 0.0024893551599234343\n",
      "Epoch 128/200, Batch 16/17, Loss G: 4.012060165405273, Loss D: 0.0023885699920356274\n",
      "Epoch 129/200, Batch 0/17, Loss G: 3.894052505493164, Loss D: 0.009203296154737473\n",
      "Epoch 129/200, Batch 1/17, Loss G: 4.055840492248535, Loss D: 0.006530200596898794\n",
      "Epoch 129/200, Batch 2/17, Loss G: 4.125693321228027, Loss D: 0.010246704332530499\n",
      "Epoch 129/200, Batch 3/17, Loss G: 4.24876070022583, Loss D: 0.0033368729054927826\n",
      "Epoch 129/200, Batch 4/17, Loss G: 3.862518787384033, Loss D: 0.007020673714578152\n",
      "Epoch 129/200, Batch 5/17, Loss G: 4.092907905578613, Loss D: 0.008675167337059975\n",
      "Epoch 129/200, Batch 6/17, Loss G: 3.9023282527923584, Loss D: 0.010558774694800377\n",
      "Epoch 129/200, Batch 7/17, Loss G: 3.983733654022217, Loss D: 0.00549307418987155\n",
      "Epoch 129/200, Batch 8/17, Loss G: 4.007609844207764, Loss D: 0.008803019300103188\n",
      "Epoch 129/200, Batch 9/17, Loss G: 3.8093342781066895, Loss D: 0.008742325007915497\n",
      "Epoch 129/200, Batch 10/17, Loss G: 3.9250941276550293, Loss D: 0.008290084078907967\n",
      "Epoch 129/200, Batch 11/17, Loss G: 4.047221660614014, Loss D: 0.017190655693411827\n",
      "Epoch 129/200, Batch 12/17, Loss G: 3.8225831985473633, Loss D: 0.03922860324382782\n",
      "Epoch 129/200, Batch 13/17, Loss G: 4.25740909576416, Loss D: 0.02421189658343792\n",
      "Epoch 129/200, Batch 14/17, Loss G: 4.047917366027832, Loss D: 0.0071393148973584175\n",
      "Epoch 129/200, Batch 15/17, Loss G: 3.995638847351074, Loss D: 0.0013356737326830626\n",
      "Epoch 129/200, Batch 16/17, Loss G: 3.757512331008911, Loss D: 0.013621180318295956\n",
      "Epoch 130/200, Batch 0/17, Loss G: 4.129583358764648, Loss D: 0.0022826616186648607\n",
      "Epoch 130/200, Batch 1/17, Loss G: 4.035694122314453, Loss D: 0.00466668838635087\n",
      "Epoch 130/200, Batch 2/17, Loss G: 3.973686695098877, Loss D: 0.00939517468214035\n",
      "Epoch 130/200, Batch 3/17, Loss G: 3.7905826568603516, Loss D: 0.00877117644995451\n",
      "Epoch 130/200, Batch 4/17, Loss G: 4.076262474060059, Loss D: 0.003454841673374176\n",
      "Epoch 130/200, Batch 5/17, Loss G: 3.8925273418426514, Loss D: 0.009150249883532524\n",
      "Epoch 130/200, Batch 6/17, Loss G: 4.170956611633301, Loss D: 0.006466031540185213\n",
      "Epoch 130/200, Batch 7/17, Loss G: 4.128952980041504, Loss D: 0.006896897219121456\n",
      "Epoch 130/200, Batch 8/17, Loss G: 4.060145854949951, Loss D: 0.004501987248659134\n",
      "Epoch 130/200, Batch 9/17, Loss G: 4.065761566162109, Loss D: 0.002834984799847007\n",
      "Epoch 130/200, Batch 10/17, Loss G: 4.134057998657227, Loss D: 0.0015127519145607948\n",
      "Epoch 130/200, Batch 11/17, Loss G: 3.9217305183410645, Loss D: 0.004390542395412922\n",
      "Epoch 130/200, Batch 12/17, Loss G: 3.975722074508667, Loss D: 0.009638749994337559\n",
      "Epoch 130/200, Batch 13/17, Loss G: 4.014657020568848, Loss D: 0.003157224738970399\n",
      "Epoch 130/200, Batch 14/17, Loss G: 4.057019233703613, Loss D: 0.004822210408747196\n",
      "Epoch 130/200, Batch 15/17, Loss G: 3.8857405185699463, Loss D: 0.01450866088271141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200, Batch 16/17, Loss G: 3.690758228302002, Loss D: 0.020875435322523117\n",
      "Epoch 131/200, Batch 0/17, Loss G: 4.084997177124023, Loss D: 0.005545036867260933\n",
      "Epoch 131/200, Batch 1/17, Loss G: 3.944653034210205, Loss D: 0.010454574599862099\n",
      "Epoch 131/200, Batch 2/17, Loss G: 3.8615474700927734, Loss D: 0.0043250094167888165\n",
      "Epoch 131/200, Batch 3/17, Loss G: 3.9555821418762207, Loss D: 0.005495773162692785\n",
      "Epoch 131/200, Batch 4/17, Loss G: 3.8596296310424805, Loss D: 0.0205455981194973\n",
      "Epoch 131/200, Batch 5/17, Loss G: 4.239645004272461, Loss D: 0.017294295132160187\n",
      "Epoch 131/200, Batch 6/17, Loss G: 3.948413372039795, Loss D: 0.00412555830553174\n",
      "Epoch 131/200, Batch 7/17, Loss G: 3.8253674507141113, Loss D: 0.029581107199192047\n",
      "Epoch 131/200, Batch 8/17, Loss G: 4.082334518432617, Loss D: 0.033691585063934326\n",
      "Epoch 131/200, Batch 9/17, Loss G: 4.076054096221924, Loss D: 0.011855353601276875\n",
      "Epoch 131/200, Batch 10/17, Loss G: 4.203073501586914, Loss D: 0.00610011164098978\n",
      "Epoch 131/200, Batch 11/17, Loss G: 4.076042175292969, Loss D: 0.009453238919377327\n",
      "Epoch 131/200, Batch 12/17, Loss G: 4.003321170806885, Loss D: 0.0023705027997493744\n",
      "Epoch 131/200, Batch 13/17, Loss G: 4.01993989944458, Loss D: 0.005445677787065506\n",
      "Epoch 131/200, Batch 14/17, Loss G: 3.9239368438720703, Loss D: 0.003816788550466299\n",
      "Epoch 131/200, Batch 15/17, Loss G: 3.985057830810547, Loss D: 0.006397427059710026\n",
      "Epoch 131/200, Batch 16/17, Loss G: 4.105543613433838, Loss D: 0.008930228650569916\n",
      "Epoch 132/200, Batch 0/17, Loss G: 4.047108173370361, Loss D: 0.006450450047850609\n",
      "Epoch 132/200, Batch 1/17, Loss G: 4.252887725830078, Loss D: 0.012124246917665005\n",
      "Epoch 132/200, Batch 2/17, Loss G: 4.080807685852051, Loss D: 0.0053220693953335285\n",
      "Epoch 132/200, Batch 3/17, Loss G: 3.838144302368164, Loss D: 0.007557720877230167\n",
      "Epoch 132/200, Batch 4/17, Loss G: 4.0702643394470215, Loss D: 0.0064639379270374775\n",
      "Epoch 132/200, Batch 5/17, Loss G: 4.0127153396606445, Loss D: 0.005341934971511364\n",
      "Epoch 132/200, Batch 6/17, Loss G: 4.082317352294922, Loss D: 0.0207842830568552\n",
      "Epoch 132/200, Batch 7/17, Loss G: 3.9878406524658203, Loss D: 0.005127356853336096\n",
      "Epoch 132/200, Batch 8/17, Loss G: 3.8210291862487793, Loss D: 0.0188519898802042\n",
      "Epoch 132/200, Batch 9/17, Loss G: 4.284607410430908, Loss D: 0.01565573178231716\n",
      "Epoch 132/200, Batch 10/17, Loss G: 4.025122165679932, Loss D: 0.006784685421735048\n",
      "Epoch 132/200, Batch 11/17, Loss G: 3.8507630825042725, Loss D: 0.004730223212391138\n",
      "Epoch 132/200, Batch 12/17, Loss G: 3.6416773796081543, Loss D: 0.053076472133398056\n",
      "Epoch 132/200, Batch 13/17, Loss G: 4.169182777404785, Loss D: 0.2017945945262909\n",
      "Epoch 132/200, Batch 14/17, Loss G: 3.89127779006958, Loss D: 0.02570592612028122\n",
      "Epoch 132/200, Batch 15/17, Loss G: 3.4761743545532227, Loss D: 0.23358756303787231\n",
      "Epoch 132/200, Batch 16/17, Loss G: 4.232842922210693, Loss D: 0.45177215337753296\n",
      "Epoch 133/200, Batch 0/17, Loss G: 4.089541435241699, Loss D: 0.37164410948753357\n",
      "Epoch 133/200, Batch 1/17, Loss G: 3.8108246326446533, Loss D: 0.3426620066165924\n",
      "Epoch 133/200, Batch 2/17, Loss G: 3.775516986846924, Loss D: 0.27206292748451233\n",
      "Epoch 133/200, Batch 3/17, Loss G: 3.660198211669922, Loss D: 0.2706826627254486\n",
      "Epoch 133/200, Batch 4/17, Loss G: 3.762010097503662, Loss D: 0.25314199924468994\n",
      "Epoch 133/200, Batch 5/17, Loss G: 3.8864331245422363, Loss D: 0.13292637467384338\n",
      "Epoch 133/200, Batch 6/17, Loss G: 3.778531074523926, Loss D: 0.03990858793258667\n",
      "Epoch 133/200, Batch 7/17, Loss G: 3.9136204719543457, Loss D: 0.07850224524736404\n",
      "Epoch 133/200, Batch 8/17, Loss G: 4.218881607055664, Loss D: 0.19356192648410797\n",
      "Epoch 133/200, Batch 9/17, Loss G: 3.221771478652954, Loss D: 0.2738427519798279\n",
      "Epoch 133/200, Batch 10/17, Loss G: 3.8343586921691895, Loss D: 0.11287672072649002\n",
      "Epoch 133/200, Batch 11/17, Loss G: 3.9478139877319336, Loss D: 0.06538545340299606\n",
      "Epoch 133/200, Batch 12/17, Loss G: 3.6500940322875977, Loss D: 0.0931016132235527\n",
      "Epoch 133/200, Batch 13/17, Loss G: 4.095732688903809, Loss D: 0.04835638031363487\n",
      "Epoch 133/200, Batch 14/17, Loss G: 3.8779563903808594, Loss D: 0.03412685543298721\n",
      "Epoch 133/200, Batch 15/17, Loss G: 3.608116626739502, Loss D: 0.06944689154624939\n",
      "Epoch 133/200, Batch 16/17, Loss G: 4.268246650695801, Loss D: 0.14480291306972504\n",
      "Epoch 134/200, Batch 0/17, Loss G: 3.3778491020202637, Loss D: 0.12695036828517914\n",
      "Epoch 134/200, Batch 1/17, Loss G: 4.14171838760376, Loss D: 0.06814214587211609\n",
      "Epoch 134/200, Batch 2/17, Loss G: 3.709440231323242, Loss D: 0.07232894748449326\n",
      "Epoch 134/200, Batch 3/17, Loss G: 4.10407018661499, Loss D: 0.059263676404953\n",
      "Epoch 134/200, Batch 4/17, Loss G: 3.8918545246124268, Loss D: 0.022569607943296432\n",
      "Epoch 134/200, Batch 5/17, Loss G: 4.004532814025879, Loss D: 0.03352905437350273\n",
      "Epoch 134/200, Batch 6/17, Loss G: 4.092156410217285, Loss D: 0.020356077700853348\n",
      "Epoch 134/200, Batch 7/17, Loss G: 4.248805046081543, Loss D: 0.04790753498673439\n",
      "Epoch 134/200, Batch 8/17, Loss G: 4.050933837890625, Loss D: 0.011479184962809086\n",
      "Epoch 134/200, Batch 9/17, Loss G: 3.8272085189819336, Loss D: 0.031229393556714058\n",
      "Epoch 134/200, Batch 10/17, Loss G: 3.904665470123291, Loss D: 0.012569218873977661\n",
      "Epoch 134/200, Batch 11/17, Loss G: 4.10087776184082, Loss D: 0.04418828710913658\n",
      "Epoch 134/200, Batch 12/17, Loss G: 3.4786617755889893, Loss D: 0.06011379882693291\n",
      "Epoch 134/200, Batch 13/17, Loss G: 4.130842685699463, Loss D: 0.024664999917149544\n",
      "Epoch 134/200, Batch 14/17, Loss G: 4.006326198577881, Loss D: 0.015497075393795967\n",
      "Epoch 134/200, Batch 15/17, Loss G: 3.8585641384124756, Loss D: 0.019413236528635025\n",
      "Epoch 134/200, Batch 16/17, Loss G: 3.8539321422576904, Loss D: 0.05447421595454216\n",
      "Epoch 135/200, Batch 0/17, Loss G: 4.137079238891602, Loss D: 0.09503880143165588\n",
      "Epoch 135/200, Batch 1/17, Loss G: 3.7087645530700684, Loss D: 0.05789399519562721\n",
      "Epoch 135/200, Batch 2/17, Loss G: 3.9352641105651855, Loss D: 0.02137940190732479\n",
      "Epoch 135/200, Batch 3/17, Loss G: 4.056614875793457, Loss D: 0.026344241574406624\n",
      "Epoch 135/200, Batch 4/17, Loss G: 3.6384761333465576, Loss D: 0.04188267141580582\n",
      "Epoch 135/200, Batch 5/17, Loss G: 3.983531951904297, Loss D: 0.032131072133779526\n",
      "Epoch 135/200, Batch 6/17, Loss G: 4.009680271148682, Loss D: 0.030990438535809517\n",
      "Epoch 135/200, Batch 7/17, Loss G: 3.9946646690368652, Loss D: 0.01799118146300316\n",
      "Epoch 135/200, Batch 8/17, Loss G: 4.052761077880859, Loss D: 0.013123084791004658\n",
      "Epoch 135/200, Batch 9/17, Loss G: 4.032280445098877, Loss D: 0.020462337881326675\n",
      "Epoch 135/200, Batch 10/17, Loss G: 4.086550712585449, Loss D: 0.034468989819288254\n",
      "Epoch 135/200, Batch 11/17, Loss G: 3.811985969543457, Loss D: 0.02160187065601349\n",
      "Epoch 135/200, Batch 12/17, Loss G: 4.03500509262085, Loss D: 0.010243795812129974\n",
      "Epoch 135/200, Batch 13/17, Loss G: 3.9821856021881104, Loss D: 0.009019749239087105\n",
      "Epoch 135/200, Batch 14/17, Loss G: 3.898759365081787, Loss D: 0.007863568142056465\n",
      "Epoch 135/200, Batch 15/17, Loss G: 4.004687309265137, Loss D: 0.007972998544573784\n",
      "Epoch 135/200, Batch 16/17, Loss G: 3.8954176902770996, Loss D: 0.0524725578725338\n",
      "Epoch 136/200, Batch 0/17, Loss G: 3.9991204738616943, Loss D: 0.008625727146863937\n",
      "Epoch 136/200, Batch 1/17, Loss G: 4.125014781951904, Loss D: 0.019443143159151077\n",
      "Epoch 136/200, Batch 2/17, Loss G: 4.197003364562988, Loss D: 0.017022661864757538\n",
      "Epoch 136/200, Batch 3/17, Loss G: 3.8959150314331055, Loss D: 0.0222514346241951\n",
      "Epoch 136/200, Batch 4/17, Loss G: 4.179240703582764, Loss D: 0.007680556271225214\n",
      "Epoch 136/200, Batch 5/17, Loss G: 3.934396743774414, Loss D: 0.020668912678956985\n",
      "Epoch 136/200, Batch 6/17, Loss G: 3.9002292156219482, Loss D: 0.018159542232751846\n",
      "Epoch 136/200, Batch 7/17, Loss G: 4.278491973876953, Loss D: 0.009968822821974754\n",
      "Epoch 136/200, Batch 8/17, Loss G: 3.8726491928100586, Loss D: 0.010351628065109253\n",
      "Epoch 136/200, Batch 9/17, Loss G: 3.857008457183838, Loss D: 0.015374522656202316\n",
      "Epoch 136/200, Batch 10/17, Loss G: 3.8327770233154297, Loss D: 0.011597685515880585\n",
      "Epoch 136/200, Batch 11/17, Loss G: 3.9276676177978516, Loss D: 0.008698681369423866\n",
      "Epoch 136/200, Batch 12/17, Loss G: 3.8447675704956055, Loss D: 0.014073085971176624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200, Batch 13/17, Loss G: 3.9818973541259766, Loss D: 0.015412733890116215\n",
      "Epoch 136/200, Batch 14/17, Loss G: 3.891047477722168, Loss D: 0.007150124292820692\n",
      "Epoch 136/200, Batch 15/17, Loss G: 3.7250099182128906, Loss D: 0.01413416676223278\n",
      "Epoch 136/200, Batch 16/17, Loss G: 3.9388394355773926, Loss D: 0.007127879653126001\n",
      "Epoch 137/200, Batch 0/17, Loss G: 3.996422052383423, Loss D: 0.004302338697016239\n",
      "Epoch 137/200, Batch 1/17, Loss G: 4.121076583862305, Loss D: 0.00950006116181612\n",
      "Epoch 137/200, Batch 2/17, Loss G: 4.012936592102051, Loss D: 0.008285446092486382\n",
      "Epoch 137/200, Batch 3/17, Loss G: 3.725770950317383, Loss D: 0.014867222867906094\n",
      "Epoch 137/200, Batch 4/17, Loss G: 3.906083583831787, Loss D: 0.011702856048941612\n",
      "Epoch 137/200, Batch 5/17, Loss G: 3.949984550476074, Loss D: 0.01671270839869976\n",
      "Epoch 137/200, Batch 6/17, Loss G: 3.8018298149108887, Loss D: 0.010630521923303604\n",
      "Epoch 137/200, Batch 7/17, Loss G: 3.8079795837402344, Loss D: 0.005777877289801836\n",
      "Epoch 137/200, Batch 8/17, Loss G: 3.7865958213806152, Loss D: 0.016975397244095802\n",
      "Epoch 137/200, Batch 9/17, Loss G: 4.007923126220703, Loss D: 0.018593931570649147\n",
      "Epoch 137/200, Batch 10/17, Loss G: 4.044150352478027, Loss D: 0.013189532794058323\n",
      "Epoch 137/200, Batch 11/17, Loss G: 4.150334358215332, Loss D: 0.004174844827502966\n",
      "Epoch 137/200, Batch 12/17, Loss G: 3.972017765045166, Loss D: 0.012614920735359192\n",
      "Epoch 137/200, Batch 13/17, Loss G: 3.9179534912109375, Loss D: 0.01197773590683937\n",
      "Epoch 137/200, Batch 14/17, Loss G: 3.8710405826568604, Loss D: 0.012163791805505753\n",
      "Epoch 137/200, Batch 15/17, Loss G: 3.9356772899627686, Loss D: 0.01162715069949627\n",
      "Epoch 137/200, Batch 16/17, Loss G: 4.2545294761657715, Loss D: 0.0280450489372015\n",
      "Epoch 138/200, Batch 0/17, Loss G: 3.8657450675964355, Loss D: 0.02042478695511818\n",
      "Epoch 138/200, Batch 1/17, Loss G: 4.074778079986572, Loss D: 0.004739288240671158\n",
      "Epoch 138/200, Batch 2/17, Loss G: 3.955702304840088, Loss D: 0.011299586854875088\n",
      "Epoch 138/200, Batch 3/17, Loss G: 3.9709534645080566, Loss D: 0.007026687264442444\n",
      "Epoch 138/200, Batch 4/17, Loss G: 3.762556552886963, Loss D: 0.01579461246728897\n",
      "Epoch 138/200, Batch 5/17, Loss G: 3.9594011306762695, Loss D: 0.005140986293554306\n",
      "Epoch 138/200, Batch 6/17, Loss G: 4.1203203201293945, Loss D: 0.004925481043756008\n",
      "Epoch 138/200, Batch 7/17, Loss G: 3.8492305278778076, Loss D: 0.020934056490659714\n",
      "Epoch 138/200, Batch 8/17, Loss G: 4.096225738525391, Loss D: 0.010116640478372574\n",
      "Epoch 138/200, Batch 9/17, Loss G: 3.8316941261291504, Loss D: 0.007662469986826181\n",
      "Epoch 138/200, Batch 10/17, Loss G: 3.9442219734191895, Loss D: 0.010047423653304577\n",
      "Epoch 138/200, Batch 11/17, Loss G: 4.026619911193848, Loss D: 0.004538490436971188\n",
      "Epoch 138/200, Batch 12/17, Loss G: 3.822601795196533, Loss D: 0.012697774916887283\n",
      "Epoch 138/200, Batch 13/17, Loss G: 4.0039963722229, Loss D: 0.03969171270728111\n",
      "Epoch 138/200, Batch 14/17, Loss G: 3.5163793563842773, Loss D: 0.08590409904718399\n",
      "Epoch 138/200, Batch 15/17, Loss G: 4.259134292602539, Loss D: 0.037488311529159546\n",
      "Epoch 138/200, Batch 16/17, Loss G: 3.996945858001709, Loss D: 0.007604418322443962\n",
      "Epoch 139/200, Batch 0/17, Loss G: 3.791951894760132, Loss D: 0.04828197509050369\n",
      "Epoch 139/200, Batch 1/17, Loss G: 4.170866012573242, Loss D: 0.10011737048625946\n",
      "Epoch 139/200, Batch 2/17, Loss G: 3.41903018951416, Loss D: 0.17067205905914307\n",
      "Epoch 139/200, Batch 3/17, Loss G: 4.039493083953857, Loss D: 0.21109133958816528\n",
      "Epoch 139/200, Batch 4/17, Loss G: 3.2908973693847656, Loss D: 0.20830215513706207\n",
      "Epoch 139/200, Batch 5/17, Loss G: 4.045033931732178, Loss D: 0.041024815291166306\n",
      "Epoch 139/200, Batch 6/17, Loss G: 4.110245704650879, Loss D: 0.0618363693356514\n",
      "Epoch 139/200, Batch 7/17, Loss G: 3.247462272644043, Loss D: 0.2774703800678253\n",
      "Epoch 139/200, Batch 8/17, Loss G: 3.872877597808838, Loss D: 0.07730074226856232\n",
      "Epoch 139/200, Batch 9/17, Loss G: 3.583045244216919, Loss D: 0.07692819833755493\n",
      "Epoch 139/200, Batch 10/17, Loss G: 3.6803908348083496, Loss D: 0.1480473130941391\n",
      "Epoch 139/200, Batch 11/17, Loss G: 4.019207954406738, Loss D: 0.056000955402851105\n",
      "Epoch 139/200, Batch 12/17, Loss G: 3.6502251625061035, Loss D: 0.08608077466487885\n",
      "Epoch 139/200, Batch 13/17, Loss G: 3.901793956756592, Loss D: 0.049141086637973785\n",
      "Epoch 139/200, Batch 14/17, Loss G: 3.7818970680236816, Loss D: 0.028237218037247658\n",
      "Epoch 139/200, Batch 15/17, Loss G: 3.7467267513275146, Loss D: 0.04767896607518196\n",
      "Epoch 139/200, Batch 16/17, Loss G: 4.154971122741699, Loss D: 0.0891907662153244\n",
      "Epoch 140/200, Batch 0/17, Loss G: 3.655447006225586, Loss D: 0.06970424205064774\n",
      "Epoch 140/200, Batch 1/17, Loss G: 3.9160618782043457, Loss D: 0.02323186956346035\n",
      "Epoch 140/200, Batch 2/17, Loss G: 4.000493049621582, Loss D: 0.08275944739580154\n",
      "Epoch 140/200, Batch 3/17, Loss G: 3.6839542388916016, Loss D: 0.11113573610782623\n",
      "Epoch 140/200, Batch 4/17, Loss G: 4.04219388961792, Loss D: 0.04200904071331024\n",
      "Epoch 140/200, Batch 5/17, Loss G: 4.102783679962158, Loss D: 0.020892618224024773\n",
      "Epoch 140/200, Batch 6/17, Loss G: 3.7929184436798096, Loss D: 0.0550411120057106\n",
      "Epoch 140/200, Batch 7/17, Loss G: 3.986539840698242, Loss D: 0.05688570439815521\n",
      "Epoch 140/200, Batch 8/17, Loss G: 3.918482780456543, Loss D: 0.02441897988319397\n",
      "Epoch 140/200, Batch 9/17, Loss G: 3.8549561500549316, Loss D: 0.017325399443507195\n",
      "Epoch 140/200, Batch 10/17, Loss G: 3.8004822731018066, Loss D: 0.020151417702436447\n",
      "Epoch 140/200, Batch 11/17, Loss G: 3.9357380867004395, Loss D: 0.03580339252948761\n",
      "Epoch 140/200, Batch 12/17, Loss G: 3.7982163429260254, Loss D: 0.015662595629692078\n",
      "Epoch 140/200, Batch 13/17, Loss G: 3.809299945831299, Loss D: 0.03281363844871521\n",
      "Epoch 140/200, Batch 14/17, Loss G: 3.920433521270752, Loss D: 0.049720264971256256\n",
      "Epoch 140/200, Batch 15/17, Loss G: 3.8212757110595703, Loss D: 0.022134236991405487\n",
      "Epoch 140/200, Batch 16/17, Loss G: 3.9936318397521973, Loss D: 0.011744752526283264\n",
      "Epoch 141/200, Batch 0/17, Loss G: 3.950839042663574, Loss D: 0.011446894146502018\n",
      "Epoch 141/200, Batch 1/17, Loss G: 3.8344902992248535, Loss D: 0.01930459961295128\n",
      "Epoch 141/200, Batch 2/17, Loss G: 3.961451530456543, Loss D: 0.008385876193642616\n",
      "Epoch 141/200, Batch 3/17, Loss G: 4.042001247406006, Loss D: 0.00837736763060093\n",
      "Epoch 141/200, Batch 4/17, Loss G: 3.8915228843688965, Loss D: 0.012427314184606075\n",
      "Epoch 141/200, Batch 5/17, Loss G: 3.898664712905884, Loss D: 0.008912669494748116\n",
      "Epoch 141/200, Batch 6/17, Loss G: 3.935894012451172, Loss D: 0.008570301346480846\n",
      "Epoch 141/200, Batch 7/17, Loss G: 3.9769585132598877, Loss D: 0.012961062602698803\n",
      "Epoch 141/200, Batch 8/17, Loss G: 4.12779426574707, Loss D: 0.006896779872477055\n",
      "Epoch 141/200, Batch 9/17, Loss G: 4.171905517578125, Loss D: 0.006159353069961071\n",
      "Epoch 141/200, Batch 10/17, Loss G: 3.936798572540283, Loss D: 0.014413104392588139\n",
      "Epoch 141/200, Batch 11/17, Loss G: 3.8050472736358643, Loss D: 0.011694281361997128\n",
      "Epoch 141/200, Batch 12/17, Loss G: 3.7960996627807617, Loss D: 0.01663787290453911\n",
      "Epoch 141/200, Batch 13/17, Loss G: 4.057499885559082, Loss D: 0.00405097333714366\n",
      "Epoch 141/200, Batch 14/17, Loss G: 4.073837757110596, Loss D: 0.016162745654582977\n",
      "Epoch 141/200, Batch 15/17, Loss G: 3.908029556274414, Loss D: 0.004907437600195408\n",
      "Epoch 141/200, Batch 16/17, Loss G: 4.0069990158081055, Loss D: 0.013908394612371922\n",
      "Epoch 142/200, Batch 0/17, Loss G: 4.048033237457275, Loss D: 0.012698870152235031\n",
      "Epoch 142/200, Batch 1/17, Loss G: 4.011634349822998, Loss D: 0.004889545496553183\n",
      "Epoch 142/200, Batch 2/17, Loss G: 3.9889254570007324, Loss D: 0.009770186617970467\n",
      "Epoch 142/200, Batch 3/17, Loss G: 3.9931230545043945, Loss D: 0.007996969856321812\n",
      "Epoch 142/200, Batch 4/17, Loss G: 4.002220630645752, Loss D: 0.005946577992290258\n",
      "Epoch 142/200, Batch 5/17, Loss G: 3.775261878967285, Loss D: 0.009907042607665062\n",
      "Epoch 142/200, Batch 6/17, Loss G: 3.961284875869751, Loss D: 0.0031022087205201387\n",
      "Epoch 142/200, Batch 7/17, Loss G: 3.940425395965576, Loss D: 0.004609595984220505\n",
      "Epoch 142/200, Batch 8/17, Loss G: 3.8935439586639404, Loss D: 0.0053062038496136665\n",
      "Epoch 142/200, Batch 9/17, Loss G: 3.816728115081787, Loss D: 0.01707530952990055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200, Batch 10/17, Loss G: 3.9806721210479736, Loss D: 0.011444265954196453\n",
      "Epoch 142/200, Batch 11/17, Loss G: 4.040525913238525, Loss D: 0.003546102438122034\n",
      "Epoch 142/200, Batch 12/17, Loss G: 3.7520337104797363, Loss D: 0.023043595254421234\n",
      "Epoch 142/200, Batch 13/17, Loss G: 4.1343994140625, Loss D: 0.037029944360256195\n",
      "Epoch 142/200, Batch 14/17, Loss G: 3.9709386825561523, Loss D: 0.016432570293545723\n",
      "Epoch 142/200, Batch 15/17, Loss G: 3.9079651832580566, Loss D: 0.015222764573991299\n",
      "Epoch 142/200, Batch 16/17, Loss G: 4.137477874755859, Loss D: 0.008617645129561424\n",
      "Epoch 143/200, Batch 0/17, Loss G: 4.035084247589111, Loss D: 0.004070959519594908\n",
      "Epoch 143/200, Batch 1/17, Loss G: 3.909468173980713, Loss D: 0.007355013862252235\n",
      "Epoch 143/200, Batch 2/17, Loss G: 3.9062368869781494, Loss D: 0.0062995608896017075\n",
      "Epoch 143/200, Batch 3/17, Loss G: 3.980530261993408, Loss D: 0.004384906962513924\n",
      "Epoch 143/200, Batch 4/17, Loss G: 3.8194193840026855, Loss D: 0.024036871269345284\n",
      "Epoch 143/200, Batch 5/17, Loss G: 3.967055320739746, Loss D: 0.012141005136072636\n",
      "Epoch 143/200, Batch 6/17, Loss G: 3.914175510406494, Loss D: 0.012908875942230225\n",
      "Epoch 143/200, Batch 7/17, Loss G: 3.6598784923553467, Loss D: 0.05503073334693909\n",
      "Epoch 143/200, Batch 8/17, Loss G: 4.221630096435547, Loss D: 0.058045171201229095\n",
      "Epoch 143/200, Batch 9/17, Loss G: 3.939959764480591, Loss D: 0.0016662979032844305\n",
      "Epoch 143/200, Batch 10/17, Loss G: 3.4897866249084473, Loss D: 0.07734191417694092\n",
      "Epoch 143/200, Batch 11/17, Loss G: 4.0252509117126465, Loss D: 0.04060472920536995\n",
      "Epoch 143/200, Batch 12/17, Loss G: 4.104060649871826, Loss D: 0.017993908375501633\n",
      "Epoch 143/200, Batch 13/17, Loss G: 3.884474754333496, Loss D: 0.0013121580705046654\n",
      "Epoch 143/200, Batch 14/17, Loss G: 3.832594156265259, Loss D: 0.024148495867848396\n",
      "Epoch 143/200, Batch 15/17, Loss G: 3.758662462234497, Loss D: 0.027566589415073395\n",
      "Epoch 143/200, Batch 16/17, Loss G: 4.202993869781494, Loss D: 0.02912716194987297\n",
      "Epoch 144/200, Batch 0/17, Loss G: 4.131220817565918, Loss D: 0.011362995952367783\n",
      "Epoch 144/200, Batch 1/17, Loss G: 3.803614854812622, Loss D: 0.012717132456600666\n",
      "Epoch 144/200, Batch 2/17, Loss G: 3.905794143676758, Loss D: 0.021739475429058075\n",
      "Epoch 144/200, Batch 3/17, Loss G: 3.930696487426758, Loss D: 0.029698144644498825\n",
      "Epoch 144/200, Batch 4/17, Loss G: 3.7995617389678955, Loss D: 0.015520778484642506\n",
      "Epoch 144/200, Batch 5/17, Loss G: 3.9443533420562744, Loss D: 0.006692999042570591\n",
      "Epoch 144/200, Batch 6/17, Loss G: 4.037837028503418, Loss D: 0.02389119192957878\n",
      "Epoch 144/200, Batch 7/17, Loss G: 3.72676944732666, Loss D: 0.014378510415554047\n",
      "Epoch 144/200, Batch 8/17, Loss G: 3.871793270111084, Loss D: 0.017772438004612923\n",
      "Epoch 144/200, Batch 9/17, Loss G: 3.916051149368286, Loss D: 0.020855069160461426\n",
      "Epoch 144/200, Batch 10/17, Loss G: 3.921318531036377, Loss D: 0.007123841904103756\n",
      "Epoch 144/200, Batch 11/17, Loss G: 3.798044204711914, Loss D: 0.010430477559566498\n",
      "Epoch 144/200, Batch 12/17, Loss G: 4.022677898406982, Loss D: 0.0070897969417274\n",
      "Epoch 144/200, Batch 13/17, Loss G: 3.96107816696167, Loss D: 0.007474502548575401\n",
      "Epoch 144/200, Batch 14/17, Loss G: 3.811528444290161, Loss D: 0.006915769074112177\n",
      "Epoch 144/200, Batch 15/17, Loss G: 3.7524757385253906, Loss D: 0.011039767414331436\n",
      "Epoch 144/200, Batch 16/17, Loss G: 4.122676372528076, Loss D: 0.010881107300519943\n",
      "Epoch 145/200, Batch 0/17, Loss G: 4.000852584838867, Loss D: 0.00821934174746275\n",
      "Epoch 145/200, Batch 1/17, Loss G: 3.916525363922119, Loss D: 0.003651819657534361\n",
      "Epoch 145/200, Batch 2/17, Loss G: 3.931858539581299, Loss D: 0.006620748899877071\n",
      "Epoch 145/200, Batch 3/17, Loss G: 3.9205660820007324, Loss D: 0.0050316317938268185\n",
      "Epoch 145/200, Batch 4/17, Loss G: 3.939511775970459, Loss D: 0.003858580021187663\n",
      "Epoch 145/200, Batch 5/17, Loss G: 3.967958688735962, Loss D: 0.005719644017517567\n",
      "Epoch 145/200, Batch 6/17, Loss G: 3.8365936279296875, Loss D: 0.0065872385166585445\n",
      "Epoch 145/200, Batch 7/17, Loss G: 4.005789756774902, Loss D: 0.0039043142460286617\n",
      "Epoch 145/200, Batch 8/17, Loss G: 3.7864997386932373, Loss D: 0.01690290868282318\n",
      "Epoch 145/200, Batch 9/17, Loss G: 3.9827075004577637, Loss D: 0.023341942578554153\n",
      "Epoch 145/200, Batch 10/17, Loss G: 3.6820993423461914, Loss D: 0.017682110890746117\n",
      "Epoch 145/200, Batch 11/17, Loss G: 3.990121841430664, Loss D: 0.01047760434448719\n",
      "Epoch 145/200, Batch 12/17, Loss G: 3.9950947761535645, Loss D: 0.00989051815122366\n",
      "Epoch 145/200, Batch 13/17, Loss G: 3.888216972351074, Loss D: 0.007161003537476063\n",
      "Epoch 145/200, Batch 14/17, Loss G: 3.8371167182922363, Loss D: 0.004507116507738829\n",
      "Epoch 145/200, Batch 15/17, Loss G: 3.7561535835266113, Loss D: 0.016972245648503304\n",
      "Epoch 145/200, Batch 16/17, Loss G: 4.030713081359863, Loss D: 0.0319860614836216\n",
      "Epoch 146/200, Batch 0/17, Loss G: 3.6237263679504395, Loss D: 0.032593052834272385\n",
      "Epoch 146/200, Batch 1/17, Loss G: 4.148249626159668, Loss D: 0.008207847364246845\n",
      "Epoch 146/200, Batch 2/17, Loss G: 4.127946376800537, Loss D: 0.007506310008466244\n",
      "Epoch 146/200, Batch 3/17, Loss G: 3.999354362487793, Loss D: 0.0043052854016423225\n",
      "Epoch 146/200, Batch 4/17, Loss G: 3.819453716278076, Loss D: 0.0114040058106184\n",
      "Epoch 146/200, Batch 5/17, Loss G: 4.108236312866211, Loss D: 0.006295479834079742\n",
      "Epoch 146/200, Batch 6/17, Loss G: 3.870626449584961, Loss D: 0.0091462517157197\n",
      "Epoch 146/200, Batch 7/17, Loss G: 3.922128200531006, Loss D: 0.0073191942647099495\n",
      "Epoch 146/200, Batch 8/17, Loss G: 3.9938769340515137, Loss D: 0.006009052973240614\n",
      "Epoch 146/200, Batch 9/17, Loss G: 3.8680169582366943, Loss D: 0.008972516283392906\n",
      "Epoch 146/200, Batch 10/17, Loss G: 3.9948935508728027, Loss D: 0.002739054150879383\n",
      "Epoch 146/200, Batch 11/17, Loss G: 3.941387176513672, Loss D: 0.007225221022963524\n",
      "Epoch 146/200, Batch 12/17, Loss G: 3.929854393005371, Loss D: 0.0038106634747236967\n",
      "Epoch 146/200, Batch 13/17, Loss G: 3.907982349395752, Loss D: 0.00352036883123219\n",
      "Epoch 146/200, Batch 14/17, Loss G: 3.8767783641815186, Loss D: 0.0035564908757805824\n",
      "Epoch 146/200, Batch 15/17, Loss G: 3.9082260131835938, Loss D: 0.002889803145080805\n",
      "Epoch 146/200, Batch 16/17, Loss G: 3.833437919616699, Loss D: 0.010045095346868038\n",
      "Epoch 147/200, Batch 0/17, Loss G: 3.7804431915283203, Loss D: 0.01195189356803894\n",
      "Epoch 147/200, Batch 1/17, Loss G: 3.863577127456665, Loss D: 0.011742859147489071\n",
      "Epoch 147/200, Batch 2/17, Loss G: 3.964470148086548, Loss D: 0.013010971248149872\n",
      "Epoch 147/200, Batch 3/17, Loss G: 3.865966320037842, Loss D: 0.0025979101192206144\n",
      "Epoch 147/200, Batch 4/17, Loss G: 3.858900547027588, Loss D: 0.006835319567471743\n",
      "Epoch 147/200, Batch 5/17, Loss G: 3.875047206878662, Loss D: 0.008346237242221832\n",
      "Epoch 147/200, Batch 6/17, Loss G: 4.001843452453613, Loss D: 0.004001626744866371\n",
      "Epoch 147/200, Batch 7/17, Loss G: 3.9856088161468506, Loss D: 0.009226727299392223\n",
      "Epoch 147/200, Batch 8/17, Loss G: 4.010157108306885, Loss D: 0.005644176155328751\n",
      "Epoch 147/200, Batch 9/17, Loss G: 3.839775800704956, Loss D: 0.0036071413196623325\n",
      "Epoch 147/200, Batch 10/17, Loss G: 3.971780300140381, Loss D: 0.002103960607200861\n",
      "Epoch 147/200, Batch 11/17, Loss G: 4.083884239196777, Loss D: 0.004473825450986624\n",
      "Epoch 147/200, Batch 12/17, Loss G: 3.778470039367676, Loss D: 0.005018696654587984\n",
      "Epoch 147/200, Batch 13/17, Loss G: 3.810222864151001, Loss D: 0.0038316308055073023\n",
      "Epoch 147/200, Batch 14/17, Loss G: 4.007283687591553, Loss D: 0.0028533735312521458\n",
      "Epoch 147/200, Batch 15/17, Loss G: 3.959402322769165, Loss D: 0.0031985400710254908\n",
      "Epoch 147/200, Batch 16/17, Loss G: 3.742846965789795, Loss D: 0.0041157836094498634\n",
      "Epoch 148/200, Batch 0/17, Loss G: 3.960879325866699, Loss D: 0.006422238424420357\n",
      "Epoch 148/200, Batch 1/17, Loss G: 3.9664978981018066, Loss D: 0.004314744379371405\n",
      "Epoch 148/200, Batch 2/17, Loss G: 3.7487387657165527, Loss D: 0.009369592182338238\n",
      "Epoch 148/200, Batch 3/17, Loss G: 4.000593185424805, Loss D: 0.004966943059116602\n",
      "Epoch 148/200, Batch 4/17, Loss G: 3.850581169128418, Loss D: 0.009149087592959404\n",
      "Epoch 148/200, Batch 5/17, Loss G: 3.928091287612915, Loss D: 0.002264568116515875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200, Batch 6/17, Loss G: 3.797302484512329, Loss D: 0.010146229527890682\n",
      "Epoch 148/200, Batch 7/17, Loss G: 4.084156036376953, Loss D: 0.005190529860556126\n",
      "Epoch 148/200, Batch 8/17, Loss G: 4.089317321777344, Loss D: 0.004865181166678667\n",
      "Epoch 148/200, Batch 9/17, Loss G: 3.817957878112793, Loss D: 0.027082029730081558\n",
      "Epoch 148/200, Batch 10/17, Loss G: 3.3934741020202637, Loss D: 0.11503677815198898\n",
      "Epoch 148/200, Batch 11/17, Loss G: 3.986171245574951, Loss D: 0.1439359039068222\n",
      "Epoch 148/200, Batch 12/17, Loss G: 3.235891580581665, Loss D: 0.1153671070933342\n",
      "Epoch 148/200, Batch 13/17, Loss G: 3.906569004058838, Loss D: 0.07783026993274689\n",
      "Epoch 148/200, Batch 14/17, Loss G: 3.408782958984375, Loss D: 0.19919312000274658\n",
      "Epoch 148/200, Batch 15/17, Loss G: 3.9664306640625, Loss D: 0.11898935586214066\n",
      "Epoch 148/200, Batch 16/17, Loss G: 3.7717504501342773, Loss D: 0.08794605731964111\n",
      "Epoch 149/200, Batch 0/17, Loss G: 4.054406642913818, Loss D: 0.13225972652435303\n",
      "Epoch 149/200, Batch 1/17, Loss G: 3.7645583152770996, Loss D: 0.041134633123874664\n",
      "Epoch 149/200, Batch 2/17, Loss G: 3.485173225402832, Loss D: 0.10379370301961899\n",
      "Epoch 149/200, Batch 3/17, Loss G: 4.096573829650879, Loss D: 0.16168217360973358\n",
      "Epoch 149/200, Batch 4/17, Loss G: 3.620968818664551, Loss D: 0.10187603533267975\n",
      "Epoch 149/200, Batch 5/17, Loss G: 3.8806557655334473, Loss D: 0.04275579750537872\n",
      "Epoch 149/200, Batch 6/17, Loss G: 3.709681510925293, Loss D: 0.024379948154091835\n",
      "Epoch 149/200, Batch 7/17, Loss G: 3.956087112426758, Loss D: 0.024169135838747025\n",
      "Epoch 149/200, Batch 8/17, Loss G: 3.8066086769104004, Loss D: 0.015445812605321407\n",
      "Epoch 149/200, Batch 9/17, Loss G: 3.780219554901123, Loss D: 0.035104699432849884\n",
      "Epoch 149/200, Batch 10/17, Loss G: 3.976994752883911, Loss D: 0.04779181256890297\n",
      "Epoch 149/200, Batch 11/17, Loss G: 3.716987133026123, Loss D: 0.03684525191783905\n",
      "Epoch 149/200, Batch 12/17, Loss G: 3.9799952507019043, Loss D: 0.017104696482419968\n",
      "Epoch 149/200, Batch 13/17, Loss G: 3.8817341327667236, Loss D: 0.01938972808420658\n",
      "Epoch 149/200, Batch 14/17, Loss G: 4.082762718200684, Loss D: 0.029649518430233\n",
      "Epoch 149/200, Batch 15/17, Loss G: 3.497591495513916, Loss D: 0.054862815886735916\n",
      "Epoch 149/200, Batch 16/17, Loss G: 4.011101722717285, Loss D: 0.07198281586170197\n",
      "Epoch 150/200, Batch 0/17, Loss G: 3.570169687271118, Loss D: 0.0447302982211113\n",
      "Epoch 150/200, Batch 1/17, Loss G: 3.8950650691986084, Loss D: 0.014940040186047554\n",
      "Epoch 150/200, Batch 2/17, Loss G: 3.884221076965332, Loss D: 0.015293190255761147\n",
      "Epoch 150/200, Batch 3/17, Loss G: 4.004459857940674, Loss D: 0.018391557037830353\n",
      "Epoch 150/200, Batch 4/17, Loss G: 3.7792928218841553, Loss D: 0.012755559757351875\n",
      "Epoch 150/200, Batch 5/17, Loss G: 3.8385446071624756, Loss D: 0.01282290555536747\n",
      "Epoch 150/200, Batch 6/17, Loss G: 3.7926461696624756, Loss D: 0.013503829017281532\n",
      "Epoch 150/200, Batch 7/17, Loss G: 3.9356203079223633, Loss D: 0.013856526464223862\n",
      "Epoch 150/200, Batch 8/17, Loss G: 3.799713134765625, Loss D: 0.02088434062898159\n",
      "Epoch 150/200, Batch 9/17, Loss G: 4.077380180358887, Loss D: 0.00946975126862526\n",
      "Epoch 150/200, Batch 10/17, Loss G: 3.9910497665405273, Loss D: 0.01750068925321102\n",
      "Epoch 150/200, Batch 11/17, Loss G: 3.729966640472412, Loss D: 0.022897999733686447\n",
      "Epoch 150/200, Batch 12/17, Loss G: 3.994457721710205, Loss D: 0.010452859103679657\n",
      "Epoch 150/200, Batch 13/17, Loss G: 3.9065895080566406, Loss D: 0.007544397842139006\n",
      "Epoch 150/200, Batch 14/17, Loss G: 3.836432933807373, Loss D: 0.014816101640462875\n",
      "Epoch 150/200, Batch 15/17, Loss G: 3.891345262527466, Loss D: 0.00881221704185009\n",
      "Epoch 150/200, Batch 16/17, Loss G: 3.9780149459838867, Loss D: 0.006505021825432777\n",
      "Epoch 151/200, Batch 0/17, Loss G: 3.9600672721862793, Loss D: 0.007750699296593666\n",
      "Epoch 151/200, Batch 1/17, Loss G: 3.878114700317383, Loss D: 0.011076556518673897\n",
      "Epoch 151/200, Batch 2/17, Loss G: 3.896965980529785, Loss D: 0.004622506909072399\n",
      "Epoch 151/200, Batch 3/17, Loss G: 3.900294065475464, Loss D: 0.009428245015442371\n",
      "Epoch 151/200, Batch 4/17, Loss G: 3.840045928955078, Loss D: 0.01680395007133484\n",
      "Epoch 151/200, Batch 5/17, Loss G: 3.8050594329833984, Loss D: 0.019561372697353363\n",
      "Epoch 151/200, Batch 6/17, Loss G: 3.866231918334961, Loss D: 0.005833653267472982\n",
      "Epoch 151/200, Batch 7/17, Loss G: 3.966120719909668, Loss D: 0.02576124295592308\n",
      "Epoch 151/200, Batch 8/17, Loss G: 3.992672920227051, Loss D: 0.012356633320450783\n",
      "Epoch 151/200, Batch 9/17, Loss G: 3.812730550765991, Loss D: 0.028855040669441223\n",
      "Epoch 151/200, Batch 10/17, Loss G: 3.5301146507263184, Loss D: 0.04746813327074051\n",
      "Epoch 151/200, Batch 11/17, Loss G: 4.1298723220825195, Loss D: 0.004951993469148874\n",
      "Epoch 151/200, Batch 12/17, Loss G: 3.95271372795105, Loss D: 0.023270202800631523\n",
      "Epoch 151/200, Batch 13/17, Loss G: 3.8885440826416016, Loss D: 0.012280440889298916\n",
      "Epoch 151/200, Batch 14/17, Loss G: 3.832174062728882, Loss D: 0.020267542451620102\n",
      "Epoch 151/200, Batch 15/17, Loss G: 3.877408504486084, Loss D: 0.014774505980312824\n",
      "Epoch 151/200, Batch 16/17, Loss G: 3.828594207763672, Loss D: 0.0017664933111518621\n",
      "Epoch 152/200, Batch 0/17, Loss G: 3.8653976917266846, Loss D: 0.008362127467989922\n",
      "Epoch 152/200, Batch 1/17, Loss G: 3.9991650581359863, Loss D: 0.007261405698955059\n",
      "Epoch 152/200, Batch 2/17, Loss G: 4.013367176055908, Loss D: 0.010632411576807499\n",
      "Epoch 152/200, Batch 3/17, Loss G: 3.70535945892334, Loss D: 0.01083577424287796\n",
      "Epoch 152/200, Batch 4/17, Loss G: 3.8106329441070557, Loss D: 0.009686000645160675\n",
      "Epoch 152/200, Batch 5/17, Loss G: 3.8893585205078125, Loss D: 0.005608255043625832\n",
      "Epoch 152/200, Batch 6/17, Loss G: 3.932394027709961, Loss D: 0.0076703839004039764\n",
      "Epoch 152/200, Batch 7/17, Loss G: 3.8235251903533936, Loss D: 0.026108894497156143\n",
      "Epoch 152/200, Batch 8/17, Loss G: 4.026690483093262, Loss D: 0.03676137700676918\n",
      "Epoch 152/200, Batch 9/17, Loss G: 3.745565891265869, Loss D: 0.011765925213694572\n",
      "Epoch 152/200, Batch 10/17, Loss G: 3.765126943588257, Loss D: 0.018768999725580215\n",
      "Epoch 152/200, Batch 11/17, Loss G: 3.954943895339966, Loss D: 0.019839003682136536\n",
      "Epoch 152/200, Batch 12/17, Loss G: 3.8058724403381348, Loss D: 0.01161511056125164\n",
      "Epoch 152/200, Batch 13/17, Loss G: 3.718944549560547, Loss D: 0.009445956908166409\n",
      "Epoch 152/200, Batch 14/17, Loss G: 4.044806957244873, Loss D: 0.002716025337576866\n",
      "Epoch 152/200, Batch 15/17, Loss G: 3.901015281677246, Loss D: 0.008213388733565807\n",
      "Epoch 152/200, Batch 16/17, Loss G: 3.846862316131592, Loss D: 0.008909557946026325\n",
      "Epoch 153/200, Batch 0/17, Loss G: 3.9545035362243652, Loss D: 0.004774430766701698\n",
      "Epoch 153/200, Batch 1/17, Loss G: 4.132334232330322, Loss D: 0.02048545889556408\n",
      "Epoch 153/200, Batch 2/17, Loss G: 4.081955909729004, Loss D: 0.03748690336942673\n",
      "Epoch 153/200, Batch 3/17, Loss G: 3.7484607696533203, Loss D: 0.00939666386693716\n",
      "Epoch 153/200, Batch 4/17, Loss G: 3.5327320098876953, Loss D: 0.022020677104592323\n",
      "Epoch 153/200, Batch 5/17, Loss G: 3.9423811435699463, Loss D: 0.004752218257635832\n",
      "Epoch 153/200, Batch 6/17, Loss G: 3.8939638137817383, Loss D: 0.04555267095565796\n",
      "Epoch 153/200, Batch 7/17, Loss G: 3.4740872383117676, Loss D: 0.04705960676074028\n",
      "Epoch 153/200, Batch 8/17, Loss G: 3.820385694503784, Loss D: 0.007028159685432911\n",
      "Epoch 153/200, Batch 9/17, Loss G: 3.9151716232299805, Loss D: 0.010735268704593182\n",
      "Epoch 153/200, Batch 10/17, Loss G: 3.7792248725891113, Loss D: 0.0133481714874506\n",
      "Epoch 153/200, Batch 11/17, Loss G: 3.7910242080688477, Loss D: 0.004281155299395323\n",
      "Epoch 153/200, Batch 12/17, Loss G: 3.809177875518799, Loss D: 0.011206992901861668\n",
      "Epoch 153/200, Batch 13/17, Loss G: 4.031628131866455, Loss D: 0.005753896199166775\n",
      "Epoch 153/200, Batch 14/17, Loss G: 3.89328932762146, Loss D: 0.029376480728387833\n",
      "Epoch 153/200, Batch 15/17, Loss G: 4.069845676422119, Loss D: 0.004948211833834648\n",
      "Epoch 153/200, Batch 16/17, Loss G: 4.045875549316406, Loss D: 0.011348873376846313\n",
      "Epoch 154/200, Batch 0/17, Loss G: 3.8187036514282227, Loss D: 0.011361061595380306\n",
      "Epoch 154/200, Batch 1/17, Loss G: 3.852595806121826, Loss D: 0.010720258578658104\n",
      "Epoch 154/200, Batch 2/17, Loss G: 3.9312832355499268, Loss D: 0.008968446403741837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200, Batch 3/17, Loss G: 4.013192653656006, Loss D: 0.005028487183153629\n",
      "Epoch 154/200, Batch 4/17, Loss G: 3.978433132171631, Loss D: 0.0027128939982503653\n",
      "Epoch 154/200, Batch 5/17, Loss G: 3.9767684936523438, Loss D: 0.005226841662079096\n",
      "Epoch 154/200, Batch 6/17, Loss G: 3.8005640506744385, Loss D: 0.004811410326510668\n",
      "Epoch 154/200, Batch 7/17, Loss G: 3.953294277191162, Loss D: 0.002935202559456229\n",
      "Epoch 154/200, Batch 8/17, Loss G: 3.7018065452575684, Loss D: 0.0037048235535621643\n",
      "Epoch 154/200, Batch 9/17, Loss G: 3.8475046157836914, Loss D: 0.0030365141574293375\n",
      "Epoch 154/200, Batch 10/17, Loss G: 3.781386137008667, Loss D: 0.004613938275724649\n",
      "Epoch 154/200, Batch 11/17, Loss G: 3.734419107437134, Loss D: 0.005317450501024723\n",
      "Epoch 154/200, Batch 12/17, Loss G: 3.916104793548584, Loss D: 0.014047758653759956\n",
      "Epoch 154/200, Batch 13/17, Loss G: 3.843743324279785, Loss D: 0.015249448828399181\n",
      "Epoch 154/200, Batch 14/17, Loss G: 4.003101348876953, Loss D: 0.005601201206445694\n",
      "Epoch 154/200, Batch 15/17, Loss G: 3.9949746131896973, Loss D: 0.008571038022637367\n",
      "Epoch 154/200, Batch 16/17, Loss G: 3.885329008102417, Loss D: 0.012095094658434391\n",
      "Epoch 155/200, Batch 0/17, Loss G: 3.6977598667144775, Loss D: 0.02922949008643627\n",
      "Epoch 155/200, Batch 1/17, Loss G: 3.9438376426696777, Loss D: 0.01371309906244278\n",
      "Epoch 155/200, Batch 2/17, Loss G: 3.7140042781829834, Loss D: 0.006527315825223923\n",
      "Epoch 155/200, Batch 3/17, Loss G: 3.827829599380493, Loss D: 0.009898899123072624\n",
      "Epoch 155/200, Batch 4/17, Loss G: 3.917543649673462, Loss D: 0.005183476489037275\n",
      "Epoch 155/200, Batch 5/17, Loss G: 3.917442798614502, Loss D: 0.010463254526257515\n",
      "Epoch 155/200, Batch 6/17, Loss G: 3.7791175842285156, Loss D: 0.008031751029193401\n",
      "Epoch 155/200, Batch 7/17, Loss G: 3.933178663253784, Loss D: 0.007267845794558525\n",
      "Epoch 155/200, Batch 8/17, Loss G: 3.867422103881836, Loss D: 0.004085452761501074\n",
      "Epoch 155/200, Batch 9/17, Loss G: 3.7206101417541504, Loss D: 0.009103484451770782\n",
      "Epoch 155/200, Batch 10/17, Loss G: 3.907165050506592, Loss D: 0.006201659329235554\n",
      "Epoch 155/200, Batch 11/17, Loss G: 4.067107677459717, Loss D: 0.031248226761817932\n",
      "Epoch 155/200, Batch 12/17, Loss G: 3.6909356117248535, Loss D: 0.017279574647545815\n",
      "Epoch 155/200, Batch 13/17, Loss G: 3.809175729751587, Loss D: 0.005010570399463177\n",
      "Epoch 155/200, Batch 14/17, Loss G: 3.9190621376037598, Loss D: 0.005785520654171705\n",
      "Epoch 155/200, Batch 15/17, Loss G: 3.923057794570923, Loss D: 0.006467811297625303\n",
      "Epoch 155/200, Batch 16/17, Loss G: 3.9109203815460205, Loss D: 0.0027015709783881903\n",
      "Epoch 156/200, Batch 0/17, Loss G: 3.7529938220977783, Loss D: 0.007092545740306377\n",
      "Epoch 156/200, Batch 1/17, Loss G: 3.860825300216675, Loss D: 0.008444222621619701\n",
      "Epoch 156/200, Batch 2/17, Loss G: 3.840305805206299, Loss D: 0.010028041899204254\n",
      "Epoch 156/200, Batch 3/17, Loss G: 3.8423349857330322, Loss D: 0.00503155030310154\n",
      "Epoch 156/200, Batch 4/17, Loss G: 3.85673189163208, Loss D: 0.0030946568585932255\n",
      "Epoch 156/200, Batch 5/17, Loss G: 3.8796935081481934, Loss D: 0.00409423653036356\n",
      "Epoch 156/200, Batch 6/17, Loss G: 3.762031078338623, Loss D: 0.006209319457411766\n",
      "Epoch 156/200, Batch 7/17, Loss G: 3.911848545074463, Loss D: 0.0038261977024376392\n",
      "Epoch 156/200, Batch 8/17, Loss G: 3.8181843757629395, Loss D: 0.006944349966943264\n",
      "Epoch 156/200, Batch 9/17, Loss G: 3.7941994667053223, Loss D: 0.004632174968719482\n",
      "Epoch 156/200, Batch 10/17, Loss G: 3.9666624069213867, Loss D: 0.007291840389370918\n",
      "Epoch 156/200, Batch 11/17, Loss G: 3.9839773178100586, Loss D: 0.004360494203865528\n",
      "Epoch 156/200, Batch 12/17, Loss G: 3.8786895275115967, Loss D: 0.0033268537372350693\n",
      "Epoch 156/200, Batch 13/17, Loss G: 3.859998941421509, Loss D: 0.008786585181951523\n",
      "Epoch 156/200, Batch 14/17, Loss G: 3.6929264068603516, Loss D: 0.001104740658774972\n",
      "Epoch 156/200, Batch 15/17, Loss G: 3.7508163452148438, Loss D: 0.016541700810194016\n",
      "Epoch 156/200, Batch 16/17, Loss G: 4.040952682495117, Loss D: 0.007370134349912405\n",
      "Epoch 157/200, Batch 0/17, Loss G: 4.034361839294434, Loss D: 0.004803517367690802\n",
      "Epoch 157/200, Batch 1/17, Loss G: 3.9392831325531006, Loss D: 0.007432985119521618\n",
      "Epoch 157/200, Batch 2/17, Loss G: 3.9728987216949463, Loss D: 0.002460653427988291\n",
      "Epoch 157/200, Batch 3/17, Loss G: 3.8598217964172363, Loss D: 0.005529012996703386\n",
      "Epoch 157/200, Batch 4/17, Loss G: 3.883068561553955, Loss D: 0.003897078800946474\n",
      "Epoch 157/200, Batch 5/17, Loss G: 3.899343490600586, Loss D: 0.0022705127485096455\n",
      "Epoch 157/200, Batch 6/17, Loss G: 3.8549115657806396, Loss D: 0.006238441448658705\n",
      "Epoch 157/200, Batch 7/17, Loss G: 3.751488208770752, Loss D: 0.004742655903100967\n",
      "Epoch 157/200, Batch 8/17, Loss G: 3.7183899879455566, Loss D: 0.003225286491215229\n",
      "Epoch 157/200, Batch 9/17, Loss G: 3.7552216053009033, Loss D: 0.0033468948677182198\n",
      "Epoch 157/200, Batch 10/17, Loss G: 3.786694288253784, Loss D: 0.00430265162140131\n",
      "Epoch 157/200, Batch 11/17, Loss G: 3.9497454166412354, Loss D: 0.0049235522747039795\n",
      "Epoch 157/200, Batch 12/17, Loss G: 3.581915855407715, Loss D: 0.0029162997379899025\n",
      "Epoch 157/200, Batch 13/17, Loss G: 3.724726915359497, Loss D: 0.009650155901908875\n",
      "Epoch 157/200, Batch 14/17, Loss G: 4.04319429397583, Loss D: 0.00849364697933197\n",
      "Epoch 157/200, Batch 15/17, Loss G: 3.8562886714935303, Loss D: 0.00922444649040699\n",
      "Epoch 157/200, Batch 16/17, Loss G: 3.889911651611328, Loss D: 0.0176579300314188\n",
      "Epoch 158/200, Batch 0/17, Loss G: 3.544442892074585, Loss D: 0.049821123480796814\n",
      "Epoch 158/200, Batch 1/17, Loss G: 3.792865753173828, Loss D: 0.04099789634346962\n",
      "Epoch 158/200, Batch 2/17, Loss G: 3.8403549194335938, Loss D: 0.0019137976923957467\n",
      "Epoch 158/200, Batch 3/17, Loss G: 3.4960808753967285, Loss D: 0.06709030270576477\n",
      "Epoch 158/200, Batch 4/17, Loss G: 4.175447463989258, Loss D: 0.1257370412349701\n",
      "Epoch 158/200, Batch 5/17, Loss G: 3.497884750366211, Loss D: 0.08882966637611389\n",
      "Epoch 158/200, Batch 6/17, Loss G: 3.8855085372924805, Loss D: 0.007478205021470785\n",
      "Epoch 158/200, Batch 7/17, Loss G: 3.860342025756836, Loss D: 0.03798636794090271\n",
      "Epoch 158/200, Batch 8/17, Loss G: 3.447594165802002, Loss D: 0.13651181757450104\n",
      "Epoch 158/200, Batch 9/17, Loss G: 4.088870048522949, Loss D: 0.33205369114875793\n",
      "Epoch 158/200, Batch 10/17, Loss G: 3.362517833709717, Loss D: 0.2656007409095764\n",
      "Epoch 158/200, Batch 11/17, Loss G: 3.6324493885040283, Loss D: 0.17741113901138306\n",
      "Epoch 158/200, Batch 12/17, Loss G: 3.7176451683044434, Loss D: 0.16926094889640808\n",
      "Epoch 158/200, Batch 13/17, Loss G: 3.400214672088623, Loss D: 0.2391062080860138\n",
      "Epoch 158/200, Batch 14/17, Loss G: 3.6953470706939697, Loss D: 0.0962371826171875\n",
      "Epoch 158/200, Batch 15/17, Loss G: 3.712768077850342, Loss D: 0.05377867817878723\n",
      "Epoch 158/200, Batch 16/17, Loss G: 3.8844902515411377, Loss D: 0.05242022126913071\n",
      "Epoch 159/200, Batch 0/17, Loss G: 3.4198098182678223, Loss D: 0.13410131633281708\n",
      "Epoch 159/200, Batch 1/17, Loss G: 4.125612258911133, Loss D: 0.19543510675430298\n",
      "Epoch 159/200, Batch 2/17, Loss G: 3.667982816696167, Loss D: 0.042770180851221085\n",
      "Epoch 159/200, Batch 3/17, Loss G: 3.7186079025268555, Loss D: 0.05086870491504669\n",
      "Epoch 159/200, Batch 4/17, Loss G: 3.9120731353759766, Loss D: 0.04758818447589874\n",
      "Epoch 159/200, Batch 5/17, Loss G: 3.6346523761749268, Loss D: 0.04620945826172829\n",
      "Epoch 159/200, Batch 6/17, Loss G: 4.018578052520752, Loss D: 0.04170295223593712\n",
      "Epoch 159/200, Batch 7/17, Loss G: 3.7093381881713867, Loss D: 0.031726494431495667\n",
      "Epoch 159/200, Batch 8/17, Loss G: 3.7462501525878906, Loss D: 0.02248375676572323\n",
      "Epoch 159/200, Batch 9/17, Loss G: 3.9543864727020264, Loss D: 0.01634271629154682\n",
      "Epoch 159/200, Batch 10/17, Loss G: 3.7887277603149414, Loss D: 0.00929994136095047\n",
      "Epoch 159/200, Batch 11/17, Loss G: 3.902261257171631, Loss D: 0.01608627662062645\n",
      "Epoch 159/200, Batch 12/17, Loss G: 3.6761231422424316, Loss D: 0.019366338849067688\n",
      "Epoch 159/200, Batch 13/17, Loss G: 3.713815927505493, Loss D: 0.009406667202711105\n",
      "Epoch 159/200, Batch 14/17, Loss G: 3.7252798080444336, Loss D: 0.009650705382227898\n",
      "Epoch 159/200, Batch 15/17, Loss G: 3.873352527618408, Loss D: 0.007433249149471521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200, Batch 16/17, Loss G: 3.871826648712158, Loss D: 0.008995059877634048\n",
      "Epoch 160/200, Batch 0/17, Loss G: 3.702056407928467, Loss D: 0.013231514021754265\n",
      "Epoch 160/200, Batch 1/17, Loss G: 3.8332090377807617, Loss D: 0.007899039424955845\n",
      "Epoch 160/200, Batch 2/17, Loss G: 3.8845744132995605, Loss D: 0.012169057503342628\n",
      "Epoch 160/200, Batch 3/17, Loss G: 3.79542875289917, Loss D: 0.012059936299920082\n",
      "Epoch 160/200, Batch 4/17, Loss G: 3.8573098182678223, Loss D: 0.0045766085386276245\n",
      "Epoch 160/200, Batch 5/17, Loss G: 3.7861320972442627, Loss D: 0.015960730612277985\n",
      "Epoch 160/200, Batch 6/17, Loss G: 4.00382137298584, Loss D: 0.010936575941741467\n",
      "Epoch 160/200, Batch 7/17, Loss G: 3.944192409515381, Loss D: 0.01747315376996994\n",
      "Epoch 160/200, Batch 8/17, Loss G: 3.719200372695923, Loss D: 0.011557932943105698\n",
      "Epoch 160/200, Batch 9/17, Loss G: 3.5863254070281982, Loss D: 0.03906989097595215\n",
      "Epoch 160/200, Batch 10/17, Loss G: 4.005393028259277, Loss D: 0.06501488387584686\n",
      "Epoch 160/200, Batch 11/17, Loss G: 3.791565418243408, Loss D: 0.007934735156595707\n",
      "Epoch 160/200, Batch 12/17, Loss G: 3.7442541122436523, Loss D: 0.04449918493628502\n",
      "Epoch 160/200, Batch 13/17, Loss G: 4.066325664520264, Loss D: 0.014737101271748543\n",
      "Epoch 160/200, Batch 14/17, Loss G: 3.949260711669922, Loss D: 0.016888640820980072\n",
      "Epoch 160/200, Batch 15/17, Loss G: 3.6349782943725586, Loss D: 0.009153496474027634\n",
      "Epoch 160/200, Batch 16/17, Loss G: 3.899362802505493, Loss D: 0.02087555080652237\n",
      "Epoch 161/200, Batch 0/17, Loss G: 3.9104158878326416, Loss D: 0.015511948615312576\n",
      "Epoch 161/200, Batch 1/17, Loss G: 3.866637945175171, Loss D: 0.013871933333575726\n",
      "Epoch 161/200, Batch 2/17, Loss G: 3.9132347106933594, Loss D: 0.005102426744997501\n",
      "Epoch 161/200, Batch 3/17, Loss G: 3.7769439220428467, Loss D: 0.015052814036607742\n",
      "Epoch 161/200, Batch 4/17, Loss G: 3.953435182571411, Loss D: 0.006258739158511162\n",
      "Epoch 161/200, Batch 5/17, Loss G: 3.8077921867370605, Loss D: 0.021778207272291183\n",
      "Epoch 161/200, Batch 6/17, Loss G: 3.5301826000213623, Loss D: 0.04519191011786461\n",
      "Epoch 161/200, Batch 7/17, Loss G: 3.887584686279297, Loss D: 0.0227751936763525\n",
      "Epoch 161/200, Batch 8/17, Loss G: 3.7948687076568604, Loss D: 0.012556303292512894\n",
      "Epoch 161/200, Batch 9/17, Loss G: 3.7867183685302734, Loss D: 0.006704267114400864\n",
      "Epoch 161/200, Batch 10/17, Loss G: 3.5832414627075195, Loss D: 0.02227110229432583\n",
      "Epoch 161/200, Batch 11/17, Loss G: 4.0223894119262695, Loss D: 0.0066588930785655975\n",
      "Epoch 161/200, Batch 12/17, Loss G: 3.8316750526428223, Loss D: 0.033948492258787155\n",
      "Epoch 161/200, Batch 13/17, Loss G: 3.6692910194396973, Loss D: 0.0238483976572752\n",
      "Epoch 161/200, Batch 14/17, Loss G: 3.9705514907836914, Loss D: 0.007548444904386997\n",
      "Epoch 161/200, Batch 15/17, Loss G: 3.8278231620788574, Loss D: 0.013458951376378536\n",
      "Epoch 161/200, Batch 16/17, Loss G: 4.0913472175598145, Loss D: 0.009913519024848938\n",
      "Epoch 162/200, Batch 0/17, Loss G: 3.869955539703369, Loss D: 0.008985437452793121\n",
      "Epoch 162/200, Batch 1/17, Loss G: 3.802504062652588, Loss D: 0.0052382247522473335\n",
      "Epoch 162/200, Batch 2/17, Loss G: 3.763885974884033, Loss D: 0.013197948224842548\n",
      "Epoch 162/200, Batch 3/17, Loss G: 4.002211570739746, Loss D: 0.005031154491007328\n",
      "Epoch 162/200, Batch 4/17, Loss G: 3.8674755096435547, Loss D: 0.012344438582658768\n",
      "Epoch 162/200, Batch 5/17, Loss G: 3.88694167137146, Loss D: 0.006757728755474091\n",
      "Epoch 162/200, Batch 6/17, Loss G: 3.8672938346862793, Loss D: 0.009560968726873398\n",
      "Epoch 162/200, Batch 7/17, Loss G: 3.8617241382598877, Loss D: 0.0032410090789198875\n",
      "Epoch 162/200, Batch 8/17, Loss G: 4.035658359527588, Loss D: 0.002770937280729413\n",
      "Epoch 162/200, Batch 9/17, Loss G: 3.71054744720459, Loss D: 0.02105570212006569\n",
      "Epoch 162/200, Batch 10/17, Loss G: 3.9643564224243164, Loss D: 0.022826511412858963\n",
      "Epoch 162/200, Batch 11/17, Loss G: 3.7025227546691895, Loss D: 0.006505096331238747\n",
      "Epoch 162/200, Batch 12/17, Loss G: 3.620901584625244, Loss D: 0.013631932437419891\n",
      "Epoch 162/200, Batch 13/17, Loss G: 3.872058629989624, Loss D: 0.0036907335743308067\n",
      "Epoch 162/200, Batch 14/17, Loss G: 3.8292150497436523, Loss D: 0.005973622668534517\n",
      "Epoch 162/200, Batch 15/17, Loss G: 3.7192368507385254, Loss D: 0.003311645472422242\n",
      "Epoch 162/200, Batch 16/17, Loss G: 3.7106106281280518, Loss D: 0.0022350563667714596\n",
      "Epoch 163/200, Batch 0/17, Loss G: 3.845111846923828, Loss D: 0.005818184930831194\n",
      "Epoch 163/200, Batch 1/17, Loss G: 3.8553037643432617, Loss D: 0.00560722267255187\n",
      "Epoch 163/200, Batch 2/17, Loss G: 3.945011615753174, Loss D: 0.004734862130135298\n",
      "Epoch 163/200, Batch 3/17, Loss G: 3.8364310264587402, Loss D: 0.0057322075590491295\n",
      "Epoch 163/200, Batch 4/17, Loss G: 3.8455216884613037, Loss D: 0.004136235453188419\n",
      "Epoch 163/200, Batch 5/17, Loss G: 3.740950584411621, Loss D: 0.005871623754501343\n",
      "Epoch 163/200, Batch 6/17, Loss G: 3.8520379066467285, Loss D: 0.002453469205647707\n",
      "Epoch 163/200, Batch 7/17, Loss G: 3.6408777236938477, Loss D: 0.00627020513638854\n",
      "Epoch 163/200, Batch 8/17, Loss G: 3.815859794616699, Loss D: 0.00594705156981945\n",
      "Epoch 163/200, Batch 9/17, Loss G: 3.9069395065307617, Loss D: 0.006836537271738052\n",
      "Epoch 163/200, Batch 10/17, Loss G: 3.68649959564209, Loss D: 0.005514209158718586\n",
      "Epoch 163/200, Batch 11/17, Loss G: 3.8547139167785645, Loss D: 0.006245956756174564\n",
      "Epoch 163/200, Batch 12/17, Loss G: 3.9223577976226807, Loss D: 0.004682775586843491\n",
      "Epoch 163/200, Batch 13/17, Loss G: 3.8723597526550293, Loss D: 0.007439508102834225\n",
      "Epoch 163/200, Batch 14/17, Loss G: 3.7951836585998535, Loss D: 0.004244558047503233\n",
      "Epoch 163/200, Batch 15/17, Loss G: 3.8614282608032227, Loss D: 0.005336693488061428\n",
      "Epoch 163/200, Batch 16/17, Loss G: 3.845398187637329, Loss D: 0.003662558039650321\n",
      "Epoch 164/200, Batch 0/17, Loss G: 3.8132882118225098, Loss D: 0.004959641490131617\n",
      "Epoch 164/200, Batch 1/17, Loss G: 3.838320016860962, Loss D: 0.0021363564301282167\n",
      "Epoch 164/200, Batch 2/17, Loss G: 3.680809259414673, Loss D: 0.010135658085346222\n",
      "Epoch 164/200, Batch 3/17, Loss G: 3.710033655166626, Loss D: 0.0062251933850348\n",
      "Epoch 164/200, Batch 4/17, Loss G: 3.914726972579956, Loss D: 0.0021915610413998365\n",
      "Epoch 164/200, Batch 5/17, Loss G: 3.8948240280151367, Loss D: 0.003345477394759655\n",
      "Epoch 164/200, Batch 6/17, Loss G: 3.6560819149017334, Loss D: 0.010066644288599491\n",
      "Epoch 164/200, Batch 7/17, Loss G: 3.9824929237365723, Loss D: 0.004259383771568537\n",
      "Epoch 164/200, Batch 8/17, Loss G: 3.915879487991333, Loss D: 0.004042061045765877\n",
      "Epoch 164/200, Batch 9/17, Loss G: 3.798783779144287, Loss D: 0.004357217811048031\n",
      "Epoch 164/200, Batch 10/17, Loss G: 3.7586276531219482, Loss D: 0.008199268020689487\n",
      "Epoch 164/200, Batch 11/17, Loss G: 3.8195323944091797, Loss D: 0.0025896141305565834\n",
      "Epoch 164/200, Batch 12/17, Loss G: 3.855192184448242, Loss D: 0.0021171681582927704\n",
      "Epoch 164/200, Batch 13/17, Loss G: 3.9664788246154785, Loss D: 0.004666594788432121\n",
      "Epoch 164/200, Batch 14/17, Loss G: 3.8452606201171875, Loss D: 0.005471919197589159\n",
      "Epoch 164/200, Batch 15/17, Loss G: 3.7852611541748047, Loss D: 0.006601717323064804\n",
      "Epoch 164/200, Batch 16/17, Loss G: 3.80490779876709, Loss D: 0.0029074526391923428\n",
      "Epoch 165/200, Batch 0/17, Loss G: 3.6675992012023926, Loss D: 0.0017858988139778376\n",
      "Epoch 165/200, Batch 1/17, Loss G: 3.6134796142578125, Loss D: 0.00451664999127388\n",
      "Epoch 165/200, Batch 2/17, Loss G: 3.7008485794067383, Loss D: 0.006688866764307022\n",
      "Epoch 165/200, Batch 3/17, Loss G: 3.8143632411956787, Loss D: 0.0035923891700804234\n",
      "Epoch 165/200, Batch 4/17, Loss G: 3.8073811531066895, Loss D: 0.004877177067101002\n",
      "Epoch 165/200, Batch 5/17, Loss G: 3.8120174407958984, Loss D: 0.0023949244059622288\n",
      "Epoch 165/200, Batch 6/17, Loss G: 3.845047950744629, Loss D: 0.010691003873944283\n",
      "Epoch 165/200, Batch 7/17, Loss G: 3.961249589920044, Loss D: 0.00541657255962491\n",
      "Epoch 165/200, Batch 8/17, Loss G: 3.950514078140259, Loss D: 0.007626855745911598\n",
      "Epoch 165/200, Batch 9/17, Loss G: 3.789900779724121, Loss D: 0.0030015194788575172\n",
      "Epoch 165/200, Batch 10/17, Loss G: 3.6460790634155273, Loss D: 0.00853050872683525\n",
      "Epoch 165/200, Batch 11/17, Loss G: 3.819044589996338, Loss D: 0.003216087818145752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200, Batch 12/17, Loss G: 3.8202996253967285, Loss D: 0.010668781585991383\n",
      "Epoch 165/200, Batch 13/17, Loss G: 3.8159544467926025, Loss D: 0.00245217839255929\n",
      "Epoch 165/200, Batch 14/17, Loss G: 3.775754928588867, Loss D: 0.004918953403830528\n",
      "Epoch 165/200, Batch 15/17, Loss G: 3.8114607334136963, Loss D: 0.0031147301197052\n",
      "Epoch 165/200, Batch 16/17, Loss G: 3.950181007385254, Loss D: 0.010578486137092113\n",
      "Epoch 166/200, Batch 0/17, Loss G: 4.088505744934082, Loss D: 0.0028117001056671143\n",
      "Epoch 166/200, Batch 1/17, Loss G: 3.689990282058716, Loss D: 0.005830388516187668\n",
      "Epoch 166/200, Batch 2/17, Loss G: 3.7072720527648926, Loss D: 0.004066869150847197\n",
      "Epoch 166/200, Batch 3/17, Loss G: 3.809302806854248, Loss D: 0.003681020112708211\n",
      "Epoch 166/200, Batch 4/17, Loss G: 3.738837957382202, Loss D: 0.007053840905427933\n",
      "Epoch 166/200, Batch 5/17, Loss G: 3.786527395248413, Loss D: 0.002890452276915312\n",
      "Epoch 166/200, Batch 6/17, Loss G: 3.7171502113342285, Loss D: 0.004072521813213825\n",
      "Epoch 166/200, Batch 7/17, Loss G: 3.801546096801758, Loss D: 0.004753457382321358\n",
      "Epoch 166/200, Batch 8/17, Loss G: 3.903254985809326, Loss D: 0.0028705333825200796\n",
      "Epoch 166/200, Batch 9/17, Loss G: 3.8577494621276855, Loss D: 0.003034616820514202\n",
      "Epoch 166/200, Batch 10/17, Loss G: 3.8781700134277344, Loss D: 0.004147901199758053\n",
      "Epoch 166/200, Batch 11/17, Loss G: 3.7227399349212646, Loss D: 0.0029551167972385883\n",
      "Epoch 166/200, Batch 12/17, Loss G: 3.690563201904297, Loss D: 0.0035579320974648\n",
      "Epoch 166/200, Batch 13/17, Loss G: 3.844148635864258, Loss D: 0.0011758038308471441\n",
      "Epoch 166/200, Batch 14/17, Loss G: 3.6677324771881104, Loss D: 0.011146814562380314\n",
      "Epoch 166/200, Batch 15/17, Loss G: 3.8408546447753906, Loss D: 0.003349051345139742\n",
      "Epoch 166/200, Batch 16/17, Loss G: 3.707188129425049, Loss D: 0.008048931136727333\n",
      "Epoch 167/200, Batch 0/17, Loss G: 3.9389023780822754, Loss D: 0.0014023017138242722\n",
      "Epoch 167/200, Batch 1/17, Loss G: 3.912092924118042, Loss D: 0.005527161061763763\n",
      "Epoch 167/200, Batch 2/17, Loss G: 3.730163335800171, Loss D: 0.01284159068018198\n",
      "Epoch 167/200, Batch 3/17, Loss G: 3.970813512802124, Loss D: 0.007897059433162212\n",
      "Epoch 167/200, Batch 4/17, Loss G: 3.8771772384643555, Loss D: 0.007856268435716629\n",
      "Epoch 167/200, Batch 5/17, Loss G: 3.7334771156311035, Loss D: 0.0009691463783383369\n",
      "Epoch 167/200, Batch 6/17, Loss G: 3.560027599334717, Loss D: 0.0041749607771635056\n",
      "Epoch 167/200, Batch 7/17, Loss G: 3.6989054679870605, Loss D: 0.013890980742871761\n",
      "Epoch 167/200, Batch 8/17, Loss G: 3.9242990016937256, Loss D: 0.009484569542109966\n",
      "Epoch 167/200, Batch 9/17, Loss G: 3.9621500968933105, Loss D: 0.006146999076008797\n",
      "Epoch 167/200, Batch 10/17, Loss G: 3.6548821926116943, Loss D: 0.011162687093019485\n",
      "Epoch 167/200, Batch 11/17, Loss G: 3.8744113445281982, Loss D: 0.02798370085656643\n",
      "Epoch 167/200, Batch 12/17, Loss G: 3.5475704669952393, Loss D: 0.03693902865052223\n",
      "Epoch 167/200, Batch 13/17, Loss G: 3.9597361087799072, Loss D: 0.004027890041470528\n",
      "Epoch 167/200, Batch 14/17, Loss G: 3.9585962295532227, Loss D: 0.002341805025935173\n",
      "Epoch 167/200, Batch 15/17, Loss G: 3.9013376235961914, Loss D: 0.009976819157600403\n",
      "Epoch 167/200, Batch 16/17, Loss G: 3.6784040927886963, Loss D: 0.004542143549770117\n",
      "Epoch 168/200, Batch 0/17, Loss G: 3.530867099761963, Loss D: 0.03666331619024277\n",
      "Epoch 168/200, Batch 1/17, Loss G: 4.15261173248291, Loss D: 0.05579638108611107\n",
      "Epoch 168/200, Batch 2/17, Loss G: 3.8426356315612793, Loss D: 0.005026237107813358\n",
      "Epoch 168/200, Batch 3/17, Loss G: 3.857368230819702, Loss D: 0.002126870211213827\n",
      "Epoch 168/200, Batch 4/17, Loss G: 3.6427865028381348, Loss D: 0.010791193693876266\n",
      "Epoch 168/200, Batch 5/17, Loss G: 3.7143192291259766, Loss D: 0.009348902851343155\n",
      "Epoch 168/200, Batch 6/17, Loss G: 3.872466564178467, Loss D: 0.006467737257480621\n",
      "Epoch 168/200, Batch 7/17, Loss G: 3.792161464691162, Loss D: 0.010282626375555992\n",
      "Epoch 168/200, Batch 8/17, Loss G: 3.6729249954223633, Loss D: 0.005818889010697603\n",
      "Epoch 168/200, Batch 9/17, Loss G: 3.757763385772705, Loss D: 0.0016651568002998829\n",
      "Epoch 168/200, Batch 10/17, Loss G: 3.6778979301452637, Loss D: 0.005418475717306137\n",
      "Epoch 168/200, Batch 11/17, Loss G: 3.970386505126953, Loss D: 0.0038324755150824785\n",
      "Epoch 168/200, Batch 12/17, Loss G: 3.6938576698303223, Loss D: 0.007392760366201401\n",
      "Epoch 168/200, Batch 13/17, Loss G: 3.98909592628479, Loss D: 0.005046450532972813\n",
      "Epoch 168/200, Batch 14/17, Loss G: 3.7648091316223145, Loss D: 0.003981039393693209\n",
      "Epoch 168/200, Batch 15/17, Loss G: 3.772432804107666, Loss D: 0.0033973348326981068\n",
      "Epoch 168/200, Batch 16/17, Loss G: 3.982941150665283, Loss D: 0.00033375591738149524\n",
      "Epoch 169/200, Batch 0/17, Loss G: 3.6929376125335693, Loss D: 0.01806722581386566\n",
      "Epoch 169/200, Batch 1/17, Loss G: 3.8491508960723877, Loss D: 0.03981756046414375\n",
      "Epoch 169/200, Batch 2/17, Loss G: 3.748859405517578, Loss D: 0.0028141899965703487\n",
      "Epoch 169/200, Batch 3/17, Loss G: 3.3799870014190674, Loss D: 0.10214190930128098\n",
      "Epoch 169/200, Batch 4/17, Loss G: 4.184211730957031, Loss D: 0.12286105006933212\n",
      "Epoch 169/200, Batch 5/17, Loss G: 3.9995169639587402, Loss D: 0.000720132899004966\n",
      "Epoch 169/200, Batch 6/17, Loss G: 3.6487040519714355, Loss D: 0.03935542702674866\n",
      "Epoch 169/200, Batch 7/17, Loss G: 3.789486885070801, Loss D: 0.006320776883512735\n",
      "Epoch 169/200, Batch 8/17, Loss G: 3.9868335723876953, Loss D: 0.0030293227173388004\n",
      "Epoch 169/200, Batch 9/17, Loss G: 3.780773162841797, Loss D: 0.02090512216091156\n",
      "Epoch 169/200, Batch 10/17, Loss G: 3.9608399868011475, Loss D: 0.05702345073223114\n",
      "Epoch 169/200, Batch 11/17, Loss G: 3.331430435180664, Loss D: 0.0756308063864708\n",
      "Epoch 169/200, Batch 12/17, Loss G: 3.868039608001709, Loss D: 0.008118989877402782\n",
      "Epoch 169/200, Batch 13/17, Loss G: 3.886366367340088, Loss D: 0.035897813737392426\n",
      "Epoch 169/200, Batch 14/17, Loss G: 3.3790388107299805, Loss D: 0.12669143080711365\n",
      "Epoch 169/200, Batch 15/17, Loss G: 4.053785800933838, Loss D: 0.1531033217906952\n",
      "Epoch 169/200, Batch 16/17, Loss G: 3.822627067565918, Loss D: 0.010963033884763718\n",
      "Epoch 170/200, Batch 0/17, Loss G: 3.638690948486328, Loss D: 0.06192835047841072\n",
      "Epoch 170/200, Batch 1/17, Loss G: 3.9616053104400635, Loss D: 0.12553107738494873\n",
      "Epoch 170/200, Batch 2/17, Loss G: 3.523043155670166, Loss D: 0.12051615864038467\n",
      "Epoch 170/200, Batch 3/17, Loss G: 3.7480721473693848, Loss D: 0.07847963273525238\n",
      "Epoch 170/200, Batch 4/17, Loss G: 3.868616819381714, Loss D: 0.056329283863306046\n",
      "Epoch 170/200, Batch 5/17, Loss G: 3.25789737701416, Loss D: 0.07418617606163025\n",
      "Epoch 170/200, Batch 6/17, Loss G: 3.9304118156433105, Loss D: 0.03274320065975189\n",
      "Epoch 170/200, Batch 7/17, Loss G: 3.799093008041382, Loss D: 0.045903537422418594\n",
      "Epoch 170/200, Batch 8/17, Loss G: 3.470221996307373, Loss D: 0.09131766110658646\n",
      "Epoch 170/200, Batch 9/17, Loss G: 3.993220329284668, Loss D: 0.18642181158065796\n",
      "Epoch 170/200, Batch 10/17, Loss G: 3.657909393310547, Loss D: 0.06549277156591415\n",
      "Epoch 170/200, Batch 11/17, Loss G: 3.863853931427002, Loss D: 0.015358556061983109\n",
      "Epoch 170/200, Batch 12/17, Loss G: 3.7137598991394043, Loss D: 0.018016589805483818\n",
      "Epoch 170/200, Batch 13/17, Loss G: 3.8255906105041504, Loss D: 0.02568044885993004\n",
      "Epoch 170/200, Batch 14/17, Loss G: 3.7525477409362793, Loss D: 0.016541779041290283\n",
      "Epoch 170/200, Batch 15/17, Loss G: 3.8068766593933105, Loss D: 0.020185505971312523\n",
      "Epoch 170/200, Batch 16/17, Loss G: 3.934126377105713, Loss D: 0.022334659472107887\n",
      "Epoch 171/200, Batch 0/17, Loss G: 3.6721668243408203, Loss D: 0.02740360051393509\n",
      "Epoch 171/200, Batch 1/17, Loss G: 3.919710874557495, Loss D: 0.019397318363189697\n",
      "Epoch 171/200, Batch 2/17, Loss G: 3.745734214782715, Loss D: 0.006241230294108391\n",
      "Epoch 171/200, Batch 3/17, Loss G: 3.603048324584961, Loss D: 0.03006473369896412\n",
      "Epoch 171/200, Batch 4/17, Loss G: 3.9793426990509033, Loss D: 0.024922333657741547\n",
      "Epoch 171/200, Batch 5/17, Loss G: 3.909391164779663, Loss D: 0.013963861390948296\n",
      "Epoch 171/200, Batch 6/17, Loss G: 3.701192855834961, Loss D: 0.014969957061111927\n",
      "Epoch 171/200, Batch 7/17, Loss G: 3.724137783050537, Loss D: 0.007233285345137119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200, Batch 8/17, Loss G: 3.771962881088257, Loss D: 0.0273169856518507\n",
      "Epoch 171/200, Batch 9/17, Loss G: 3.5439612865448, Loss D: 0.05772662162780762\n",
      "Epoch 171/200, Batch 10/17, Loss G: 3.9011905193328857, Loss D: 0.05697999522089958\n",
      "Epoch 171/200, Batch 11/17, Loss G: 3.839308738708496, Loss D: 0.007318070158362389\n",
      "Epoch 171/200, Batch 12/17, Loss G: 3.798757553100586, Loss D: 0.0200339425355196\n",
      "Epoch 171/200, Batch 13/17, Loss G: 3.9093947410583496, Loss D: 0.005488401744514704\n",
      "Epoch 171/200, Batch 14/17, Loss G: 3.8614661693573, Loss D: 0.026018977165222168\n",
      "Epoch 171/200, Batch 15/17, Loss G: 3.6250991821289062, Loss D: 0.007322028279304504\n",
      "Epoch 171/200, Batch 16/17, Loss G: 4.231297016143799, Loss D: 0.008963568136096\n",
      "Epoch 172/200, Batch 0/17, Loss G: 3.7433485984802246, Loss D: 0.010226740501821041\n",
      "Epoch 172/200, Batch 1/17, Loss G: 3.785066843032837, Loss D: 0.009338497184216976\n",
      "Epoch 172/200, Batch 2/17, Loss G: 3.864358425140381, Loss D: 0.007905283942818642\n",
      "Epoch 172/200, Batch 3/17, Loss G: 3.990818500518799, Loss D: 0.0088281175121665\n",
      "Epoch 172/200, Batch 4/17, Loss G: 3.699601411819458, Loss D: 0.004134733229875565\n",
      "Epoch 172/200, Batch 5/17, Loss G: 3.6585536003112793, Loss D: 0.005847942549735308\n",
      "Epoch 172/200, Batch 6/17, Loss G: 3.78753399848938, Loss D: 0.01155599020421505\n",
      "Epoch 172/200, Batch 7/17, Loss G: 3.903759479522705, Loss D: 0.0030668950639665127\n",
      "Epoch 172/200, Batch 8/17, Loss G: 3.717963695526123, Loss D: 0.007806818466633558\n",
      "Epoch 172/200, Batch 9/17, Loss G: 3.7779860496520996, Loss D: 0.0050412071868777275\n",
      "Epoch 172/200, Batch 10/17, Loss G: 3.813220977783203, Loss D: 0.006002206355333328\n",
      "Epoch 172/200, Batch 11/17, Loss G: 3.741530418395996, Loss D: 0.004638421814888716\n",
      "Epoch 172/200, Batch 12/17, Loss G: 3.80277943611145, Loss D: 0.006384973879903555\n",
      "Epoch 172/200, Batch 13/17, Loss G: 3.6809728145599365, Loss D: 0.0042837499640882015\n",
      "Epoch 172/200, Batch 14/17, Loss G: 3.714341640472412, Loss D: 0.01458560861647129\n",
      "Epoch 172/200, Batch 15/17, Loss G: 3.848904609680176, Loss D: 0.013395403511822224\n",
      "Epoch 172/200, Batch 16/17, Loss G: 3.951963186264038, Loss D: 0.0053073326125741005\n",
      "Epoch 173/200, Batch 0/17, Loss G: 3.9827075004577637, Loss D: 0.01627272367477417\n",
      "Epoch 173/200, Batch 1/17, Loss G: 3.7309696674346924, Loss D: 0.006579920649528503\n",
      "Epoch 173/200, Batch 2/17, Loss G: 3.792804479598999, Loss D: 0.00899908784776926\n",
      "Epoch 173/200, Batch 3/17, Loss G: 3.7404794692993164, Loss D: 0.00900943297892809\n",
      "Epoch 173/200, Batch 4/17, Loss G: 3.8915953636169434, Loss D: 0.0045415619388222694\n",
      "Epoch 173/200, Batch 5/17, Loss G: 3.583383083343506, Loss D: 0.005798141472041607\n",
      "Epoch 173/200, Batch 6/17, Loss G: 3.5839638710021973, Loss D: 0.01059948094189167\n",
      "Epoch 173/200, Batch 7/17, Loss G: 3.7330899238586426, Loss D: 0.009787027724087238\n",
      "Epoch 173/200, Batch 8/17, Loss G: 3.8481569290161133, Loss D: 0.014107473194599152\n",
      "Epoch 173/200, Batch 9/17, Loss G: 3.7133092880249023, Loss D: 0.004687372129410505\n",
      "Epoch 173/200, Batch 10/17, Loss G: 3.6743764877319336, Loss D: 0.004488207399845123\n",
      "Epoch 173/200, Batch 11/17, Loss G: 3.808056354522705, Loss D: 0.008444275707006454\n",
      "Epoch 173/200, Batch 12/17, Loss G: 3.836723804473877, Loss D: 0.0014383313246071339\n",
      "Epoch 173/200, Batch 13/17, Loss G: 3.674651861190796, Loss D: 0.012045313604176044\n",
      "Epoch 173/200, Batch 14/17, Loss G: 3.8742518424987793, Loss D: 0.00464654341340065\n",
      "Epoch 173/200, Batch 15/17, Loss G: 3.8953046798706055, Loss D: 0.002376334508880973\n",
      "Epoch 173/200, Batch 16/17, Loss G: 3.9262630939483643, Loss D: 0.024938929826021194\n",
      "Epoch 174/200, Batch 0/17, Loss G: 3.342906951904297, Loss D: 0.05998271703720093\n",
      "Epoch 174/200, Batch 1/17, Loss G: 3.8869142532348633, Loss D: 0.016524670645594597\n",
      "Epoch 174/200, Batch 2/17, Loss G: 4.024822235107422, Loss D: 0.016915882006287575\n",
      "Epoch 174/200, Batch 3/17, Loss G: 3.8349828720092773, Loss D: 0.0041915480978786945\n",
      "Epoch 174/200, Batch 4/17, Loss G: 3.6247406005859375, Loss D: 0.007697517517954111\n",
      "Epoch 174/200, Batch 5/17, Loss G: 3.702199697494507, Loss D: 0.004842056892812252\n",
      "Epoch 174/200, Batch 6/17, Loss G: 3.7499136924743652, Loss D: 0.00955989956855774\n",
      "Epoch 174/200, Batch 7/17, Loss G: 3.9177467823028564, Loss D: 0.009355731308460236\n",
      "Epoch 174/200, Batch 8/17, Loss G: 3.7179148197174072, Loss D: 0.006309225223958492\n",
      "Epoch 174/200, Batch 9/17, Loss G: 3.733670711517334, Loss D: 0.008596979081630707\n",
      "Epoch 174/200, Batch 10/17, Loss G: 3.7344970703125, Loss D: 0.00405093701556325\n",
      "Epoch 174/200, Batch 11/17, Loss G: 3.8133440017700195, Loss D: 0.004290296696126461\n",
      "Epoch 174/200, Batch 12/17, Loss G: 3.6816742420196533, Loss D: 0.0064548589289188385\n",
      "Epoch 174/200, Batch 13/17, Loss G: 3.6778626441955566, Loss D: 0.00733202276751399\n",
      "Epoch 174/200, Batch 14/17, Loss G: 3.947683811187744, Loss D: 0.005235718563199043\n",
      "Epoch 174/200, Batch 15/17, Loss G: 3.75795841217041, Loss D: 0.003984740935266018\n",
      "Epoch 174/200, Batch 16/17, Loss G: 3.8159561157226562, Loss D: 0.0033688941039144993\n",
      "Epoch 175/200, Batch 0/17, Loss G: 3.5869717597961426, Loss D: 0.0053260549902915955\n",
      "Epoch 175/200, Batch 1/17, Loss G: 3.817322254180908, Loss D: 0.003467240836471319\n",
      "Epoch 175/200, Batch 2/17, Loss G: 3.7271618843078613, Loss D: 0.003137741470709443\n",
      "Epoch 175/200, Batch 3/17, Loss G: 3.810849666595459, Loss D: 0.012126787565648556\n",
      "Epoch 175/200, Batch 4/17, Loss G: 3.8580727577209473, Loss D: 0.008523937314748764\n",
      "Epoch 175/200, Batch 5/17, Loss G: 3.7945141792297363, Loss D: 0.002081449842080474\n",
      "Epoch 175/200, Batch 6/17, Loss G: 3.766876220703125, Loss D: 0.006003423593938351\n",
      "Epoch 175/200, Batch 7/17, Loss G: 3.6681411266326904, Loss D: 0.008517203852534294\n",
      "Epoch 175/200, Batch 8/17, Loss G: 3.808943510055542, Loss D: 0.0023314249701797962\n",
      "Epoch 175/200, Batch 9/17, Loss G: 3.73060941696167, Loss D: 0.005325233098119497\n",
      "Epoch 175/200, Batch 10/17, Loss G: 3.631474733352661, Loss D: 0.006873107049614191\n",
      "Epoch 175/200, Batch 11/17, Loss G: 3.8745765686035156, Loss D: 0.00935339741408825\n",
      "Epoch 175/200, Batch 12/17, Loss G: 3.9105007648468018, Loss D: 0.0009324887068942189\n",
      "Epoch 175/200, Batch 13/17, Loss G: 3.67110538482666, Loss D: 0.005616764537990093\n",
      "Epoch 175/200, Batch 14/17, Loss G: 3.765542507171631, Loss D: 0.0017925106221809983\n",
      "Epoch 175/200, Batch 15/17, Loss G: 3.7371110916137695, Loss D: 0.0036180666647851467\n",
      "Epoch 175/200, Batch 16/17, Loss G: 3.7532858848571777, Loss D: 0.0023015597835183144\n",
      "Epoch 176/200, Batch 0/17, Loss G: 3.645613193511963, Loss D: 0.01631886139512062\n",
      "Epoch 176/200, Batch 1/17, Loss G: 3.856173038482666, Loss D: 0.016501206904649734\n",
      "Epoch 176/200, Batch 2/17, Loss G: 3.885352611541748, Loss D: 0.002359129022806883\n",
      "Epoch 176/200, Batch 3/17, Loss G: 3.624478816986084, Loss D: 0.002353623742237687\n",
      "Epoch 176/200, Batch 4/17, Loss G: 3.5797955989837646, Loss D: 0.014203334227204323\n",
      "Epoch 176/200, Batch 5/17, Loss G: 3.9498605728149414, Loss D: 0.004651311319321394\n",
      "Epoch 176/200, Batch 6/17, Loss G: 3.835261821746826, Loss D: 0.0076843807473778725\n",
      "Epoch 176/200, Batch 7/17, Loss G: 3.6755266189575195, Loss D: 0.005546480417251587\n",
      "Epoch 176/200, Batch 8/17, Loss G: 3.644374370574951, Loss D: 0.001955320592969656\n",
      "Epoch 176/200, Batch 9/17, Loss G: 3.6212844848632812, Loss D: 0.004571875091642141\n",
      "Epoch 176/200, Batch 10/17, Loss G: 3.9038004875183105, Loss D: 0.002701005432754755\n",
      "Epoch 176/200, Batch 11/17, Loss G: 3.8050873279571533, Loss D: 0.0028408498037606478\n",
      "Epoch 176/200, Batch 12/17, Loss G: 3.72945499420166, Loss D: 0.004467618186026812\n",
      "Epoch 176/200, Batch 13/17, Loss G: 3.6850082874298096, Loss D: 0.002120470628142357\n",
      "Epoch 176/200, Batch 14/17, Loss G: 3.550549030303955, Loss D: 0.006333332974463701\n",
      "Epoch 176/200, Batch 15/17, Loss G: 3.8711800575256348, Loss D: 0.001761546009220183\n",
      "Epoch 176/200, Batch 16/17, Loss G: 3.956315040588379, Loss D: 0.007858657278120518\n",
      "Epoch 177/200, Batch 0/17, Loss G: 3.7904934883117676, Loss D: 0.00655547808855772\n",
      "Epoch 177/200, Batch 1/17, Loss G: 3.736752986907959, Loss D: 0.0015080408193171024\n",
      "Epoch 177/200, Batch 2/17, Loss G: 3.792487621307373, Loss D: 0.0024205786176025867\n",
      "Epoch 177/200, Batch 3/17, Loss G: 3.7898902893066406, Loss D: 0.0017063601408153772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200, Batch 4/17, Loss G: 3.6723697185516357, Loss D: 0.002637745812535286\n",
      "Epoch 177/200, Batch 5/17, Loss G: 3.779456377029419, Loss D: 0.0013681005220860243\n",
      "Epoch 177/200, Batch 6/17, Loss G: 3.7773597240448, Loss D: 0.005186197813600302\n",
      "Epoch 177/200, Batch 7/17, Loss G: 3.7173166275024414, Loss D: 0.011059457436203957\n",
      "Epoch 177/200, Batch 8/17, Loss G: 3.636151075363159, Loss D: 0.00261456030420959\n",
      "Epoch 177/200, Batch 9/17, Loss G: 3.719331741333008, Loss D: 0.004737219773232937\n",
      "Epoch 177/200, Batch 10/17, Loss G: 3.781048536300659, Loss D: 0.006111695896834135\n",
      "Epoch 177/200, Batch 11/17, Loss G: 3.938969135284424, Loss D: 0.003988390788435936\n",
      "Epoch 177/200, Batch 12/17, Loss G: 3.7538208961486816, Loss D: 0.0015458648558706045\n",
      "Epoch 177/200, Batch 13/17, Loss G: 3.8964691162109375, Loss D: 0.003130412194877863\n",
      "Epoch 177/200, Batch 14/17, Loss G: 3.6919682025909424, Loss D: 0.0028313444927334785\n",
      "Epoch 177/200, Batch 15/17, Loss G: 3.869879722595215, Loss D: 0.001372031751088798\n",
      "Epoch 177/200, Batch 16/17, Loss G: 3.7802844047546387, Loss D: 0.003600264433771372\n",
      "Epoch 178/200, Batch 0/17, Loss G: 3.7811360359191895, Loss D: 0.005115614738315344\n",
      "Epoch 178/200, Batch 1/17, Loss G: 3.759127140045166, Loss D: 0.004026789218187332\n",
      "Epoch 178/200, Batch 2/17, Loss G: 3.74955153465271, Loss D: 0.002628822810947895\n",
      "Epoch 178/200, Batch 3/17, Loss G: 3.8058691024780273, Loss D: 0.0029308749362826347\n",
      "Epoch 178/200, Batch 4/17, Loss G: 3.619006633758545, Loss D: 0.004016609396785498\n",
      "Epoch 178/200, Batch 5/17, Loss G: 3.698488712310791, Loss D: 0.01721937023103237\n",
      "Epoch 178/200, Batch 6/17, Loss G: 4.001645565032959, Loss D: 0.023582519963383675\n",
      "Epoch 178/200, Batch 7/17, Loss G: 3.738227128982544, Loss D: 0.0033614900894463062\n",
      "Epoch 178/200, Batch 8/17, Loss G: 3.6318297386169434, Loss D: 0.01928737759590149\n",
      "Epoch 178/200, Batch 9/17, Loss G: 3.7823400497436523, Loss D: 0.018465640023350716\n",
      "Epoch 178/200, Batch 10/17, Loss G: 3.811630964279175, Loss D: 0.007122233044356108\n",
      "Epoch 178/200, Batch 11/17, Loss G: 3.7179341316223145, Loss D: 0.0011269296519458294\n",
      "Epoch 178/200, Batch 12/17, Loss G: 3.7252888679504395, Loss D: 0.001408807816915214\n",
      "Epoch 178/200, Batch 13/17, Loss G: 3.6518611907958984, Loss D: 0.008640355430543423\n",
      "Epoch 178/200, Batch 14/17, Loss G: 3.7879068851470947, Loss D: 0.0038002824876457453\n",
      "Epoch 178/200, Batch 15/17, Loss G: 3.7469778060913086, Loss D: 0.0063240104354918\n",
      "Epoch 178/200, Batch 16/17, Loss G: 3.8358535766601562, Loss D: 0.0031421734020113945\n",
      "Epoch 179/200, Batch 0/17, Loss G: 3.674752950668335, Loss D: 0.004765044432133436\n",
      "Epoch 179/200, Batch 1/17, Loss G: 3.8682332038879395, Loss D: 0.0014751674607396126\n",
      "Epoch 179/200, Batch 2/17, Loss G: 3.602614641189575, Loss D: 0.00259345187805593\n",
      "Epoch 179/200, Batch 3/17, Loss G: 3.7096476554870605, Loss D: 0.00417717220261693\n",
      "Epoch 179/200, Batch 4/17, Loss G: 3.7681100368499756, Loss D: 0.0037852455861866474\n",
      "Epoch 179/200, Batch 5/17, Loss G: 3.762052536010742, Loss D: 0.003224415937438607\n",
      "Epoch 179/200, Batch 6/17, Loss G: 3.712301731109619, Loss D: 0.007439378648996353\n",
      "Epoch 179/200, Batch 7/17, Loss G: 3.737539291381836, Loss D: 0.005034278146922588\n",
      "Epoch 179/200, Batch 8/17, Loss G: 3.7760114669799805, Loss D: 0.004387872759252787\n",
      "Epoch 179/200, Batch 9/17, Loss G: 3.696509838104248, Loss D: 0.004836206324398518\n",
      "Epoch 179/200, Batch 10/17, Loss G: 3.7791244983673096, Loss D: 0.0009519446175545454\n",
      "Epoch 179/200, Batch 11/17, Loss G: 3.614961624145508, Loss D: 0.0014821233926340938\n",
      "Epoch 179/200, Batch 12/17, Loss G: 3.6505849361419678, Loss D: 0.004387913271784782\n",
      "Epoch 179/200, Batch 13/17, Loss G: 3.727196216583252, Loss D: 0.012142064049839973\n",
      "Epoch 179/200, Batch 14/17, Loss G: 3.9284543991088867, Loss D: 0.0032010781578719616\n",
      "Epoch 179/200, Batch 15/17, Loss G: 3.8028697967529297, Loss D: 0.0020127727184444666\n",
      "Epoch 179/200, Batch 16/17, Loss G: 4.168202877044678, Loss D: 0.0021203658543527126\n",
      "Epoch 180/200, Batch 0/17, Loss G: 3.804837226867676, Loss D: 0.001061002491042018\n",
      "Epoch 180/200, Batch 1/17, Loss G: 3.753653049468994, Loss D: 0.0011511415941640735\n",
      "Epoch 180/200, Batch 2/17, Loss G: 3.65091872215271, Loss D: 0.0021269856952130795\n",
      "Epoch 180/200, Batch 3/17, Loss G: 3.861530303955078, Loss D: 0.004365634638816118\n",
      "Epoch 180/200, Batch 4/17, Loss G: 3.7004177570343018, Loss D: 0.0021859859116375446\n",
      "Epoch 180/200, Batch 5/17, Loss G: 3.720820188522339, Loss D: 0.004436071030795574\n",
      "Epoch 180/200, Batch 6/17, Loss G: 3.77364444732666, Loss D: 0.006301127839833498\n",
      "Epoch 180/200, Batch 7/17, Loss G: 3.7066152095794678, Loss D: 0.001884458470158279\n",
      "Epoch 180/200, Batch 8/17, Loss G: 3.7867300510406494, Loss D: 0.001265555853024125\n",
      "Epoch 180/200, Batch 9/17, Loss G: 3.737182855606079, Loss D: 0.007803153712302446\n",
      "Epoch 180/200, Batch 10/17, Loss G: 3.5357751846313477, Loss D: 0.010370944626629353\n",
      "Epoch 180/200, Batch 11/17, Loss G: 3.728912115097046, Loss D: 0.001027086516842246\n",
      "Epoch 180/200, Batch 12/17, Loss G: 3.735642671585083, Loss D: 0.006863193120807409\n",
      "Epoch 180/200, Batch 13/17, Loss G: 3.6975419521331787, Loss D: 0.003271101973950863\n",
      "Epoch 180/200, Batch 14/17, Loss G: 3.719766139984131, Loss D: 0.00251278979703784\n",
      "Epoch 180/200, Batch 15/17, Loss G: 3.608311176300049, Loss D: 0.007206956390291452\n",
      "Epoch 180/200, Batch 16/17, Loss G: 3.932624340057373, Loss D: 0.008363681845366955\n",
      "Epoch 181/200, Batch 0/17, Loss G: 3.6454434394836426, Loss D: 0.0074422019533813\n",
      "Epoch 181/200, Batch 1/17, Loss G: 3.8536598682403564, Loss D: 0.003037834307178855\n",
      "Epoch 181/200, Batch 2/17, Loss G: 3.802522659301758, Loss D: 0.0013868426904082298\n",
      "Epoch 181/200, Batch 3/17, Loss G: 3.777456760406494, Loss D: 0.0025256448425352573\n",
      "Epoch 181/200, Batch 4/17, Loss G: 3.660200595855713, Loss D: 0.0005725031951442361\n",
      "Epoch 181/200, Batch 5/17, Loss G: 3.7029948234558105, Loss D: 0.002109087072312832\n",
      "Epoch 181/200, Batch 6/17, Loss G: 3.6543405055999756, Loss D: 0.001211844151839614\n",
      "Epoch 181/200, Batch 7/17, Loss G: 3.6813406944274902, Loss D: 0.001033960608765483\n",
      "Epoch 181/200, Batch 8/17, Loss G: 3.6863598823547363, Loss D: 0.0012701021041721106\n",
      "Epoch 181/200, Batch 9/17, Loss G: 3.6294496059417725, Loss D: 0.001444926019757986\n",
      "Epoch 181/200, Batch 10/17, Loss G: 3.7538468837738037, Loss D: 0.0007738707354292274\n",
      "Epoch 181/200, Batch 11/17, Loss G: 3.6364142894744873, Loss D: 0.0019017988815903664\n",
      "Epoch 181/200, Batch 12/17, Loss G: 3.584406614303589, Loss D: 0.0018957227002829313\n",
      "Epoch 181/200, Batch 13/17, Loss G: 3.698730707168579, Loss D: 0.0032960386015474796\n",
      "Epoch 181/200, Batch 14/17, Loss G: 3.9037394523620605, Loss D: 0.0019987933337688446\n",
      "Epoch 181/200, Batch 15/17, Loss G: 3.797311305999756, Loss D: 0.005653953179717064\n",
      "Epoch 181/200, Batch 16/17, Loss G: 3.901841163635254, Loss D: 0.008270085789263248\n",
      "Epoch 182/200, Batch 0/17, Loss G: 3.7076869010925293, Loss D: 0.0027334499172866344\n",
      "Epoch 182/200, Batch 1/17, Loss G: 3.561069965362549, Loss D: 0.0014202619204297662\n",
      "Epoch 182/200, Batch 2/17, Loss G: 3.5847702026367188, Loss D: 0.006848436314612627\n",
      "Epoch 182/200, Batch 3/17, Loss G: 3.8995141983032227, Loss D: 0.001701928791590035\n",
      "Epoch 182/200, Batch 4/17, Loss G: 3.891301393508911, Loss D: 0.006141563877463341\n",
      "Epoch 182/200, Batch 5/17, Loss G: 3.6713619232177734, Loss D: 0.0011235653655603528\n",
      "Epoch 182/200, Batch 6/17, Loss G: 3.6524038314819336, Loss D: 0.0018772347830235958\n",
      "Epoch 182/200, Batch 7/17, Loss G: 3.654418468475342, Loss D: 0.003644775366410613\n",
      "Epoch 182/200, Batch 8/17, Loss G: 3.801693916320801, Loss D: 0.0006396719836629927\n",
      "Epoch 182/200, Batch 9/17, Loss G: 3.7330496311187744, Loss D: 0.0010419628815725446\n",
      "Epoch 182/200, Batch 10/17, Loss G: 3.662203788757324, Loss D: 0.002780322916805744\n",
      "Epoch 182/200, Batch 11/17, Loss G: 3.5994129180908203, Loss D: 0.003811820177361369\n",
      "Epoch 182/200, Batch 12/17, Loss G: 3.6743667125701904, Loss D: 0.006773678120225668\n",
      "Epoch 182/200, Batch 13/17, Loss G: 3.721111297607422, Loss D: 0.00464341277256608\n",
      "Epoch 182/200, Batch 14/17, Loss G: 3.8131844997406006, Loss D: 0.0020395906176418066\n",
      "Epoch 182/200, Batch 15/17, Loss G: 3.6652183532714844, Loss D: 0.0016664720606058836\n",
      "Epoch 182/200, Batch 16/17, Loss G: 3.757472038269043, Loss D: 0.0004218761168885976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200, Batch 0/17, Loss G: 3.7182955741882324, Loss D: 0.0035513064358383417\n",
      "Epoch 183/200, Batch 1/17, Loss G: 3.7048254013061523, Loss D: 0.007198923267424107\n",
      "Epoch 183/200, Batch 2/17, Loss G: 3.8078837394714355, Loss D: 0.031031476333737373\n",
      "Epoch 183/200, Batch 3/17, Loss G: 3.4689717292785645, Loss D: 0.054298464208841324\n",
      "Epoch 183/200, Batch 4/17, Loss G: 3.887603282928467, Loss D: 0.006422160193324089\n",
      "Epoch 183/200, Batch 5/17, Loss G: 3.7834086418151855, Loss D: 0.011645939201116562\n",
      "Epoch 183/200, Batch 6/17, Loss G: 3.791487693786621, Loss D: 0.0012764029670506716\n",
      "Epoch 183/200, Batch 7/17, Loss G: 3.6849215030670166, Loss D: 0.004694134928286076\n",
      "Epoch 183/200, Batch 8/17, Loss G: 3.5897130966186523, Loss D: 0.0021422808058559895\n",
      "Epoch 183/200, Batch 9/17, Loss G: 3.5752758979797363, Loss D: 0.006246720906347036\n",
      "Epoch 183/200, Batch 10/17, Loss G: 3.7369370460510254, Loss D: 0.004128226079046726\n",
      "Epoch 183/200, Batch 11/17, Loss G: 3.717252731323242, Loss D: 0.003716083010658622\n",
      "Epoch 183/200, Batch 12/17, Loss G: 3.7541685104370117, Loss D: 0.001456266501918435\n",
      "Epoch 183/200, Batch 13/17, Loss G: 3.7476892471313477, Loss D: 0.0034738839603960514\n",
      "Epoch 183/200, Batch 14/17, Loss G: 3.66768217086792, Loss D: 0.0015972433611750603\n",
      "Epoch 183/200, Batch 15/17, Loss G: 3.7262723445892334, Loss D: 0.004741188138723373\n",
      "Epoch 183/200, Batch 16/17, Loss G: 3.8264670372009277, Loss D: 0.002954503521323204\n",
      "Epoch 184/200, Batch 0/17, Loss G: 3.8300693035125732, Loss D: 0.0013016504235565662\n",
      "Epoch 184/200, Batch 1/17, Loss G: 3.703400135040283, Loss D: 0.006401490420103073\n",
      "Epoch 184/200, Batch 2/17, Loss G: 3.6163060665130615, Loss D: 0.002456484129652381\n",
      "Epoch 184/200, Batch 3/17, Loss G: 3.6154589653015137, Loss D: 0.005192694719880819\n",
      "Epoch 184/200, Batch 4/17, Loss G: 3.650087833404541, Loss D: 0.002369202673435211\n",
      "Epoch 184/200, Batch 5/17, Loss G: 3.7568793296813965, Loss D: 0.004896366503089666\n",
      "Epoch 184/200, Batch 6/17, Loss G: 3.8938045501708984, Loss D: 0.0029916868079453707\n",
      "Epoch 184/200, Batch 7/17, Loss G: 3.732623815536499, Loss D: 0.005565562751144171\n",
      "Epoch 184/200, Batch 8/17, Loss G: 3.65677809715271, Loss D: 0.0032782447524368763\n",
      "Epoch 184/200, Batch 9/17, Loss G: 3.894216299057007, Loss D: 0.0015824532601982355\n",
      "Epoch 184/200, Batch 10/17, Loss G: 3.6428215503692627, Loss D: 0.004289072006940842\n",
      "Epoch 184/200, Batch 11/17, Loss G: 3.602062463760376, Loss D: 0.007584718056023121\n",
      "Epoch 184/200, Batch 12/17, Loss G: 3.935253143310547, Loss D: 0.0056925625540316105\n",
      "Epoch 184/200, Batch 13/17, Loss G: 3.824502468109131, Loss D: 0.0018096728017553687\n",
      "Epoch 184/200, Batch 14/17, Loss G: 3.8231613636016846, Loss D: 0.00214462005533278\n",
      "Epoch 184/200, Batch 15/17, Loss G: 3.609440803527832, Loss D: 0.0021598304156214\n",
      "Epoch 184/200, Batch 16/17, Loss G: 3.6648507118225098, Loss D: 0.0028440889436751604\n",
      "Epoch 185/200, Batch 0/17, Loss G: 3.88655948638916, Loss D: 0.0011024923296645284\n",
      "Epoch 185/200, Batch 1/17, Loss G: 3.665884017944336, Loss D: 0.009588058106601238\n",
      "Epoch 185/200, Batch 2/17, Loss G: 3.8180594444274902, Loss D: 0.010678153485059738\n",
      "Epoch 185/200, Batch 3/17, Loss G: 3.7162604331970215, Loss D: 0.0012433393858373165\n",
      "Epoch 185/200, Batch 4/17, Loss G: 3.705120086669922, Loss D: 0.003418153850361705\n",
      "Epoch 185/200, Batch 5/17, Loss G: 3.713146686553955, Loss D: 0.0022312698420137167\n",
      "Epoch 185/200, Batch 6/17, Loss G: 3.654933452606201, Loss D: 0.003856130177155137\n",
      "Epoch 185/200, Batch 7/17, Loss G: 3.715883731842041, Loss D: 0.0019729137420654297\n",
      "Epoch 185/200, Batch 8/17, Loss G: 3.6539504528045654, Loss D: 0.006500766612589359\n",
      "Epoch 185/200, Batch 9/17, Loss G: 3.5982351303100586, Loss D: 0.0016259815311059356\n",
      "Epoch 185/200, Batch 10/17, Loss G: 3.4847469329833984, Loss D: 0.012291976250708103\n",
      "Epoch 185/200, Batch 11/17, Loss G: 3.8264036178588867, Loss D: 0.0026621324941515923\n",
      "Epoch 185/200, Batch 12/17, Loss G: 3.735670804977417, Loss D: 0.014390486292541027\n",
      "Epoch 185/200, Batch 13/17, Loss G: 3.5753350257873535, Loss D: 0.0015437506372109056\n",
      "Epoch 185/200, Batch 14/17, Loss G: 3.4543228149414062, Loss D: 0.06720030307769775\n",
      "Epoch 185/200, Batch 15/17, Loss G: 4.065580368041992, Loss D: 0.2707749605178833\n",
      "Epoch 185/200, Batch 16/17, Loss G: 3.260688543319702, Loss D: 0.21951760351657867\n",
      "Epoch 186/200, Batch 0/17, Loss G: 3.9064862728118896, Loss D: 0.05860089138150215\n",
      "Epoch 186/200, Batch 1/17, Loss G: 3.595951557159424, Loss D: 0.04591558128595352\n",
      "Epoch 186/200, Batch 2/17, Loss G: 3.725907325744629, Loss D: 0.06510909646749496\n",
      "Epoch 186/200, Batch 3/17, Loss G: 3.0793685913085938, Loss D: 0.20227469503879547\n",
      "Epoch 186/200, Batch 4/17, Loss G: 3.785172939300537, Loss D: 0.1730501651763916\n",
      "Epoch 186/200, Batch 5/17, Loss G: 2.978733539581299, Loss D: 0.3128012716770172\n",
      "Epoch 186/200, Batch 6/17, Loss G: 3.355278730392456, Loss D: 0.12993182241916656\n",
      "Epoch 186/200, Batch 7/17, Loss G: 3.755188465118408, Loss D: 0.2458135485649109\n",
      "Epoch 186/200, Batch 8/17, Loss G: 3.23907470703125, Loss D: 0.13021385669708252\n",
      "Epoch 186/200, Batch 9/17, Loss G: 3.5104868412017822, Loss D: 0.09918060898780823\n",
      "Epoch 186/200, Batch 10/17, Loss G: 3.9447481632232666, Loss D: 0.14367936551570892\n",
      "Epoch 186/200, Batch 11/17, Loss G: 3.309317111968994, Loss D: 0.12113369256258011\n",
      "Epoch 186/200, Batch 12/17, Loss G: 3.4499473571777344, Loss D: 0.07687855511903763\n",
      "Epoch 186/200, Batch 13/17, Loss G: 3.9518327713012695, Loss D: 0.19686684012413025\n",
      "Epoch 186/200, Batch 14/17, Loss G: 3.4227006435394287, Loss D: 0.1473056674003601\n",
      "Epoch 186/200, Batch 15/17, Loss G: 3.661414384841919, Loss D: 0.0832202211022377\n",
      "Epoch 186/200, Batch 16/17, Loss G: 3.730374574661255, Loss D: 0.03415931388735771\n",
      "Epoch 187/200, Batch 0/17, Loss G: 3.2921738624572754, Loss D: 0.1488339602947235\n",
      "Epoch 187/200, Batch 1/17, Loss G: 3.9548001289367676, Loss D: 0.21295689046382904\n",
      "Epoch 187/200, Batch 2/17, Loss G: 3.62979793548584, Loss D: 0.03271667659282684\n",
      "Epoch 187/200, Batch 3/17, Loss G: 3.5362555980682373, Loss D: 0.07165311276912689\n",
      "Epoch 187/200, Batch 4/17, Loss G: 3.7241601943969727, Loss D: 0.0536511167883873\n",
      "Epoch 187/200, Batch 5/17, Loss G: 3.6504039764404297, Loss D: 0.030140265822410583\n",
      "Epoch 187/200, Batch 6/17, Loss G: 3.569091796875, Loss D: 0.023640699684619904\n",
      "Epoch 187/200, Batch 7/17, Loss G: 3.7668004035949707, Loss D: 0.02377067692577839\n",
      "Epoch 187/200, Batch 8/17, Loss G: 3.5424904823303223, Loss D: 0.030195798724889755\n",
      "Epoch 187/200, Batch 9/17, Loss G: 3.7344884872436523, Loss D: 0.023242155089974403\n",
      "Epoch 187/200, Batch 10/17, Loss G: 3.8326001167297363, Loss D: 0.012710902839899063\n",
      "Epoch 187/200, Batch 11/17, Loss G: 3.8141441345214844, Loss D: 0.014570878818631172\n",
      "Epoch 187/200, Batch 12/17, Loss G: 3.6801788806915283, Loss D: 0.01861671358346939\n",
      "Epoch 187/200, Batch 13/17, Loss G: 4.012880325317383, Loss D: 0.009699374437332153\n",
      "Epoch 187/200, Batch 14/17, Loss G: 3.716273307800293, Loss D: 0.012415232136845589\n",
      "Epoch 187/200, Batch 15/17, Loss G: 3.653296947479248, Loss D: 0.011376993730664253\n",
      "Epoch 187/200, Batch 16/17, Loss G: 3.9684486389160156, Loss D: 0.0048984261229634285\n",
      "Epoch 188/200, Batch 0/17, Loss G: 3.7517905235290527, Loss D: 0.015653083100914955\n",
      "Epoch 188/200, Batch 1/17, Loss G: 3.59218168258667, Loss D: 0.008822562173008919\n",
      "Epoch 188/200, Batch 2/17, Loss G: 3.8630177974700928, Loss D: 0.005440224893391132\n",
      "Epoch 188/200, Batch 3/17, Loss G: 3.648743152618408, Loss D: 0.007695465814322233\n",
      "Epoch 188/200, Batch 4/17, Loss G: 3.5918526649475098, Loss D: 0.008323165588080883\n",
      "Epoch 188/200, Batch 5/17, Loss G: 3.6565427780151367, Loss D: 0.016658367589116096\n",
      "Epoch 188/200, Batch 6/17, Loss G: 3.8405094146728516, Loss D: 0.012844225391745567\n",
      "Epoch 188/200, Batch 7/17, Loss G: 3.8245632648468018, Loss D: 0.016928352415561676\n",
      "Epoch 188/200, Batch 8/17, Loss G: 3.455010414123535, Loss D: 0.03088780865073204\n",
      "Epoch 188/200, Batch 9/17, Loss G: 3.7631900310516357, Loss D: 0.024217793717980385\n",
      "Epoch 188/200, Batch 10/17, Loss G: 3.670118808746338, Loss D: 0.008172683417797089\n",
      "Epoch 188/200, Batch 11/17, Loss G: 3.556410789489746, Loss D: 0.013800028711557388\n",
      "Epoch 188/200, Batch 12/17, Loss G: 3.814530372619629, Loss D: 0.010126380249857903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200, Batch 13/17, Loss G: 3.8534607887268066, Loss D: 0.020218877121806145\n",
      "Epoch 188/200, Batch 14/17, Loss G: 3.5410847663879395, Loss D: 0.022355714812874794\n",
      "Epoch 188/200, Batch 15/17, Loss G: 3.722806930541992, Loss D: 0.00954044796526432\n",
      "Epoch 188/200, Batch 16/17, Loss G: 3.9457836151123047, Loss D: 0.015416326001286507\n",
      "Epoch 189/200, Batch 0/17, Loss G: 3.765031337738037, Loss D: 0.0037101060152053833\n",
      "Epoch 189/200, Batch 1/17, Loss G: 3.594924211502075, Loss D: 0.019442973658442497\n",
      "Epoch 189/200, Batch 2/17, Loss G: 3.691051959991455, Loss D: 0.01127806305885315\n",
      "Epoch 189/200, Batch 3/17, Loss G: 3.7132742404937744, Loss D: 0.010845760814845562\n",
      "Epoch 189/200, Batch 4/17, Loss G: 3.6200788021087646, Loss D: 0.016957396641373634\n",
      "Epoch 189/200, Batch 5/17, Loss G: 3.815310001373291, Loss D: 0.005102886352688074\n",
      "Epoch 189/200, Batch 6/17, Loss G: 3.6554148197174072, Loss D: 0.013583659194409847\n",
      "Epoch 189/200, Batch 7/17, Loss G: 3.4853129386901855, Loss D: 0.004066471941769123\n",
      "Epoch 189/200, Batch 8/17, Loss G: 3.6445024013519287, Loss D: 0.010246185585856438\n",
      "Epoch 189/200, Batch 9/17, Loss G: 3.8094987869262695, Loss D: 0.012438631616532803\n",
      "Epoch 189/200, Batch 10/17, Loss G: 3.7064907550811768, Loss D: 0.012672245502471924\n",
      "Epoch 189/200, Batch 11/17, Loss G: 3.7112412452697754, Loss D: 0.009183067828416824\n",
      "Epoch 189/200, Batch 12/17, Loss G: 3.880427122116089, Loss D: 0.002323081251233816\n",
      "Epoch 189/200, Batch 13/17, Loss G: 3.7078723907470703, Loss D: 0.007385268807411194\n",
      "Epoch 189/200, Batch 14/17, Loss G: 3.8063199520111084, Loss D: 0.0030572116374969482\n",
      "Epoch 189/200, Batch 15/17, Loss G: 3.806554079055786, Loss D: 0.008697852492332458\n",
      "Epoch 189/200, Batch 16/17, Loss G: 4.012387275695801, Loss D: 0.001320005627349019\n",
      "Epoch 190/200, Batch 0/17, Loss G: 3.714587688446045, Loss D: 0.006048798561096191\n",
      "Epoch 190/200, Batch 1/17, Loss G: 3.8625049591064453, Loss D: 0.00229642353951931\n",
      "Epoch 190/200, Batch 2/17, Loss G: 3.620882511138916, Loss D: 0.0036567654460668564\n",
      "Epoch 190/200, Batch 3/17, Loss G: 3.628990650177002, Loss D: 0.006970117334276438\n",
      "Epoch 190/200, Batch 4/17, Loss G: 3.764643907546997, Loss D: 0.0013648559106513858\n",
      "Epoch 190/200, Batch 5/17, Loss G: 3.7197327613830566, Loss D: 0.004922792781144381\n",
      "Epoch 190/200, Batch 6/17, Loss G: 3.759687900543213, Loss D: 0.0030395728535950184\n",
      "Epoch 190/200, Batch 7/17, Loss G: 3.6916041374206543, Loss D: 0.007700410205870867\n",
      "Epoch 190/200, Batch 8/17, Loss G: 3.6411309242248535, Loss D: 0.001927760778926313\n",
      "Epoch 190/200, Batch 9/17, Loss G: 3.715183734893799, Loss D: 0.004761653020977974\n",
      "Epoch 190/200, Batch 10/17, Loss G: 3.7116410732269287, Loss D: 0.002709538908675313\n",
      "Epoch 190/200, Batch 11/17, Loss G: 3.660109043121338, Loss D: 0.007445947267115116\n",
      "Epoch 190/200, Batch 12/17, Loss G: 3.6042532920837402, Loss D: 0.011517936363816261\n",
      "Epoch 190/200, Batch 13/17, Loss G: 3.859910011291504, Loss D: 0.0022132585290819407\n",
      "Epoch 190/200, Batch 14/17, Loss G: 3.6451375484466553, Loss D: 0.0024349107407033443\n",
      "Epoch 190/200, Batch 15/17, Loss G: 3.862684726715088, Loss D: 0.005932893604040146\n",
      "Epoch 190/200, Batch 16/17, Loss G: 3.6740102767944336, Loss D: 0.0033568572252988815\n",
      "Epoch 191/200, Batch 0/17, Loss G: 3.56538724899292, Loss D: 0.002577848033979535\n",
      "Epoch 191/200, Batch 1/17, Loss G: 3.83093523979187, Loss D: 0.003547584405168891\n",
      "Epoch 191/200, Batch 2/17, Loss G: 3.7091689109802246, Loss D: 0.0013417417649179697\n",
      "Epoch 191/200, Batch 3/17, Loss G: 3.5527467727661133, Loss D: 0.004528768360614777\n",
      "Epoch 191/200, Batch 4/17, Loss G: 3.767760753631592, Loss D: 0.004794621840119362\n",
      "Epoch 191/200, Batch 5/17, Loss G: 3.781177043914795, Loss D: 0.011151866056025028\n",
      "Epoch 191/200, Batch 6/17, Loss G: 3.6211743354797363, Loss D: 0.003959914669394493\n",
      "Epoch 191/200, Batch 7/17, Loss G: 3.7468743324279785, Loss D: 0.0032887861598283052\n",
      "Epoch 191/200, Batch 8/17, Loss G: 3.6474695205688477, Loss D: 0.004645760636776686\n",
      "Epoch 191/200, Batch 9/17, Loss G: 3.848421573638916, Loss D: 0.0033521854784339666\n",
      "Epoch 191/200, Batch 10/17, Loss G: 3.722791910171509, Loss D: 0.0058839451521635056\n",
      "Epoch 191/200, Batch 11/17, Loss G: 3.8299217224121094, Loss D: 0.0030751139856874943\n",
      "Epoch 191/200, Batch 12/17, Loss G: 3.669508695602417, Loss D: 0.0024615274742245674\n",
      "Epoch 191/200, Batch 13/17, Loss G: 3.6175150871276855, Loss D: 0.002875302452594042\n",
      "Epoch 191/200, Batch 14/17, Loss G: 3.602005958557129, Loss D: 0.0022559885401278734\n",
      "Epoch 191/200, Batch 15/17, Loss G: 3.647793769836426, Loss D: 0.0018186334054917097\n",
      "Epoch 191/200, Batch 16/17, Loss G: 3.6783394813537598, Loss D: 0.0019886004738509655\n",
      "Epoch 192/200, Batch 0/17, Loss G: 3.5478885173797607, Loss D: 0.003163337241858244\n",
      "Epoch 192/200, Batch 1/17, Loss G: 3.5485808849334717, Loss D: 0.003982463851571083\n",
      "Epoch 192/200, Batch 2/17, Loss G: 3.6463398933410645, Loss D: 0.0029910204466432333\n",
      "Epoch 192/200, Batch 3/17, Loss G: 3.6286141872406006, Loss D: 0.0035391903948038816\n",
      "Epoch 192/200, Batch 4/17, Loss G: 3.821157932281494, Loss D: 0.002020635176450014\n",
      "Epoch 192/200, Batch 5/17, Loss G: 3.689992904663086, Loss D: 0.008004955016076565\n",
      "Epoch 192/200, Batch 6/17, Loss G: 3.8012843132019043, Loss D: 0.0010847114026546478\n",
      "Epoch 192/200, Batch 7/17, Loss G: 3.6194324493408203, Loss D: 0.001926891622133553\n",
      "Epoch 192/200, Batch 8/17, Loss G: 3.719327926635742, Loss D: 0.005183530505746603\n",
      "Epoch 192/200, Batch 9/17, Loss G: 3.5853915214538574, Loss D: 0.010568110272288322\n",
      "Epoch 192/200, Batch 10/17, Loss G: 3.7058563232421875, Loss D: 0.0019830891396850348\n",
      "Epoch 192/200, Batch 11/17, Loss G: 3.668844699859619, Loss D: 0.00758766196668148\n",
      "Epoch 192/200, Batch 12/17, Loss G: 3.639589309692383, Loss D: 0.0017811486031860113\n",
      "Epoch 192/200, Batch 13/17, Loss G: 3.6132712364196777, Loss D: 0.0019426937215030193\n",
      "Epoch 192/200, Batch 14/17, Loss G: 3.5020639896392822, Loss D: 0.009376675821840763\n",
      "Epoch 192/200, Batch 15/17, Loss G: 3.6739518642425537, Loss D: 0.006950823590159416\n",
      "Epoch 192/200, Batch 16/17, Loss G: 3.884964942932129, Loss D: 0.0015548605006188154\n",
      "Epoch 193/200, Batch 0/17, Loss G: 3.848994016647339, Loss D: 0.0026059290394186974\n",
      "Epoch 193/200, Batch 1/17, Loss G: 3.8174824714660645, Loss D: 0.0009806226007640362\n",
      "Epoch 193/200, Batch 2/17, Loss G: 3.855602264404297, Loss D: 0.0028922511264681816\n",
      "Epoch 193/200, Batch 3/17, Loss G: 3.673985719680786, Loss D: 0.0015915422700345516\n",
      "Epoch 193/200, Batch 4/17, Loss G: 3.6587719917297363, Loss D: 0.010287151671946049\n",
      "Epoch 193/200, Batch 5/17, Loss G: 3.7549924850463867, Loss D: 0.008710665628314018\n",
      "Epoch 193/200, Batch 6/17, Loss G: 3.7825140953063965, Loss D: 0.0012680161744356155\n",
      "Epoch 193/200, Batch 7/17, Loss G: 3.61002254486084, Loss D: 0.0015168980462476611\n",
      "Epoch 193/200, Batch 8/17, Loss G: 3.616189479827881, Loss D: 0.0029227021150290966\n",
      "Epoch 193/200, Batch 9/17, Loss G: 3.564502000808716, Loss D: 0.0026372962165623903\n",
      "Epoch 193/200, Batch 10/17, Loss G: 3.641965389251709, Loss D: 0.0021763404365628958\n",
      "Epoch 193/200, Batch 11/17, Loss G: 3.4958527088165283, Loss D: 0.002439219504594803\n",
      "Epoch 193/200, Batch 12/17, Loss G: 3.8180088996887207, Loss D: 0.0033761330414563417\n",
      "Epoch 193/200, Batch 13/17, Loss G: 3.561817169189453, Loss D: 0.003171992488205433\n",
      "Epoch 193/200, Batch 14/17, Loss G: 3.6802635192871094, Loss D: 0.001511097070761025\n",
      "Epoch 193/200, Batch 15/17, Loss G: 3.571467399597168, Loss D: 0.003482960630208254\n",
      "Epoch 193/200, Batch 16/17, Loss G: 3.7067794799804688, Loss D: 0.0024881798308342695\n",
      "Epoch 194/200, Batch 0/17, Loss G: 3.758882761001587, Loss D: 0.0027180942706763744\n",
      "Epoch 194/200, Batch 1/17, Loss G: 3.549213171005249, Loss D: 0.0016751186922192574\n",
      "Epoch 194/200, Batch 2/17, Loss G: 3.7182374000549316, Loss D: 0.0026045851409435272\n",
      "Epoch 194/200, Batch 3/17, Loss G: 3.7556369304656982, Loss D: 0.0019280471606180072\n",
      "Epoch 194/200, Batch 4/17, Loss G: 3.5156209468841553, Loss D: 0.011195052415132523\n",
      "Epoch 194/200, Batch 5/17, Loss G: 3.7300987243652344, Loss D: 0.01614489033818245\n",
      "Epoch 194/200, Batch 6/17, Loss G: 3.5600271224975586, Loss D: 0.003307472448796034\n",
      "Epoch 194/200, Batch 7/17, Loss G: 3.73684024810791, Loss D: 0.006241151597350836\n",
      "Epoch 194/200, Batch 8/17, Loss G: 3.673516273498535, Loss D: 0.0026150736957788467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200, Batch 9/17, Loss G: 3.6206982135772705, Loss D: 0.008514698594808578\n",
      "Epoch 194/200, Batch 10/17, Loss G: 3.5458412170410156, Loss D: 0.005737924482673407\n",
      "Epoch 194/200, Batch 11/17, Loss G: 3.7980499267578125, Loss D: 0.001375660765916109\n",
      "Epoch 194/200, Batch 12/17, Loss G: 3.563847541809082, Loss D: 0.0032285614870488644\n",
      "Epoch 194/200, Batch 13/17, Loss G: 3.6870410442352295, Loss D: 0.0020482041873037815\n",
      "Epoch 194/200, Batch 14/17, Loss G: 3.696399688720703, Loss D: 0.002757232403382659\n",
      "Epoch 194/200, Batch 15/17, Loss G: 3.655787229537964, Loss D: 0.0034587751142680645\n",
      "Epoch 194/200, Batch 16/17, Loss G: 3.658210039138794, Loss D: 0.0029142575804144144\n",
      "Epoch 195/200, Batch 0/17, Loss G: 3.8092217445373535, Loss D: 0.0023223161697387695\n",
      "Epoch 195/200, Batch 1/17, Loss G: 3.723594903945923, Loss D: 0.0014968004543334246\n",
      "Epoch 195/200, Batch 2/17, Loss G: 3.639802932739258, Loss D: 0.004043163266032934\n",
      "Epoch 195/200, Batch 3/17, Loss G: 3.5511512756347656, Loss D: 0.0009025224135257304\n",
      "Epoch 195/200, Batch 4/17, Loss G: 3.567138671875, Loss D: 0.01771377958357334\n",
      "Epoch 195/200, Batch 5/17, Loss G: 3.824803113937378, Loss D: 0.016814682632684708\n",
      "Epoch 195/200, Batch 6/17, Loss G: 3.5258290767669678, Loss D: 0.007712931372225285\n",
      "Epoch 195/200, Batch 7/17, Loss G: 3.6124393939971924, Loss D: 0.0028771981596946716\n",
      "Epoch 195/200, Batch 8/17, Loss G: 3.5540382862091064, Loss D: 0.0045361267402768135\n",
      "Epoch 195/200, Batch 9/17, Loss G: 3.621713876724243, Loss D: 0.0012703125830739737\n",
      "Epoch 195/200, Batch 10/17, Loss G: 3.641176462173462, Loss D: 0.0027351900935173035\n",
      "Epoch 195/200, Batch 11/17, Loss G: 3.7678136825561523, Loss D: 0.0033620051108300686\n",
      "Epoch 195/200, Batch 12/17, Loss G: 3.8298330307006836, Loss D: 0.0036506822798401117\n",
      "Epoch 195/200, Batch 13/17, Loss G: 3.687255859375, Loss D: 0.0030096808914095163\n",
      "Epoch 195/200, Batch 14/17, Loss G: 3.504392147064209, Loss D: 0.0017210650257766247\n",
      "Epoch 195/200, Batch 15/17, Loss G: 3.7407045364379883, Loss D: 0.002340848557651043\n",
      "Epoch 195/200, Batch 16/17, Loss G: 3.7799854278564453, Loss D: 0.0010631722398102283\n",
      "Epoch 196/200, Batch 0/17, Loss G: 3.6310834884643555, Loss D: 0.00213501020334661\n",
      "Epoch 196/200, Batch 1/17, Loss G: 3.7039952278137207, Loss D: 0.0015740515664219856\n",
      "Epoch 196/200, Batch 2/17, Loss G: 3.7413196563720703, Loss D: 0.00097587367054075\n",
      "Epoch 196/200, Batch 3/17, Loss G: 3.79443359375, Loss D: 0.0021221947390586138\n",
      "Epoch 196/200, Batch 4/17, Loss G: 3.6794142723083496, Loss D: 0.0019285546150058508\n",
      "Epoch 196/200, Batch 5/17, Loss G: 3.6643218994140625, Loss D: 0.004164547193795443\n",
      "Epoch 196/200, Batch 6/17, Loss G: 3.6432154178619385, Loss D: 0.0018404602305963635\n",
      "Epoch 196/200, Batch 7/17, Loss G: 3.7305350303649902, Loss D: 0.00216753501445055\n",
      "Epoch 196/200, Batch 8/17, Loss G: 3.7685933113098145, Loss D: 0.0007900076452642679\n",
      "Epoch 196/200, Batch 9/17, Loss G: 3.5947229862213135, Loss D: 0.0048297615721821785\n",
      "Epoch 196/200, Batch 10/17, Loss G: 3.5426173210144043, Loss D: 0.006397891789674759\n",
      "Epoch 196/200, Batch 11/17, Loss G: 3.8229384422302246, Loss D: 0.0008185334736481309\n",
      "Epoch 196/200, Batch 12/17, Loss G: 3.6129331588745117, Loss D: 0.005643678829073906\n",
      "Epoch 196/200, Batch 13/17, Loss G: 3.666933059692383, Loss D: 0.004203543066978455\n",
      "Epoch 196/200, Batch 14/17, Loss G: 3.562584638595581, Loss D: 0.005757386330515146\n",
      "Epoch 196/200, Batch 15/17, Loss G: 3.7609071731567383, Loss D: 0.001502402825281024\n",
      "Epoch 196/200, Batch 16/17, Loss G: 3.6484603881835938, Loss D: 0.003189963288605213\n",
      "Epoch 197/200, Batch 0/17, Loss G: 3.7704572677612305, Loss D: 0.0007782684988342226\n",
      "Epoch 197/200, Batch 1/17, Loss G: 3.727451801300049, Loss D: 0.001108712749555707\n",
      "Epoch 197/200, Batch 2/17, Loss G: 3.6536831855773926, Loss D: 0.0013229247415438294\n",
      "Epoch 197/200, Batch 3/17, Loss G: 3.5973901748657227, Loss D: 0.001598965609446168\n",
      "Epoch 197/200, Batch 4/17, Loss G: 3.650317907333374, Loss D: 0.003059841226786375\n",
      "Epoch 197/200, Batch 5/17, Loss G: 3.748476266860962, Loss D: 0.0016840756870806217\n",
      "Epoch 197/200, Batch 6/17, Loss G: 3.5809073448181152, Loss D: 0.0011319690383970737\n",
      "Epoch 197/200, Batch 7/17, Loss G: 3.571195602416992, Loss D: 0.0013952015433460474\n",
      "Epoch 197/200, Batch 8/17, Loss G: 3.645742893218994, Loss D: 0.0028191921301186085\n",
      "Epoch 197/200, Batch 9/17, Loss G: 3.698573112487793, Loss D: 0.0018838475225493312\n",
      "Epoch 197/200, Batch 10/17, Loss G: 3.681823253631592, Loss D: 0.003628498176112771\n",
      "Epoch 197/200, Batch 11/17, Loss G: 3.582646131515503, Loss D: 0.0020542885176837444\n",
      "Epoch 197/200, Batch 12/17, Loss G: 3.6542704105377197, Loss D: 0.0012679032515734434\n",
      "Epoch 197/200, Batch 13/17, Loss G: 3.5076324939727783, Loss D: 0.0016627393197268248\n",
      "Epoch 197/200, Batch 14/17, Loss G: 3.569817304611206, Loss D: 0.0023050494492053986\n",
      "Epoch 197/200, Batch 15/17, Loss G: 3.622307062149048, Loss D: 0.0066736480221152306\n",
      "Epoch 197/200, Batch 16/17, Loss G: 3.8424506187438965, Loss D: 0.002566657727584243\n",
      "Epoch 198/200, Batch 0/17, Loss G: 3.730531930923462, Loss D: 0.01041865162551403\n",
      "Epoch 198/200, Batch 1/17, Loss G: 3.5706307888031006, Loss D: 0.0008555546519346535\n",
      "Epoch 198/200, Batch 2/17, Loss G: 3.505251884460449, Loss D: 0.0014582037692889571\n",
      "Epoch 198/200, Batch 3/17, Loss G: 3.5196728706359863, Loss D: 0.009331608191132545\n",
      "Epoch 198/200, Batch 4/17, Loss G: 3.7102556228637695, Loss D: 0.0030002137646079063\n",
      "Epoch 198/200, Batch 5/17, Loss G: 3.678298234939575, Loss D: 0.01317949965596199\n",
      "Epoch 198/200, Batch 6/17, Loss G: 3.672539710998535, Loss D: 0.003295360831543803\n",
      "Epoch 198/200, Batch 7/17, Loss G: 3.6130547523498535, Loss D: 0.008077552542090416\n",
      "Epoch 198/200, Batch 8/17, Loss G: 3.747135877609253, Loss D: 0.004817432723939419\n",
      "Epoch 198/200, Batch 9/17, Loss G: 3.618584632873535, Loss D: 0.001850024564191699\n",
      "Epoch 198/200, Batch 10/17, Loss G: 3.6462204456329346, Loss D: 0.001427347306162119\n",
      "Epoch 198/200, Batch 11/17, Loss G: 3.675990104675293, Loss D: 0.0015179116744548082\n",
      "Epoch 198/200, Batch 12/17, Loss G: 3.618136405944824, Loss D: 0.005478020291775465\n",
      "Epoch 198/200, Batch 13/17, Loss G: 3.6856751441955566, Loss D: 0.006591904442757368\n",
      "Epoch 198/200, Batch 14/17, Loss G: 3.670588254928589, Loss D: 0.002034514443948865\n",
      "Epoch 198/200, Batch 15/17, Loss G: 3.4815196990966797, Loss D: 0.02472519874572754\n",
      "Epoch 198/200, Batch 16/17, Loss G: 3.7956671714782715, Loss D: 0.03744449093937874\n",
      "Epoch 199/200, Batch 0/17, Loss G: 3.488295555114746, Loss D: 0.011831860989332199\n",
      "Epoch 199/200, Batch 1/17, Loss G: 3.736546516418457, Loss D: 0.0019623066764324903\n",
      "Epoch 199/200, Batch 2/17, Loss G: 3.627349376678467, Loss D: 0.021609731018543243\n",
      "Epoch 199/200, Batch 3/17, Loss G: 3.686129093170166, Loss D: 0.0036608371883630753\n",
      "Epoch 199/200, Batch 4/17, Loss G: 3.6587576866149902, Loss D: 0.009413611143827438\n",
      "Epoch 199/200, Batch 5/17, Loss G: 3.5387163162231445, Loss D: 0.011569318361580372\n",
      "Epoch 199/200, Batch 6/17, Loss G: 3.614600658416748, Loss D: 0.003691280260682106\n",
      "Epoch 199/200, Batch 7/17, Loss G: 3.7911837100982666, Loss D: 0.0017862526001408696\n",
      "Epoch 199/200, Batch 8/17, Loss G: 3.5883212089538574, Loss D: 0.006513532716780901\n",
      "Epoch 199/200, Batch 9/17, Loss G: 3.742292642593384, Loss D: 0.002084057778120041\n",
      "Epoch 199/200, Batch 10/17, Loss G: 3.6440351009368896, Loss D: 0.0034870076924562454\n",
      "Epoch 199/200, Batch 11/17, Loss G: 3.744142770767212, Loss D: 0.0038274768739938736\n",
      "Epoch 199/200, Batch 12/17, Loss G: 3.6369855403900146, Loss D: 0.004001472145318985\n",
      "Epoch 199/200, Batch 13/17, Loss G: 3.602478265762329, Loss D: 0.00375161855481565\n",
      "Epoch 199/200, Batch 14/17, Loss G: 3.586902141571045, Loss D: 0.0031843227334320545\n",
      "Epoch 199/200, Batch 15/17, Loss G: 3.5580365657806396, Loss D: 0.0027429491747170687\n",
      "Epoch 199/200, Batch 16/17, Loss G: 3.7603981494903564, Loss D: 0.00705735245719552\n",
      "Epoch 200/200, Batch 0/17, Loss G: 3.901862621307373, Loss D: 0.0037508707027882338\n",
      "Epoch 200/200, Batch 1/17, Loss G: 3.6647043228149414, Loss D: 0.005421078763902187\n",
      "Epoch 200/200, Batch 2/17, Loss G: 3.7452919483184814, Loss D: 0.0025073462165892124\n",
      "Epoch 200/200, Batch 3/17, Loss G: 3.674506187438965, Loss D: 0.00524917570874095\n",
      "Epoch 200/200, Batch 4/17, Loss G: 3.704007625579834, Loss D: 0.005411716178059578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Batch 5/17, Loss G: 3.657466173171997, Loss D: 0.00466145109385252\n",
      "Epoch 200/200, Batch 6/17, Loss G: 3.6123080253601074, Loss D: 0.0025287370663136244\n",
      "Epoch 200/200, Batch 7/17, Loss G: 3.5896005630493164, Loss D: 0.0017605919856578112\n",
      "Epoch 200/200, Batch 8/17, Loss G: 3.5299763679504395, Loss D: 0.0009864085586741567\n",
      "Epoch 200/200, Batch 9/17, Loss G: 3.6142935752868652, Loss D: 0.0040749660693109035\n",
      "Epoch 200/200, Batch 10/17, Loss G: 3.72916316986084, Loss D: 0.0013421502662822604\n",
      "Epoch 200/200, Batch 11/17, Loss G: 3.6513428688049316, Loss D: 0.004924968350678682\n",
      "Epoch 200/200, Batch 12/17, Loss G: 3.5499281883239746, Loss D: 0.0016350150108337402\n",
      "Epoch 200/200, Batch 13/17, Loss G: 3.560455799102783, Loss D: 0.0010701711289584637\n",
      "Epoch 200/200, Batch 14/17, Loss G: 3.535938262939453, Loss D: 0.0025572404265403748\n",
      "Epoch 200/200, Batch 15/17, Loss G: 3.534745216369629, Loss D: 0.0047152601182460785\n",
      "Epoch 200/200, Batch 16/17, Loss G: 3.577332019805908, Loss D: 0.010950588621199131\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        middle = self.middle(encoded)\n",
    "        decoded = self.decoder(middle)\n",
    "        return decoded\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "def preprocess_image(image_path, size):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomResizedCrop(size, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def load_dataset(bw_folder, color_folder, image_size, batch_size):\n",
    "    bw_image_paths = [os.path.join(bw_folder, filename) for filename in os.listdir(bw_folder) if not filename.startswith('.')]\n",
    "    color_image_paths = [os.path.join(color_folder, filename) for filename in os.listdir(color_folder) if not filename.startswith('.')]\n",
    "\n",
    "    dataset = [(preprocess_image(bw_path, image_size), preprocess_image(color_path, image_size)) for bw_path, color_path in zip(bw_image_paths, color_image_paths)]\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "def train(generator, discriminator, train_loader, num_epochs, device):\n",
    "    criterion_GAN = nn.MSELoss()\n",
    "    criterion_cycle = nn.L1Loss()\n",
    "    criterion_identity = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (bw_imgs, color_imgs) in enumerate(train_loader):\n",
    "            bw_imgs = bw_imgs.to(device)\n",
    "            color_imgs = color_imgs.to(device)\n",
    "\n",
    "            # Train the generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            fake_color_imgs = generator(bw_imgs)\n",
    "\n",
    "            # Identity loss\n",
    "            resized_color_imgs = F.interpolate(color_imgs, size=fake_color_imgs.size()[2:], mode='bilinear', align_corners=True)\n",
    "            loss_identity = criterion_identity(fake_color_imgs, resized_color_imgs) * 5.0\n",
    "\n",
    "            # GAN loss\n",
    "            pred_fake = discriminator(fake_color_imgs)\n",
    "            loss_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "\n",
    "            # Cycle loss\n",
    "            recovered_bw_imgs = generator(fake_color_imgs)\n",
    "            loss_cycle = criterion_cycle(recovered_bw_imgs, bw_imgs) * 10.0\n",
    "\n",
    "            # Total generator loss\n",
    "            loss_G = loss_identity + loss_GAN + loss_cycle\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train the discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            pred_real = discriminator(color_imgs)\n",
    "            loss_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "\n",
    "            pred_fake = discriminator(fake_color_imgs.detach())\n",
    "            loss_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "\n",
    "            # Total discriminator loss\n",
    "            loss_D = (loss_real + loss_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss G: {loss_G.item()}, Loss D: {loss_D.item()}\")\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                save_image(fake_color_imgs.data[:25], f\"output/generated_{epoch}_{batch_idx}.png\", nrow=5, normalize=True)\n",
    "                torch.save(generator.state_dict(), f\"output/generator_{epoch}_{batch_idx}.pth\")\n",
    "                torch.save(discriminator.state_dict(), f\"output/discriminator_{epoch}_{batch_idx}.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = (256, 256)\n",
    "batch_size = 8\n",
    "num_epochs = 200\n",
    "\n",
    "bw_folder = \"/Users/zachariahalzubi/Desktop/onePieceColoring/black_and_white\"\n",
    "color_folder = \"/Users/zachariahalzubi/Desktop/onePieceColoring/colored\"\n",
    "\n",
    "train_loader = load_dataset(bw_folder, color_folder, image_size, batch_size)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "train(generator, discriminator, train_loader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b59645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the path to the user's desktop\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "\n",
    "# Save the weights to the desktop\n",
    "weights_path = os.path.join(desktop_path, \"saved_weights2.pth\")\n",
    "torch.save(generator.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0316f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the trained model\n",
    "generator = Generator()\n",
    "weights_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"saved_weights2.pth\")\n",
    "generator.load_state_dict(torch.load(weights_path))\n",
    "generator.eval()\n",
    "\n",
    "# Preprocess the test image\n",
    "test_image_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\",\"onePieceColoring\", \"black_and_white\", \"image_14.jpg\")\n",
    "image_size = (256, 256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_image = Image.open(test_image_path)\n",
    "input_image = transform(test_image).unsqueeze(0)\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    colorized_image = generator(input_image)\n",
    "\n",
    "# Postprocess the colorized image\n",
    "colorized_image = colorized_image.squeeze(0).permute(1, 2, 0)\n",
    "colorized_image = (colorized_image + 1) / 2  # Denormalize the image\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "colorized_image_np = colorized_image.cpu().numpy()\n",
    "\n",
    "# Convert the numpy array to a PIL Image\n",
    "colorized_image_pil = Image.fromarray((255 * colorized_image_np).astype(np.uint8))\n",
    "\n",
    "# Save the colorized image\n",
    "save_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"colored_by_model3.jpg\")\n",
    "colorized_image_pil.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
